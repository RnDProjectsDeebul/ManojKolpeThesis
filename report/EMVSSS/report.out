\BOOKMARK [0][-]{chapter*.4}{List of Figures}{}% 1
\BOOKMARK [0][-]{chapter*.5}{List of Tables}{}% 2
\BOOKMARK [0][-]{chapter.1}{Introduction}{}% 3
\BOOKMARK [1][-]{section.1.1}{Motivation}{chapter.1}% 4
\BOOKMARK [2][-]{subsection.1.1.1}{Temporal fusion}{section.1.1}% 5
\BOOKMARK [2][-]{subsection.1.1.2}{Semantic segmentation}{section.1.1}% 6
\BOOKMARK [1][-]{section.1.2}{Challenges and Difficulties}{chapter.1}% 7
\BOOKMARK [2][-]{subsection.1.2.1}{Dataset}{section.1.2}% 8
\BOOKMARK [2][-]{subsection.1.2.2}{Fusion architecture}{section.1.2}% 9
\BOOKMARK [2][-]{subsection.1.2.3}{Computation cost}{section.1.2}% 10
\BOOKMARK [2][-]{subsection.1.2.4}{Real time inference for various application areas}{section.1.2}% 11
\BOOKMARK [1][-]{section.1.3}{Use cases}{chapter.1}% 12
\BOOKMARK [2][-]{subsection.1.3.1}{Autonomous driving and Robotics}{section.1.3}% 13
\BOOKMARK [2][-]{subsection.1.3.2}{Weed mapping using Unmanned Aerial Vehicle \(UAV\)}{section.1.3}% 14
\BOOKMARK [2][-]{subsection.1.3.3}{Real-Time Hand Gesture Recognition}{section.1.3}% 15
\BOOKMARK [1][-]{section.1.4}{Problem Statement and Contribution}{chapter.1}% 16
\BOOKMARK [2][-]{subsection.1.4.1}{Research question}{section.1.4}% 17
\BOOKMARK [2][-]{subsection.1.4.2}{Contribution}{section.1.4}% 18
\BOOKMARK [1][-]{section.1.5}{Report outline}{chapter.1}% 19
\BOOKMARK [0][-]{chapter.2}{State of the Art}{}% 20
\BOOKMARK [1][-]{section.2.1}{Deep Learning}{chapter.2}% 21
\BOOKMARK [1][-]{section.2.2}{Temporal Fusion}{chapter.2}% 22
\BOOKMARK [1][-]{section.2.3}{Semantic Segmentation}{chapter.2}% 23
\BOOKMARK [2][-]{subsection.2.3.1}{Classical Semantic Segmentation}{section.2.3}% 24
\BOOKMARK [2][-]{subsection.2.3.2}{Deep Learning based Semantic Segmentation}{section.2.3}% 25
\BOOKMARK [1][-]{section.2.4}{Temporal Fusion in Semantic Segmentation}{chapter.2}% 26
\BOOKMARK [1][-]{section.2.5}{Limitations of Previous Work}{chapter.2}% 27
\BOOKMARK [0][-]{chapter.3}{Methodology}{}% 28
\BOOKMARK [1][-]{section.3.1}{Dataset}{chapter.3}% 29
\BOOKMARK [2][-]{subsection.3.1.1}{ScanNet}{section.3.1}% 30
\BOOKMARK [2][-]{subsection.3.1.2}{Virtual KITTI 2}{section.3.1}% 31
\BOOKMARK [2][-]{subsection.3.1.3}{VIODE}{section.3.1}% 32
\BOOKMARK [1][-]{section.3.2}{Data Collection and Preprocessing}{chapter.3}% 33
\BOOKMARK [1][-]{section.3.3}{Experimental Design}{chapter.3}% 34
\BOOKMARK [2][-]{subsection.3.3.1}{U-Net Vanilla model}{section.3.3}% 35
\BOOKMARK [2][-]{subsection.3.3.2}{U-Net with Temporal Fusion}{section.3.3}% 36
\BOOKMARK [2][-]{subsection.3.3.3}{W-Net Vanilla model}{section.3.3}% 37
\BOOKMARK [2][-]{subsection.3.3.4}{W-Net with Temporal Fusion}{section.3.3}% 38
\BOOKMARK [1][-]{section.3.4}{Training and Evaluation Pipeline}{chapter.3}% 39
\BOOKMARK [1][-]{section.3.5}{Training Procedure}{chapter.3}% 40
\BOOKMARK [1][-]{section.3.6}{Hardware Configuration}{chapter.3}% 41
\BOOKMARK [0][-]{chapter.4}{Evaluation and Experimental Result}{}% 42
\BOOKMARK [1][-]{section.4.1}{Evaluation Metric}{chapter.4}% 43
\BOOKMARK [2][-]{subsection.4.1.1}{Pixel Accuracy}{section.4.1}% 44
\BOOKMARK [2][-]{subsection.4.1.2}{IoU}{section.4.1}% 45
\BOOKMARK [1][-]{section.4.2}{RQ1: What are the works on state-of-the-art temporal fusion?}{chapter.4}% 46
\BOOKMARK [2][-]{subsection.4.2.1}{Experiment1.1: U-Net and W-Net model with single sequence data}{section.4.2}% 47
\BOOKMARK [2][-]{subsection.4.2.2}{Experiment1.2: U-Net and W-Net model with two sequence data}{section.4.2}% 48
\BOOKMARK [2][-]{subsection.4.2.3}{Experiment1.3: U-Net and W-Net model with three sequence data}{section.4.2}% 49
\BOOKMARK [2][-]{subsection.4.2.4}{Experiment1.4: U-Net and W-Net model with four sequence data}{section.4.2}% 50
\BOOKMARK [2][-]{subsection.4.2.5}{Experiment1.5: U-Net and W-Net model with all sequence data}{section.4.2}% 51
\BOOKMARK [1][-]{section.4.3}{RQ2: How are the results from RQ1 compared with each other to perform temporal fusion?}{chapter.4}% 52
\BOOKMARK [2][-]{subsection.4.3.1}{Experiment1.1: U-Net and W-Net model with single sequence data}{section.4.3}% 53
\BOOKMARK [2][-]{subsection.4.3.2}{Experiment1.2: U-Net and W-Net model with two sequence data}{section.4.3}% 54
\BOOKMARK [2][-]{subsection.4.3.3}{Experiment1.3: U-Net and W-Net model with three sequence data}{section.4.3}% 55
\BOOKMARK [2][-]{subsection.4.3.4}{Experiment1.4: U-Net and W-Net model with four sequence data}{section.4.3}% 56
\BOOKMARK [2][-]{subsection.4.3.5}{Experiment1.5: U-Net and W-Net model with all sequence data}{section.4.3}% 57
\BOOKMARK [1][-]{section.4.4}{RQ3: How to cross-transfer the temporal fusion technique to semantic segmentation?}{chapter.4}% 58
\BOOKMARK [2][-]{subsection.4.4.1}{Experiment1.1: U-Net vanilla model}{section.4.4}% 59
\BOOKMARK [2][-]{subsection.4.4.2}{Experiment1.2: U-Net temporally fused gp model}{section.4.4}% 60
\BOOKMARK [2][-]{subsection.4.4.3}{Experiment1.3: U-Net temporally fused lstm model}{section.4.4}% 61
\BOOKMARK [2][-]{subsection.4.4.4}{Temporal fusion on a continuous sequence data}{section.4.4}% 62
\BOOKMARK [2][-]{subsection.4.4.5}{RQ3.1: Which fusion method is good for the scannet data?}{section.4.4}% 63
\BOOKMARK [2][-]{subsection.4.4.6}{RQ3.2: Which fusion method is good for the virtual kitti data?}{section.4.4}% 64
\BOOKMARK [2][-]{subsection.4.4.7}{RQ3.3: Which fusion method is good for the VIODE data?}{section.4.4}% 65
\BOOKMARK [0][-]{chapter.5}{Android Deployment}{}% 66
\BOOKMARK [1][-]{section.5.1}{Framework}{chapter.5}% 67
\BOOKMARK [1][-]{section.5.2}{Pipeline}{chapter.5}% 68
\BOOKMARK [1][-]{section.5.3}{Deployment and Results}{chapter.5}% 69
\BOOKMARK [0][-]{chapter.6}{Conclusions}{}% 70
\BOOKMARK [1][-]{section.6.1}{Contributions}{chapter.6}% 71
\BOOKMARK [1][-]{section.6.2}{Lessons learned}{chapter.6}% 72
\BOOKMARK [1][-]{section.6.3}{Future work}{chapter.6}% 73
\BOOKMARK [0][-]{appendix.a.A}{Appendix Design Details}{}% 74
\BOOKMARK [0][-]{appendix.a.B}{Appendix Parameters}{}% 75
\BOOKMARK [0][-]{appendix*.37}{References}{}% 76
