\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Data fusion categories based on the type of fusion. Temporal data fusion fuse the temporal data from past sequences, and multi-sensor data fuse the data from multiple sensors collected at the moment. Generated with \href {https://app.diagrams.net/}{Diagrams.net} \relax }}{2}{figure.caption.6}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Deep learning in the artificial intelligence domain. Courtesy of \cite {35_mldl}\relax }}{7}{figure.caption.7}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Multi-View Stereo by Temporal Nonparametric Fusion. Two consecutive frames are used to construct the cost volume and the output is passed onto the encoder to generate latent space encoding. The frames coordinates are transferred to the Gaussian Process to generate the updated latent space encoding. The Gaussian Process take encoder output, frame coordinates, and the propagated latent space encoding. The output of Gaussian Process is forwarded to decoder to generate the depth map. Courtesy of \cite {52_hou2019multi}\relax }}{9}{figure.caption.8}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Picture depicting the semantic, instance and panoptic segmentation examples. Courtesy of \cite {55_WinNT}\relax }}{10}{figure.caption.9}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces Plain encoder-decoder architecture with latent space encoding. Courtesy of \cite {60_minaee2021image}\relax }}{11}{figure.caption.10}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces Simple encoder-decoder architecture with convolution and deconvolution network. Courtesy of \cite {64_noh2015learning}\relax }}{12}{figure.caption.11}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces SegNet architecture with novel upsampling of latent space encoding. Courtesy of \cite {62_badrinarayanan2017segnet}\relax }}{12}{figure.caption.12}%
\contentsline {figure}{\numberline {2.7}{\ignorespaces Unet architecture. A semantic segmentation network for biomedical applications with the strong use of augmentation. The network consist of encoder that down samples the features and a decoder for up sampling. Courtesy of \cite {70_ronneberger2015u}\relax }}{13}{figure.caption.13}%
\contentsline {figure}{\numberline {2.8}{\ignorespaces TDNet is a efficient and accurate video segmentation framework. Courtesy of \cite {78_hu2020temporally}\relax }}{14}{figure.caption.14}%
\contentsline {figure}{\numberline {2.9}{\ignorespaces VIS proposed framework.An online video instance segmentation framework with instance aware temporal fusion method. Intra frame attention framework for combining instance code and feature map. Inter-frame attention for fusing the hybrid temporal information from previous prediction stage. Courtesy of [87]\relax }}{15}{figure.caption.15}%
\contentsline {figure}{\numberline {2.10}{\ignorespaces The frame level detector takes frame queries and mask features, generates the embeddings, and pass onto the VITA model for mask prediction. Constructing the temporal interactions between the frame queries captures the object-aware knowledge in the spatial scenes. Finally, mask trajectories are obtained from the VITA model. Courtesy of [88]\relax }}{16}{figure.caption.16}%
\contentsline {figure}{\numberline {2.11}{\ignorespaces (a) MinVIS trained on query-based segmentation individually for every frame. (b) Inference of the video instance segmentation from the segmented image using bipartite matching of the query embeddings. Courtesy of [89] \relax }}{16}{figure.caption.17}%
\contentsline {figure}{\numberline {2.12}{\ignorespaces A mask2former with video instance segmentation. The architecture can also tackle the semantic and panoptic segmentation task. Courtesy of [90]\relax }}{17}{figure.caption.18}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Sample of Scannet dataset continuous frame rgb and semantic label. Courtesy of [91] \relax }}{22}{figure.caption.20}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Sample of Scannet dataset pose. Each point represent the pose of the camera in 3D frame when a particular frame is captured. The representation shows how the camera is moving in the 3D world.\relax }}{22}{figure.caption.21}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Scannet dataset class distribution. Horizontal axis represent the classes in Scannet dataset and vertical axis represent the normalized number of pixels per class. There are high number of pixels in entire data belonging to class 1 and class 2.\relax }}{23}{figure.caption.22}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces Sample of Virtual Kitti 2 dataset. Top row represent the continuous frame number followed by rgb and semantic labels. Courtesy of [93]\relax }}{24}{figure.caption.23}%
\contentsline {figure}{\numberline {3.5}{\ignorespaces Sample of Virtual Kitti 2 dataset pose. Each point represent the pose of the camera in 3D frame when a particular frame is captured. The representation shows how the camera is moving in the 3D world.\relax }}{25}{figure.caption.24}%
\contentsline {figure}{\numberline {3.6}{\ignorespaces RGB, Label and Pose sample of Scannet and Vkitti data\relax }}{26}{figure.caption.25}%
\contentsline {figure}{\numberline {3.7}{\ignorespaces Scannet data distribution. Horizontal axis represent the video sequence number and vertical axis represent the number of frames in each video sequence.\relax }}{26}{figure.caption.26}%
\contentsline {figure}{\numberline {3.8}{\ignorespaces Vkitti data distribution. Horizontal axis represent the scenes and under each scene there is 15-deg-left, fog, overcast, morning, 30-deg-right, 15-deg-right, 30-deg-left, clone, rain and sunset categories.\relax }}{27}{figure.caption.27}%
\contentsline {figure}{\numberline {3.9}{\ignorespaces Unet model architecture. Generated with modification \href {https://github.com/HarisIqbal88/PlotNeuralNet}{PlotNeuralNet}\relax }}{28}{figure.caption.28}%
\contentsline {figure}{\numberline {3.10}{\ignorespaces Unet model architecture with temporal fusion in latent space using Gaussian Process.The latent space encoding is propagated forward. During every computation the Gaussian Process takes latent space encoding, previous updated latent space and the current and previous pose to update the current latent space encoding. Generated with \href {https://app.diagrams.net/}{Diagrams.net}\relax }}{30}{figure.caption.29}%
\contentsline {figure}{\numberline {3.11}{\ignorespaces Unet model architecture with temporal fusion in latent space using the ConvLSTM cell. At every frames semantic label computation the previous convolutional LSTM cell hidden and cell state are passed to the current cell computation. Thereby modeling the temporal dependency in the consecutive frames. Generated with \href {https://app.diagrams.net/}{Diagrams.net}\relax }}{31}{figure.caption.30}%
\contentsline {figure}{\numberline {3.12}{\ignorespaces Training pipeline. Generated with \href {https://app.diagrams.net/}{Diagrams.net}\relax }}{32}{figure.caption.31}%
\contentsline {figure}{\numberline {3.13}{\ignorespaces Evaluation pipeline. Generated with \href {https://app.diagrams.net/}{Diagrams.net}\relax }}{33}{figure.caption.32}%
\contentsline {figure}{\numberline {3.14}{\ignorespaces Pictorial representation of training and evaluation procedure. The picture represent the how the training and evaluation is conducted with and without the temporal fusion. In the Vanilla network the model is trained on individual frames without temporal fusion and in the GP and LSTM model temporal data is propagated from one frame to another. Generated with \href {https://app.diagrams.net/}{Diagrams.net}\relax }}{34}{figure.caption.33}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Definition of IoU. Courtesy of \cite {82_iou}\relax }}{36}{figure.caption.34}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces Per class pixel distribution of the entire scannet dataset. X-axis represent the labels in the Scannet dataset and Y-axis represent the number of pixels per class. High number of pixels belongs to class 1 and class 2\relax }}{39}{figure.caption.35}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Pixel distribution for the scannet data containing all the classes\relax }}{39}{figure.caption.36}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces Pixel distribution for the scannet data for two classes. There are only two classes because all the classes except class 1 are combined with the 0th class. Horizontal axis represent the class and vertical axis represent the number of pixels belonging to that particular class.\relax }}{40}{figure.caption.37}%
\contentsline {figure}{\numberline {4.5}{\ignorespaces Pixel distribution for the scannet data for three classes. There are only three classes because all the classes except class 1 and 2 are combined with the 0th class. Horizontal axis represent the class and vertical axis represent the number of pixels belonging to that particular class.\relax }}{40}{figure.caption.38}%
\contentsline {figure}{\numberline {4.6}{\ignorespaces Ordered and Unordered set of images. Ordered set of images are the images from the video sequence without shuffling the frames. In unordered set of images the frames are shuffled. \relax }}{41}{figure.caption.39}%
\contentsline {figure}{\numberline {4.7}{\ignorespaces Distance matrix and Kernel matrix for ordered set of images. Distance matrix represent the distance between the consecutive eight frames poses with respect to each other. Kernel matrix represent the covariance between the consecutive eight frame poses.\relax }}{42}{figure.caption.40}%
\contentsline {figure}{\numberline {4.8}{\ignorespaces Distance matrix and Kernel matrix for unordered set of images. Distance matrix represent the distance between the shuffled eight frames poses with respect to each other. Kernel matrix represent the covariance between the shuffled eight frame poses.\relax }}{42}{figure.caption.41}%
\contentsline {figure}{\numberline {4.9}{\ignorespaces IoU for vanilla model considering all classes in dataset\relax }}{43}{figure.caption.43}%
\contentsline {figure}{\numberline {4.10}{\ignorespaces Pixel distribution for the ground truth and predicted scannet data for vanilla unet model\relax }}{43}{figure.caption.44}%
\contentsline {figure}{\numberline {4.11}{\ignorespaces IoU for GP model considering all classes in dataset\relax }}{44}{figure.caption.46}%
\contentsline {figure}{\numberline {4.12}{\ignorespaces Per class pixel distribution of the predicted pixel class label for gp model\relax }}{45}{figure.caption.47}%
\contentsline {figure}{\numberline {4.13}{\ignorespaces IoU for LSTM model considering all classes in dataset\relax }}{46}{figure.caption.49}%
\contentsline {figure}{\numberline {4.14}{\ignorespaces Per class pixel distribution of the predicted pixel class label for lstm model\relax }}{46}{figure.caption.50}%
\contentsline {figure}{\numberline {4.15}{\ignorespaces Comparison of VANILLA, GP and LSTM model performance based on accuracy metric. Higher the value means top performing model.\relax }}{47}{figure.caption.51}%
\contentsline {figure}{\numberline {4.16}{\ignorespaces Comparison of VANILLA, GP and LSTM model performance based on mean accuracy metric. Higher the value means top performing model.\relax }}{48}{figure.caption.52}%
\contentsline {figure}{\numberline {4.17}{\ignorespaces Comparison of VANILLA, GP and LSTM model performance based on meanIoU metric. Higher the value means top performing model.\relax }}{49}{figure.caption.53}%
\contentsline {figure}{\numberline {4.18}{\ignorespaces Comparison of VANILLA, GP and LSTM model performance based on FwIoU metric. Higher the value means top performing model.\relax }}{50}{figure.caption.54}%
\contentsline {figure}{\numberline {4.19}{\ignorespaces Plotting of raw input image, ground truth and predicted output for vanilla, gp and lstm model\relax }}{51}{figure.caption.55}%
\contentsline {figure}{\numberline {4.20}{\ignorespaces Plotting of raw input image, ground truth and predicted output for vanilla, gp and lstm model for two class scannet dataset\relax }}{52}{figure.caption.56}%
\contentsline {figure}{\numberline {4.21}{\ignorespaces IoU for vanilla, GP and LSTM comparison side by side for two classes in scannet dataset\relax }}{53}{figure.caption.58}%
\contentsline {figure}{\numberline {4.22}{\ignorespaces Comparison of VANILLA, GP and LSTM model performance based on accuracy metric for scannet two classes\relax }}{53}{figure.caption.59}%
\contentsline {figure}{\numberline {4.23}{\ignorespaces Comparison of VANILLA, GP and LSTM model performance based mean accuracy on metric for scannet two classes\relax }}{54}{figure.caption.60}%
\contentsline {figure}{\numberline {4.24}{\ignorespaces Comparison of VANILLA, GP and LSTM model performance based on meanIoU metric for scannet two classes\relax }}{55}{figure.caption.61}%
\contentsline {figure}{\numberline {4.25}{\ignorespaces Comparison of VANILLA, GP and LSTM model performance based on FwIoU metric for scannet two classes\relax }}{56}{figure.caption.62}%
\contentsline {figure}{\numberline {4.26}{\ignorespaces Plotting of raw input image, ground truth and predicted output for vanilla, gp and lstm model for three class scannet dataset\relax }}{57}{figure.caption.63}%
\contentsline {figure}{\numberline {4.27}{\ignorespaces IoU for vanilla, GP and LSTM comparison side by side for three classes in scannet dataset\relax }}{58}{figure.caption.65}%
\contentsline {figure}{\numberline {4.28}{\ignorespaces Comparison of VANILLA, GP and LSTM model performance based on accuracy metric for scannet three class scannet dataset. Higher the value means top performing model.\relax }}{58}{figure.caption.66}%
\contentsline {figure}{\numberline {4.29}{\ignorespaces Comparison of VANILLA, GP and LSTM model performance based on mean accuracy metric for three class scannet dataset. Higher the value means top performing model.\relax }}{59}{figure.caption.67}%
\contentsline {figure}{\numberline {4.30}{\ignorespaces Comparison of VANILLA, GP and LSTM model performance based on meanIoU metric for three class scannet dataset. Higher the value means top performing model.\relax }}{60}{figure.caption.68}%
\contentsline {figure}{\numberline {4.31}{\ignorespaces Comparison of VANILLA, GP and LSTM model performance based FwIoU on metric for three class scannet dataset. Higher the value means top performing model.\relax }}{61}{figure.caption.69}%
\contentsline {figure}{\numberline {4.32}{\ignorespaces Plotting of raw input image, ground truth and predicted output for vanilla, gp and lstm for vkitti dataset with fog and morning as the validation dataset\relax }}{63}{figure.caption.70}%
\contentsline {figure}{\numberline {4.33}{\ignorespaces IoU for vanilla model tested with vkitti dataset with fog and morning as the validation data\relax }}{64}{figure.caption.72}%
\contentsline {figure}{\numberline {4.34}{\ignorespaces IoU for GP model tested with vkitti dataset with fog and morning as the validation data\relax }}{64}{figure.caption.73}%
\contentsline {figure}{\numberline {4.35}{\ignorespaces IoU for LSTM model tested with vkitti dataset with fog and morning as the validation data\relax }}{65}{figure.caption.74}%
\contentsline {figure}{\numberline {4.36}{\ignorespaces Performance comparison of the VANILLA, GP and LSTM model for fog and morning as the validation data. Higher the value means top performing model.\relax }}{65}{figure.caption.75}%
\contentsline {figure}{\numberline {4.37}{\ignorespaces Side by side comparison of models predictions for continuous sequence data from frame 0 to 3\relax }}{66}{figure.caption.76}%
\contentsline {figure}{\numberline {4.38}{\ignorespaces Side by side comparison of models predictions for continuous sequence data from frame 4 to 7\relax }}{67}{figure.caption.77}%
\contentsline {figure}{\numberline {4.39}{\ignorespaces Impact of training batch size on the evaluation dataset prediction. The model is trained with batch of 2, 4 and 8. The performance of the model is plotted with different metrics. Higher the value means top performing model.\relax }}{68}{figure.caption.78}%
\contentsline {figure}{\numberline {4.40}{\ignorespaces Plotting of raw input image, ground truth and predicted output for Vanilla, GP and LSTM for vkitti dataset with five categories of dataset taken for validation.\relax }}{69}{figure.caption.79}%
\contentsline {figure}{\numberline {4.41}{\ignorespaces IoU for vanilla model with clone, sunset, rain, 15 degree right, and 30 degree left as validation data\relax }}{69}{figure.caption.81}%
\contentsline {figure}{\numberline {4.42}{\ignorespaces IoU for GP model with clone, sunset, rain, 15 degree right, and 30 degree left as validation data\relax }}{70}{figure.caption.82}%
\contentsline {figure}{\numberline {4.43}{\ignorespaces IoU for LSTM model with clone, sunset, rain, 15 degree right, and 30 degree left as validation data\relax }}{70}{figure.caption.83}%
\contentsline {figure}{\numberline {4.44}{\ignorespaces Comparison of Vanilla, GP, and LSTM model performance with clone, sunset, rain, 15-deg-right, 30-deg-left as the validation data. Higher the value means top performing model.\relax }}{71}{figure.caption.84}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Kotlin integration with python and sequence of steps from loading the image to display of predicted semantic map.\relax }}{76}{figure.caption.89}%
\contentsline {figure}{\numberline {5.2}{\ignorespaces Display of loading the image and predicted semantic map\relax }}{77}{figure.caption.91}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {C.1}{\ignorespaces Scannet step losses for vanilla\relax }}{99}{figure.caption.97}%
\contentsline {figure}{\numberline {C.2}{\ignorespaces Scannet epoch losses for vanilla\relax }}{100}{figure.caption.98}%
\contentsline {figure}{\numberline {C.3}{\ignorespaces Scannet step losses for GP\relax }}{100}{figure.caption.99}%
\contentsline {figure}{\numberline {C.4}{\ignorespaces Scannet epoch losses for GP\relax }}{100}{figure.caption.100}%
\contentsline {figure}{\numberline {C.5}{\ignorespaces Scannet step losses for lstm\relax }}{101}{figure.caption.101}%
\contentsline {figure}{\numberline {C.6}{\ignorespaces Scannet epoch losses for lstm\relax }}{101}{figure.caption.102}%
\contentsline {figure}{\numberline {C.7}{\ignorespaces Vkitti step loss vanilla\relax }}{101}{figure.caption.103}%
\contentsline {figure}{\numberline {C.8}{\ignorespaces Vkitti step loss GP\relax }}{102}{figure.caption.104}%
\contentsline {figure}{\numberline {C.9}{\ignorespaces Vkitti step loss lstm\relax }}{102}{figure.caption.105}%
\contentsline {figure}{\numberline {C.10}{\ignorespaces Learning rate finder\relax }}{102}{figure.caption.106}%
