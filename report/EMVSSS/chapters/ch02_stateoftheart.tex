%!TEX root = ../report.tex

\begin{document}
    \chapter{State of the Art}
	\label{chap:stateofart}
	
	Introduction to modern deep learning and their impact on the various vision tasks are described in the \nameref{sec:deeplearn} section. Information fusion in the temporal domain to fuse information is explained in \nameref{sec:tempfuse}. State-of-the-art segmentation of the input images, in particular, the semantic segmentation task, is illustrated in the \nameref{sec:semseg} section. State-of-the-art segmentation in the classical era and modern deep learning plays a crucial role in temporally fused semantic segmentation. However, there is very little work in fusing the camera pose into the segmentation task in a temporal fashion. More details are discussed in the \nameref{sec:semseg}. Finally, chapter \ref{chap:stateofart} is ended with a discussion on the limitations of the previous work concerning temporal fusion. 
	
    \section{Deep Learning}
    \label{sec:deeplearn}
    
	Deep learning is a subfield of machine learning that aims to learn the features present in the data by utilizing hierarchical architectures. The deep area learning falls in artificial intelligence is depicted in the picture \ref{fig:DLAI}
    
    \begin{figure}[h]
    	\centering
    	\includegraphics[width=10cm]{images/mldl.png}
    	\caption{Deep learning in the artificial intelligence domain. Courtesy of \cite{35_mldl}}
    	\label{fig:DLAI}
    \end{figure}  
    
    Classical machine learning system uses the raw input, and domain experts carefully represent the data as a feature vector from which the data is fed to the models to learn the patterns and classify them into appropriate classes \cite{36_lecun2015deep}. 
    Deep learning is a representation learning that takes raw data and finds the patterns in the data with different levels of representation in the multiple layers \cite{36_lecun2015deep}. Deep learning can learn any complex representation of the data. For example, an image is represented as pixels and is fed to the neural network; at each layer of the network, a different feature is learned. Higher-level features, such as edges at a specific orientation and location, are determined in the first layer. In the second layer, motifs are learned, and so on. The vital aspect of deep learning is that the features are not designed by the field expert but instead learned from the data with a specific set of learning procedures \cite{36_lecun2015deep}.
    
	Many current state-of-the-art learning models use the deep learning approach to learn a complex function from data. Currently, deep learning methods can be found in image recognition \cite{37_farabet2012learning}, speech technologies \cite{38_hinton2012deep}, the discovery of drug molecule \cite{39_patel2020machine}, understanding the particle accelerator data \cite{40_ciodaro2012online}, DNA sequencing \cite{41_zhang2021deep}, and natural language processing \cite{42_hirschberg2015advances}. 
    
    Computer vision is the field of computer science that deals with replicating the functionalities of the human visual system. Traditionally computer vision solved the vision problem by finding hand-crafted features. However, the performance of the classical approach is outperformed by the advancement of deep learning-based methods. Hand-crafted feature descriptors such as Speeded Up Robust Features (SURF) and Hough Transforms are used as feature vectors for the classical machine learning methods for learning \cite{46_o2019deep}. Deep learning methods automatically learn the patterns from the data. Computer vision solves a wide variety of problems in the perception domain. Latest approaches helps to solve the detection \cite{44_mohanty2016using}, \cite{45_han2021ecological}, classification \cite{43_srivastava2021comparative}, image synthesis and segmentation tasks \cite{47_minaee2021image}.
    
	Temporal data are time-varying information and can be commonly observed in financial portfolio management, accounting, medical records, inventory management, and data from the airline, hotel; train industries contain time components with it \cite{48_jensen1999temporal}. Video data are constructed by combining time variant frames and are a typical example of temporal data. Temporal fusion deals with combining past information into the current step computation with the aim of improved performance. 
    
	In general, approach segmentation is done frame by frame or by skipping between frames and computing the segmentation on the nth frame. Temporal fusion can be applied in these settings to improve segmentation by combining the past rich information in the current step. 
     
    \section{Temporal Fusion}
    \label{sec:tempfuse}
    
	Temporal fusion can be defined as the process of fusing the temporal data onto the current step to improve the model's performance. Temporal data can be observed in many fields, such as social media, healthcare, accounting, agriculture, transportation, physics, crime data, traffic dynamics, and climate science \cite{49_atluri2018spatio}. Temporal data can be encountered with different data types: video, audio, tabular data, and sensor data. Forecasting is a typical application of temporal fusion. Multi-horizon forecasting is an important problem in the domain of time series. Multi-horizon forecasts allow the user to optimize the process across the entire path. A novel Temporal fusion Transformer (TFT) \cite{50_lim2021temporal} is an attention-based DNN architecture for forecasting by fusing the important past features into the current step. Temporal fusion plays a significant role in improved video action recognition. Temporal fusion helps in two ways; firstly, by understanding the temporal data, the accuracy of the recognition for the dynamic action is improved; secondly, removing the redundant temporal data saves the computation overhead. A temporal fusion network known as the AdaFuse fuses the current and past features with a goal of improved accuracy and efficiency \cite{51_meng2021adafuse}. A temporal nonparametric fusion aims to fuse the temporal pose data to the computation of the depth map, thereby improving accuracy and efficiency \cite{52_hou2019multi}. The architecture of the multi-view stereo can be depicted in Fig \ref{fig:mvs}. An online multi-view depth prediction approach where the depth estimated in the previous step is fused onto the current step in a sensible manner. The network is named DeepVideoMVS, and it is based on the encoder-decoder architecture. A ConvLSTM is placed at the latent space to fuse the information from the previous step. The proposed approach outperformed all the existing state-of-the-art multi-view stereo methods evaluated on the standard metrics \cite{53_duzceker2021deepvideomvs}. 
	A Multiple Fusion Adaptation (MFA) method improves the segmentation accuracy on an unlabeled dataset. Three fusion approach was proposed under MFA, cross-model fusion, temporal fusion, and novel online-offline pseudo labels. The MFA produced improved semantic segmentation results of 58.2\% and 62.5\% on GTA5-to-Cityscapes and SYNTHIA-to-Cityscapes, respectively \cite{54_zhang2021multiple}.  
    
    \begin{figure}
    	\centering
    	\includegraphics[width=6cm]{images/MVS.png}
    	\caption{Multi-View Stereo by Temporal Nonparametric Fusion. Two consecutive frames are used to construct the cost volume and the output is passed onto the encoder to generate latent space encoding. The frames coordinates are transferred to the Gaussian Process to generate the updated latent space encoding. The Gaussian Process take encoder output, frame coordinates, and the propagated latent space encoding. The output of Gaussian Process is forwarded to decoder to generate the depth map. Courtesy of \cite{52_hou2019multi}}
    	\label{fig:mvs}
    \end{figure} 	

    \section{Semantic Segmentation}
    \label{sec:semseg}
    
    Humans can perceive the surrounding environment and make sense of it with high accuracy. Due to the advancement of computer vision, these capabilities are transferred to machines, performing even better than humans. Today, we have computer vision models that can detect objects, find shapes, track object movement and perform an action based on the data. Computer vision is most commonly used in autonomous driving cars, aerial mapping, surveillance applications, virtual and augmented reality, etc. One common problem in computer vision is labeling each pixel of the image to a particular category. Also known as segmentation. Mathematically image segmentation can be defined as

	If $I$ is a set of all image pixels of an image, then segmentation generates unique regions ${S_1, S_2, S_3, S_4,....S_n}$ such that combining all these regions will return $I$. 
	
	Image segmentation can be classified into three categories Semantic segmentation, Instance segmentation, and Panoptic segmentation. 
	Semantic segmentation finds the objects' shape, size, and form in addition to their location. Instance segmentation finds one more parameter of a number of unique objects in the image. Panoptic segmentation is the combination of semantic and instance segmentation. The difference between all the types of semantic segmentation can be observed in Fig \ref{fig:SS}.
    
    \begin{figure}[h]
    	\centering
    	\includegraphics[width=12cm]{images/ss.jpg}
    	\caption{Picture depicting the semantic, instance and panoptic segmentation examples. Courtesy of \cite{55_WinNT}}
    	\label{fig:SS}
    \end{figure}
    
    \subsection{Classical Semantic Segmentation}
    
	 Most commonly used traditional segmentation techniques are threshold-based technique \cite{56_otsu1979threshold}, histogram-based bundling, region-growing \cite{57_otsu1979threshold}, k-means clustering, watersheds, active contours, graph cuts, conditional and Markov random fields \cite{58_boykov2001fast}, sparsity-based methods \cite{59_starck2005image}. However, deep learning (DL) in recent years yielded a new generation of image segmentation models with state-of-the-art performance. 
    
    \subsection{Deep Learning based  Semantic Segmentation}
    
    Deep learning based segmentation network can be classified into following categories \cite{60_minaee2021image}
    
    \begin{itemize}
		\item Fully convolutional networks
		\item Convolutional models with graphical models
		\item Encoder-decoder-based models
		\item Multi-scale and pyramid network-based models
		\item R-CNN-based models (for instance segmentation)
		\item Dilated convolutional models and DeepLab family
		\item Recurrent neural network-based models
		\item Attention-based models
		\item Generative models and adversarial training
		\item Convolutional models with active contour models
	\end{itemize}
    
    Deep learning-based computer vision models most commonly use the convolutional neural network \cite{61_chen2017rethinking}, recurrent neural network (RNNs), and Long short term memory (LSTM), encoder-decoder \cite{62_badrinarayanan2017segnet}. Generative adversarial networks (GANs) based networks \cite{60_minaee2021image}. The master thesis work is concentrated on the encoder-decoder-based deep learning models. The encoder-Decoder-based network is a two-stage network that learns to map from the input point to the output point. In the encoder stage, the input data is compressed into a latent space representation $ z = f(x)$ and the decoder decompresses the latent space representation to the output $ a = g(z)$ \cite{63_goodfellow2014generative}. Latent representation of the input data in compressed form. It can be commonly observed in image-to-image translation problems and in sequence-to-sequence models in NLP. A reconstruction loss $ L(y, \hat{y})$ is defined at the output that measures the differences between the ground truth output $y$ and corresponding reconstruction $\hat{y}$. Autoencoders are the particular version of the encoder-decoder models with similar input and output.
    
    \begin{figure}[h]
    	\centering
    	\includegraphics[width=14cm]{images/en_de.png}
    	\caption{Plain encoder-decoder architecture with latent space encoding. Courtesy of \cite{60_minaee2021image}}
    	\label{fig:en_de}
    \end{figure} 		
    
	Most of the segmentation networks are encoder-decoder-based architecture. A novel semantic segmentation network was proposed by Noh et al. \cite{64_noh2015learning}. The network is based on deconvolution. The encoder network is based on the VGG 16-layer network, and the decoder network takes the latent space encoding and outputs the pixel-wise class probabilities. The segmentation mask and pixel-wise class labels are predicted by the deconvolutional and unpooling layers. The network generated an accuracy of 72.5 \% on the PASCAL VOC 2012 dataset. 
    
    \begin{figure}[h]
    	\centering
    	\includegraphics[width=14cm]{images/general_seg.png}
    	\caption{Simple encoder-decoder architecture with convolution and deconvolution network. Courtesy of \cite{64_noh2015learning}}
    	\label{fig:general_seg}
    \end{figure} 
    
    Badrinarayanan et al proposed a convolutional encoder-decoder architecture for image segmentation called as SegNet \cite{62_badrinarayanan2017segnet}. The architecture of the SegNet described in the Figure \ref{fig:segnet}  
    
    \begin{figure}[h]
    	\centering
    	\includegraphics[width=14cm]{images/segnet.png}
    	\caption{SegNet architecture with novel upsampling of latent space encoding. Courtesy of \cite{62_badrinarayanan2017segnet}}
    	\label{fig:segnet}
    \end{figure} 
    
    The encoder part of the SegNet consists of 13 convolutional layers in the VGG16 network, followed by the pixel-wise classification layer. Decoder uniquely upsamples the low-resolution feature maps. Non-linear upsampling is performed by using the pooling indices computed in the max-pooling step of the encoder. This process of reusing the encoder output helps to eliminate the need for learning to up-sample. Dense feature maps are generated by convolving with the trainable filters. To account for the uncertainty involved with the encoder-decoder network, scene segmentation is proposed \cite{65_kendall2015bayesian}. HRNet \cite{66_sun2019high} is the recently developed high-resolution network that connects the high to low-resolution convolutions streams in parallel	and exchanges information between different resolutions. HRNet maintains high-resolution representation through the encoding process. Many recent architectures use HRNet as the backbone. Other encoder decoder segmentation models are Stacked Deconvolutional Network \cite{67_fu2019stacked}, Linknet \cite{68_hu2018learning}, W-net \cite{69_xia2017w}.

	Many segmentation models are developed for medical applications, and among those, U-Net \cite{70_ronneberger2015u}, and V-Net \cite{71_milletari2016v} are the famous architecture. These architectures are now used outside of the medical domain.
    
    \begin{figure}[h]
    	\centering
    	\includegraphics[width=14cm]{images/unet.png}
    	\caption{Unet architecture. A semantic segmentation network for biomedical applications with the strong use of augmentation. The network consist of encoder that down samples the features and a decoder for up sampling. Courtesy of \cite{70_ronneberger2015u}}
    	\label{fig:unet}
    \end{figure} 
	
	 Ronneberger et al. \cite{70_ronneberger2015u} proposed a segmentation model to perform semantic segmentation on medical microscopy images. The architecture of the U-Net is described in Fig \ref{fig:unet}. The contracting part captures the context, and the expanding decoder path identifies the localization of the target area. The network heavily dependent on the annotated images efficiently. The encoder part has a 3x3 convolution features extractor, similar to the FCN-like architecture. The decoder part increases the dimensions and reduces the number of feature maps. The feature map from the encoder is mapped to the upscaled decoder to retain the pattern information. A 1x1 convolution at the output process the feature maps to generate segmentation output by categorizing each pixel of the input image to a particular class. Original U-Net was trained on the electronic microscopic images and outperformed by a large margin on the ISBI challenge. The network is fast and produces a result on 512x512 images in less than a second on the modern GPU \cite{70_ronneberger2015u}, \cite{60_minaee2021image}. In sequence data, the information from the previous frames can be utilized to segment the current frame with the aim of improved performance compared to the segmentation without the temporal fusion. 
	 
    \section{Temporal Fusion in Semantic Segmentation}
    
	Semantic segmentation of sequence data aims to assign pixel-wise semantic labels to the video frames. It is an essential task in visual understanding \cite{72_jin2017video}. A strong representation of the feature map is essential for the segmentation task. One common video segmentation approach is performing the image segmentation to each frame independently. However, this approach needs to capture the temporal information of the dynamic scenes. A standard solution to the problem is to apply semantic segmentation to every frame and add a layer on top to capture the temporal data to extract the better features \cite{73_gadde2017semantic}, \cite{74_jin2017video}, \cite{75_nilsson2018semantic}. However, such an approach does not help improve the performance as the feature needs to be computed at every frame. So a good approach is to apply the segmentation at keyframes and reuse the already computed features for the other frames \cite{76_jain2019accel}, \cite{77_mahasseni2017budget}. A new highly efficient and accuracy neural network-based model is developed for semantic video segmentation called Temporally Distributed Network (TDNet) \cite{78_hu2020temporally}. In the TDNet feature, extraction is distributed evenly across the sequential frames to eliminate the re-computation. Then these features are combined using the Attention Propagation Module (APM) to get the solid features for accurate segmentation \cite{78_hu2020temporally}. The pictorial representation of the same is described in Fig \ref{fig:TDNet}. 
    
    \begin{figure}[h]
    	\centering
    	\includegraphics[width=12cm]{images/TDNet.png}
    	\caption{TDNet is a efficient and accurate video segmentation framework. Courtesy of \cite{78_hu2020temporally}}
    	\label{fig:TDNet}
    \end{figure}  
    
    \section{RQ1: What are the works on state-of-the-art temporal fusion in semantic segmentation?}
    
    Segmentation is the process of assigning pixel labels for a given image. Segmentation can be observed in different contexts, such as Video instance segmentation (VIS), Semantic segmentation of the key frames of a video sequence, Video panoptic segmentation, MRI image segmentation, and autonomous driving scene understanding. Youtube - Video Instance Segmentation (VOS) and Scannet are the standard datasets in the semantic segmentation domain. 
    
    { \bf Youtube VIS data}
    Youtube - Video Instance Segmentation (VIS) is a dataset that extends the image instance segmentation from the image to the video domain. The dataset aims to solve the problem of simultaneous detection, segmentation, and tracking problem. Scannet is an RGB-D video dataset containing 2.5 million views with more than 1500 scans. 
    %	\ref{59_starck2005image}
    A paper by Xiang Li et al. [87] explains the temporal fusion for online video instance segmentation. The author introduced the concept of an online video framework with a novel, aware temporal fusion method. A cropping-free temporal fusion approach to model the temporal consistency between video frames. A bottom-up online transformer-based network is used to solve the VIS problem. A transformer layer is introduced in the convolutional neural network (CNN) to include the instance information. An attention layer between the frames is used to extract the instance code for the current frame. A skip connection uses low-level contextual information and dynamic convolution to generate the segmentation map—the CNN feature map and the latent code help to represent instance-aware features jointly. An instance code is an LxD vector to the VIS task, where L is the maximum detected instances number in a frame D is the feature instance for each instance. Inter and Intra frame attention module for fusing the temporal information. The network architecture is presented in Fig \ref{fig:vis}. Three types of attention are code-to-code(c2c), code-to-pixel(c2p), and pixel-to-code(p2c), used to construct the	relationship between the instance code and feature map. Interframes are used to build the temporal correlation and combine contextual features across frames. Interframe c2c, c2p, and p2c are used to construct the temporal information.	
    
    \begin{figure}
    	\centering
    	\includegraphics[width=13cm]{images/VIS.png}
    	\caption{VIS proposed framework.An online video instance segmentation framework with instance aware temporal fusion method. Intra frame attention framework for combining instance code and feature map. Inter-frame attention for fusing the hybrid temporal information from previous prediction stage. Courtesy of [87]}
    	\label{fig:vis}
    \end{figure}
    %	\ref{87_heo2022vita}
    Miran Heo et al. [88] worked on video instance segmentation via Object Token Association (VITA). The work effectively understands the video through the object-centric tokens. The VITA model requires a frame-level detector. The frame-level detector localizes instances using masks without bounding boxes. Two features are generated for the frame-level predictions: a) dynamic 1x1 convolution weight from the frame queries f b) per-pixel embeddings from the pixel decoder. The dot product between the two embeddings is taken in the frame-level predictions. The end-to-end video instance segmentation method VITA is divided into three stages. Firstly, VITA works with the frame-level detector in a frame-independent manner. No computation between the frames is involved. The frame queries that hold the object-centric information are collected throughout the video sequence, and an object encoder is used to build the video-level information between the frames. Moreover, thirdly the decoder combines the information from the frame and video queries which is finally used to find the object mask in the video frames. 
    
    \begin{figure}
    	\centering
    	\includegraphics[width=13cm]{images/VITA.png}
    	\caption{The frame level detector takes frame queries and mask features, generates the embeddings, and pass onto the VITA model for mask prediction. Constructing the temporal interactions between the frame queries captures the object-aware knowledge in the spatial scenes. Finally, mask trajectories are obtained from the VITA model. Courtesy of [88]}
    	\label{fig:vita}
    \end{figure}
    %	\ref{88_huang2022minvis}
    A minimal video instance segmentation (MinVIS) by De-An Huang et al. [89] does not require video-based training and can be applied directly to images containing sparse image instance segmentations annotations. MinVIS is a two-stage approach: 1) Independent image instance segmentation on each frame and 2) Instance association between the frames. Image Instance Segmentation has three unique main components a)Encoder to extract features from the image b) Transformer decoder, used to process the output of the image encoder to update the query embeddings c) Prediction head takes the query embeddings to predict the output. Association between the instance segmented frames is calculated by bipartite matching of the respective query embeddings.   
    
    \begin{figure}
    	\centering
    	\includegraphics[width=13cm]{images/minVIS.png}
    	\caption{(a) MinVIS trained on query-based segmentation individually for every frame. (b) Inference of the video instance segmentation from the segmented image using bipartite matching of the query embeddings. Courtesy of [89] }
    	\label{fig:minVIS}
    \end{figure}
    %	\ref{89_cheng2021mask2former}
    The state-of-the-art segmentation models solve mask classification in image segmentation problems. B. Cheng et al. [90] proposed a method to process the video directly rather than the image. This is achieved by feeding the Mask2Former with 3D spatiotemporal features and predicting the 3D volume to track each object instance across time. The Mask2Former works as a general image segmentation model for data with single frames and for data with more than one frame the model segments and tracks instances across frames. 
    
    \begin{figure}
    	\centering
    	\includegraphics[width=13cm]{images/mask2former.png}
    	\caption{ A mask2former with video instance segmentation. The architecture can also tackle the semantic and panoptic segmentation task. Courtesy of [90]}
    	\label{fig:mask2former}
    \end{figure}	
    
    \section{RQ1 Conclusion}
    
    There is much work with respect to temporal fusion. Temporal fusion can be done by taking the previous single feature data or multiple past features and combining them in the current stage. Feature combination can be done at multiple stages of the network. Subjecting the feature can be done in different ways, such as subjecting the feature to Gaussian Process, Long-Short-Term memory, combining the instance code with the encoder features, finding masks in the individual frame and combining them,or taking multiple features from the decoder and passing them to the transformer to model the temporal data. The approaches are made without considering the pose information. Hence, a new temporal fusion approach needs to be developed to model the temporal fusion data to improve performance. 
    
    \section{RQ2: How are the results from RQ1 compared with each other to perform temporal fusion?}
    
	In this research, various methods were employed for video instance segmentation with temporal fusion on the Youtube VIS 2019 and 2021 datasets. The results of these approaches' performance are shown in Table 2.1. The Hybrid instance-aware temporal fusion (HIATF) model performed exceptionally well, achieving an mAP (mean average precision) of 41.3 on the Youtube-VIS 2019 dataset. This was the best-performing model among those tested, and it even outperformed some models that used a ResNet-101 backbone. The HIATF model also demonstrated good performance when using a ResNet-50 backbone. However, other approaches were not effective when dealing with overlapping instances. The HIATF model also demonstrated good performance when using both a transformer and an instance code, as opposed to just using a transformer alone.
	
	Another approach, called Video instance segmentation via object token association (VITA), conducted experiments using both CNN-based (ResNet-50, ResNet-101) and transformer-based (Swin-L) backbones. These experiments were conducted both offline and online, with the online method having two advantages over the offline method. The offline method has a higher receptive field for temporal data, and error propagation can be mitigated through the use of hand-crafted association algorithms. The VITA approach achieved reasonable performance, with AP (average precision) values of 49.8 and 51.9 for the ResNet-50 and ResNet-101 backbones, respectively. The transformer-based Swin-L backbone outperformed all the other models, with an AP of 63.0.
	
	The Minimal Video Instance Segmentation Framework (MinVIS) approach also outperformed the state-of-the-art Mask2Former model, improving upon it by 1\% for both the ResNet and Swin-L backbones. On the Youtube data, which had relatively little temporal variation, the MinVIS model achieved AP values of 61.6 and 55.3 for the 2019 and 2021 datasets, respectively.
	
	The fourth approach, called the Mask2Former model, is a universal image segmentation model that was adapted for video segmentation by making minor changes such as applying masked attention to the spatiotemporal volume, adding extra positional encodings to the temporal dimension, and directly predicting a 3D volume rather than individual frames. This model produced AP values of 60.4 and 52.6 for the Youtube 2019 and 2021 data, respectively. 
    
    \begin{table}[h]
    	\begin{center}
    		\begin{tabular}{ | l | l| l| l| l| p{2cm} |}
    			\hline
    			
    			\cellcolor{purple!30}Method & \cellcolor{purple!30}AP & \cellcolor{purple!30}AP50 & \cellcolor{purple!30}AP75 & \cellcolor{purple!30}AR1 & \cellcolor{purple!30}AR10 \\ \hline
    			%   				\ref{86_li2022hybrid}
    			HIATF for VIS 2019 [87] & 41.3 & 61.5 & 43.5 & 42.7 & 47.8 \\ \hline
    			%   				\ref{87_heo2022vita}
    			VITA ResNet-50 2019 [88]  & 49.8 & 72.6 & 54.5 & 49.4 & 61.0  \\ \hline
    			%   				\ref{87_heo2022vita}
    			VITA ResNet-101 2019 [88]  & 51.9 & 75.4 & 57.0 & 49.6 & 59.1 \\ \hline
    			%   				\ref{87_heo2022vita}
    			VITA Swin-L 2019 [88]  & { \bf 63.0}  & { \bf 86.9} & { \bf 67.9} & { \bf 56.3} & { \bf 68.1} \\ \hline
    			%   				\ref{87_heo2022vita}
    			VITA 2021 [88]  & 45.7 & 67.4 & 49.5 & 40.9 & 53.6 \\ \hline
    			%   				\ref{88_huang2022minvis}
    			Min VIS 2019 [89] & 61.6 & 83.3 & 68.6 & 54.8 & 66.6 \\ \hline
    			%   				\ref{88_huang2022minvis}
    			Min VIS 2021 [89] & 55.3 & 76.6 & 62.0 & 45.9 & 60.8 \\ \hline
    			%   				\ref{89_cheng2021mask2former}
    			Mask2former 2019 [90]  & 60.4 & 84.4 & 67.0 & - & - \\ \hline
    			%   				\ref{89_cheng2021mask2former}   
    			Mask2former 2021 [90]  & 52.6 & 76.4 & 57.2 & - & - \\ \hline
    			\hline
    		\end{tabular}
    		\caption{Comparison of temporal fusion model performance with respect to Youtube VIS dataset}
    		\label{tab:sota_ytube_vis}
    	\end{center}
    \end{table}
    
    \section{RQ2 Conclusion}
    
    It is evident from the table \ref{tab:sota_ytube_vis} that temporal fusion with the VITA network outperformed the rest of the model performance with Swin-L as the backbone. The comparison is made with the average precision metric. Swin-L transformer is a general-purpose backbone for computer vision tasks. The representation is computed with the transformer. The comparison is made with the Youtube VIS dataset. The dataset does not contain the pose information. VIS network is a temporal fusion architecture containing inter and intra-frame fusion for a fusion of temporal data. From the result table \ref{tab:sota_ytube_vis}. The method of temporal fusion by taking only a single frame data from the previous frame is a better approach than computing the temporal fusion of all the data at a time.
    
    \section{Limitations of Previous Work}
    
    The perception system of the modern ADAS uses segmentation to understand the surrounding environment by capturing the environment with the help of modern cameras. The high FPS data collected by the camera are in a continuous sequence where every frame is related to its previous frames. In general, setting segmentation is performed on these frames to understand the object and its boundaries, the number of objects, types of objects present in the frame. A work by Hou et al. \cite{52_hou2019multi} integrates the camera pose data into the computation of the depth maps; however, a similar strategy is not studied for a segmentation task. Additionally, the study of using temporal data fusion in the encoder and decoder based network latent space with LSTM for semantic segmentation represents a novel approach for incorporating past frame information in the current step. Previous research has not explored the use of Gaussian Processes for temporal fusion in video instance segmentation, despite the potential benefits of using frame pose information. Additionally, there has been limited work on using Long Short Term Memory (LSTM) for semantic segmentation of video sequence data. This is an area that could be further investigated and could potentially lead to improved performance in video instance segmentation tasks. This thesis aims to study the impact of temporal pose data on the computation of the semantic segmentation and to take the previous frame data onto the current frame semantic segmentation task using the Gaussian Process and LSTM network is studied. 
    
\end{document}
