\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{35_mldl}
\citation{52_hou2019multi}
\citation{55_WinNT}
\citation{60_minaee2021image}
\citation{64_noh2015learning}
\citation{62_badrinarayanan2017segnet}
\citation{70_ronneberger2015u}
\citation{78_hu2020temporally}
\citation{82_iou}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{xv}{chapter*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{List of Tables}{xix}{chapter*.5}\protected@file@percent }
\citation{01_mandic2005data}
\citation{06_castanedo2013review}
\citation{02_lim2021temporal}
\citation{03_duzceker2021deepvideomvs}
\citation{04_li2021spatial}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:introduction}{{1}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation}{1}{section.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Data fusion categories\relax }}{1}{figure.caption.6}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:3D_reconstruction}{{1.1}{1}{Data fusion categories\relax }{figure.caption.6}{}}
\citation{005_hsiao2005temporal}
\citation{07_hsiao2005temporal}
\citation{07_hsiao2005temporal}
\citation{08_krause2003unsupervised}
\citation{09_lee2003line}
\citation{10_han2016seq}
\citation{11_kang2017t}
\citation{12_ning2017spatially}
\citation{13_lu2020retinatrack}
\citation{14_kopuklu2019you}
\citation{15_feichtenhofer2016convolutional}
\citation{16_wang2016temporal}
\citation{17_erccelik2021temp}
\citation{18_zhu2009spatial}
\citation{19_wu2003multi}
\citation{20_teutsch2012spatio}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Temporal fusion}{2}{subsection.1.1.1}\protected@file@percent }
\citation{21_forsyth2011computer}
\citation{22_dai2016instance}
\citation{23_fu1981survey}
\citation{24_ladys1994colour}
\citation{25_minaee2021image}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Semantic segmentation}{3}{subsection.1.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Challenges and Difficulties}{3}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Dataset}{3}{subsection.1.2.1}\protected@file@percent }
\citation{26_ronneberger2015u}
\citation{27_lin2017refinenet}
\citation{28_jegou2017one}
\citation{29_iandola2014densenet}
\citation{30_yang2018denseaspp}
\citation{25_minaee2021image}
\citation{31_chen2014semantic}
\citation{32_zhao2018icnet}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Fusion architecture}{4}{subsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Computation cost}{4}{subsection.1.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Real time inference for various application areas}{4}{subsection.1.2.4}\protected@file@percent }
\citation{33_deng2020lightweight}
\citation{34_hsieh2010real}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Use cases}{5}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Autonomous driving and Robotics}{5}{subsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Weed mapping using Unmanned Aerial Vehicle (UAV)}{5}{subsection.1.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Real-Time Hand Gesture Recognition}{5}{subsection.1.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Problem Statement and Contribution}{6}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Research question}{6}{subsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Contribution}{6}{subsection.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Report outline}{6}{section.1.5}\protected@file@percent }
\citation{35_mldl}
\citation{35_mldl}
\citation{36_lecun2015deep}
\citation{36_lecun2015deep}
\citation{36_lecun2015deep}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}State of the Art}{7}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:stateofart}{{2}{7}{State of the Art}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Deep Learning}{7}{section.2.1}\protected@file@percent }
\newlabel{sec:deeplearn}{{2.1}{7}{Deep Learning}{section.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Deep learning in the artificial intelligence domain. Courtesy of \cite  {35_mldl}\relax }}{7}{figure.caption.7}\protected@file@percent }
\newlabel{fig:DLAI}{{2.1}{7}{Deep learning in the artificial intelligence domain. Courtesy of \cite {35_mldl}\relax }{figure.caption.7}{}}
\citation{37_farabet2012learning}
\citation{38_hinton2012deep}
\citation{39_patel2020machine}
\citation{40_ciodaro2012online}
\citation{41_zhang2021deep}
\citation{42_hirschberg2015advances}
\citation{46_o2019deep}
\citation{44_mohanty2016using}
\citation{45_han2021ecological}
\citation{43_srivastava2021comparative}
\citation{47_minaee2021image}
\citation{48_jensen1999temporal}
\citation{49_atluri2018spatio}
\citation{50_lim2021temporal}
\citation{51_meng2021adafuse}
\citation{52_hou2019multi}
\citation{53_duzceker2021deepvideomvs}
\citation{54_zhang2021multiple}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Temporal Fusion}{8}{section.2.2}\protected@file@percent }
\newlabel{sec:tempfuse}{{2.2}{8}{Temporal Fusion}{section.2.2}{}}
\citation{52_hou2019multi}
\citation{52_hou2019multi}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Mulit view stereo architecture for depth estimation. Courtesy of \cite  {52_hou2019multi}\relax }}{9}{figure.caption.8}\protected@file@percent }
\newlabel{fig:mvs}{{2.2}{9}{Mulit view stereo architecture for depth estimation. Courtesy of \cite {52_hou2019multi}\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Semantic Segmentation}{9}{section.2.3}\protected@file@percent }
\newlabel{sec:semseg}{{2.3}{9}{Semantic Segmentation}{section.2.3}{}}
\citation{55_WinNT}
\citation{55_WinNT}
\citation{56_otsu1979threshold}
\citation{57_otsu1979threshold}
\citation{58_boykov2001fast}
\citation{59_starck2005image}
\citation{60_minaee2021image}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Semantic and Instance segmentation example. Courtesy of \cite  {55_WinNT}\relax }}{10}{figure.caption.9}\protected@file@percent }
\newlabel{fig:SS}{{2.3}{10}{Semantic and Instance segmentation example. Courtesy of \cite {55_WinNT}\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Classical Semantic Segmentation}{10}{subsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Deep Learning based Semantic Segmentation}{10}{subsection.2.3.2}\protected@file@percent }
\citation{61_chen2017rethinking}
\citation{62_badrinarayanan2017segnet}
\citation{60_minaee2021image}
\citation{63_goodfellow2014generative}
\citation{60_minaee2021image}
\citation{60_minaee2021image}
\citation{64_noh2015learning}
\citation{64_noh2015learning}
\citation{64_noh2015learning}
\citation{62_badrinarayanan2017segnet}
\citation{62_badrinarayanan2017segnet}
\citation{62_badrinarayanan2017segnet}
\citation{65_kendall2015bayesian}
\citation{66_sun2019high}
\citation{67_fu2019stacked}
\citation{68_hu2018learning}
\citation{69_xia2017w}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Simple encoder-decoder architecture. Courtesy of \cite  {60_minaee2021image}\relax }}{11}{figure.caption.10}\protected@file@percent }
\newlabel{fig:en_de}{{2.4}{11}{Simple encoder-decoder architecture. Courtesy of \cite {60_minaee2021image}\relax }{figure.caption.10}{}}
\citation{70_ronneberger2015u}
\citation{71_milletari2016v}
\citation{70_ronneberger2015u}
\citation{70_ronneberger2015u}
\citation{70_ronneberger2015u}
\citation{70_ronneberger2015u}
\citation{60_minaee2021image}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Simple encoder-decoder architecture. Courtesy of \cite  {64_noh2015learning}\relax }}{12}{figure.caption.11}\protected@file@percent }
\newlabel{fig:general_seg}{{2.5}{12}{Simple encoder-decoder architecture. Courtesy of \cite {64_noh2015learning}\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces SegNet architecture. Courtesy of \cite  {62_badrinarayanan2017segnet}\relax }}{12}{figure.caption.12}\protected@file@percent }
\newlabel{fig:segnet}{{2.6}{12}{SegNet architecture. Courtesy of \cite {62_badrinarayanan2017segnet}\relax }{figure.caption.12}{}}
\citation{72_jin2017video}
\citation{73_gadde2017semantic}
\citation{74_jin2017video}
\citation{75_nilsson2018semantic}
\citation{76_jain2019accel}
\citation{77_mahasseni2017budget}
\citation{78_hu2020temporally}
\citation{78_hu2020temporally}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Unet architecture. Courtesy of \cite  {70_ronneberger2015u}\relax }}{13}{figure.caption.13}\protected@file@percent }
\newlabel{fig:unet}{{2.7}{13}{Unet architecture. Courtesy of \cite {70_ronneberger2015u}\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Temporal Fusion in Semantic Segmentation}{13}{section.2.4}\protected@file@percent }
\citation{78_hu2020temporally}
\citation{78_hu2020temporally}
\citation{52_hou2019multi}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces TDNet. Courtesy of \cite  {78_hu2020temporally}\relax }}{14}{figure.caption.14}\protected@file@percent }
\newlabel{fig:TDNet}{{2.8}{14}{TDNet. Courtesy of \cite {78_hu2020temporally}\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Limitations of Previous Work}{14}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methodology}{15}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:methodology}{{3}{15}{Methodology}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Dataset}{15}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}ScanNet [91]}{15}{subsection.3.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Sample of Scannet dataset rgb and semantic label\relax }}{15}{figure.caption.15}\protected@file@percent }
\newlabel{fig:sample_rgb_seg_scannet}{{3.1}{15}{Sample of Scannet dataset rgb and semantic label\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Sample of Scannet dataset pose\relax }}{16}{figure.caption.16}\protected@file@percent }
\newlabel{fig:sample_pose_scannet}{{3.2}{16}{Sample of Scannet dataset pose\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Virtual KITTI 2 [93]}{16}{subsection.3.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Scannet dataset class distribution\relax }}{17}{figure.caption.17}\protected@file@percent }
\newlabel{fig:scannet_class_distribution}{{3.3}{17}{Scannet dataset class distribution\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Sample of Virtual Kitti 2 dataset\relax }}{17}{figure.caption.18}\protected@file@percent }
\newlabel{fig:sample_scannet_vkitti_2}{{3.4}{17}{Sample of Virtual Kitti 2 dataset\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Sample of Virtual Kitti 2 dataset pose\relax }}{18}{figure.caption.19}\protected@file@percent }
\newlabel{fig:sample_pose_scannet_vkitti_2}{{3.5}{18}{Sample of Virtual Kitti 2 dataset pose\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Data Collection and Preprocessing}{18}{section.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces RGB, Label and Pose dataset sample of scannet and vkitti data\relax }}{19}{figure.caption.20}\protected@file@percent }
\newlabel{fig:scannet_vkitti}{{3.6}{19}{RGB, Label and Pose dataset sample of scannet and vkitti data\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Experimental Design}{19}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}U-Net Vanilla model}{19}{subsection.3.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Scannet data distribution\relax }}{20}{figure.caption.21}\protected@file@percent }
\newlabel{fig:scannet_vkitti}{{3.7}{20}{Scannet data distribution\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}U-Net with Gaussian process}{20}{subsection.3.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Vkitti data distribution\relax }}{21}{figure.caption.22}\protected@file@percent }
\newlabel{fig:scannet_vkitti}{{3.8}{21}{Vkitti data distribution\relax }{figure.caption.22}{}}
\newlabel{eq:distance_matrix}{{3.1}{21}{U-Net with Gaussian process}{equation.3.3.1}{}}
\newlabel{eq:covariance_matrix}{{3.2}{21}{U-Net with Gaussian process}{equation.3.3.2}{}}
\newlabel{eq:gp}{{3.3}{21}{U-Net with Gaussian process}{equation.3.3.3}{}}
\newlabel{eq:encode_output}{{3.4}{21}{U-Net with Gaussian process}{equation.3.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Unet model architecture\relax }}{22}{figure.caption.23}\protected@file@percent }
\newlabel{fig:unet_model}{{3.9}{22}{Unet model architecture\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}U-Net with Long Short Term Memory (LSTM)}{22}{subsection.3.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Unet model architecture with temporal fusion in latent space using Gaussian Process\relax }}{23}{figure.caption.24}\protected@file@percent }
\newlabel{fig:unet_gp}{{3.10}{23}{Unet model architecture with temporal fusion in latent space using Gaussian Process\relax }{figure.caption.24}{}}
\newlabel{eq:it}{{3.5}{23}{U-Net with Long Short Term Memory (LSTM)}{equation.3.3.5}{}}
\newlabel{eq:ft}{{3.6}{23}{U-Net with Long Short Term Memory (LSTM)}{equation.3.3.6}{}}
\newlabel{eq:ot}{{3.7}{23}{U-Net with Long Short Term Memory (LSTM)}{equation.3.3.7}{}}
\newlabel{eq:gt}{{3.8}{23}{U-Net with Long Short Term Memory (LSTM)}{equation.3.3.8}{}}
\newlabel{eq:Ct}{{3.9}{23}{U-Net with Long Short Term Memory (LSTM)}{equation.3.3.9}{}}
\newlabel{eq:Ht}{{3.10}{23}{U-Net with Long Short Term Memory (LSTM)}{equation.3.3.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Training and Evaluation Pipeline}{23}{section.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Unet model architecture with temporal fusion in latent space using the ConvLSTM cell\relax }}{24}{figure.caption.25}\protected@file@percent }
\newlabel{fig:unet_lstm}{{3.11}{24}{Unet model architecture with temporal fusion in latent space using the ConvLSTM cell\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Training pipeline\relax }}{25}{figure.caption.26}\protected@file@percent }
\newlabel{fig:unet_training}{{3.12}{25}{Training pipeline\relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Hardware Configuration}{25}{section.3.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Evaluation pipeline\relax }}{26}{figure.caption.27}\protected@file@percent }
\newlabel{fig:unet_evaluation}{{3.13}{26}{Evaluation pipeline\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Pictorial representation of training and evaluation procedure\relax }}{27}{figure.caption.28}\protected@file@percent }
\newlabel{fig:unet_evaluation}{{3.14}{27}{Pictorial representation of training and evaluation procedure\relax }{figure.caption.28}{}}
\citation{79_dai2017scannet}
\citation{80_cabon2020vkitti2}
\citation{81_minodaRAL2021}
\citation{84_ulku2022survey}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Evaluation and Experimental Result}{29}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:evaluationandresult}{{4}{29}{Evaluation and Experimental Result}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Evaluation Metric}{29}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Pixel Accuracy}{29}{subsection.4.1.1}\protected@file@percent }
\citation{83_iou}
\citation{82_iou}
\citation{82_iou}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}IoU}{30}{subsection.4.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces IoU. Courtesy of \cite  {82_iou}\relax }}{30}{figure.caption.29}\protected@file@percent }
\newlabel{fig:IoU}{{4.1}{30}{IoU. Courtesy of \cite {82_iou}\relax }{figure.caption.29}{}}
\citation{84_ulku2022survey}
\citation{84_ulku2022survey}
\citation{78_hu2020temporally}
\citation{52_hou2019multi}
\citation{03_duzceker2021deepvideomvs}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Hypothesis}{31}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}RQ1: What are the works on state-of-the-art temporal fusion in semantic segmentation?}{31}{section.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces VIS proposed framework. Courtesy of [87]\relax }}{32}{figure.caption.30}\protected@file@percent }
\newlabel{fig:vis}{{4.2}{32}{VIS proposed framework. Courtesy of [87]\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces The frame level detector takes frame queries and mask features, generates the embeddings, and pass onto the VITA model for mask prediction. Constructing the temporal interactions between the frame queries captures the object-aware knowledge in the spatial scenes. Finally, mask trajectories are obtained from the VITA model. Courtesy of [88]\relax }}{33}{figure.caption.31}\protected@file@percent }
\newlabel{fig:vita}{{4.3}{33}{The frame level detector takes frame queries and mask features, generates the embeddings, and pass onto the VITA model for mask prediction. Constructing the temporal interactions between the frame queries captures the object-aware knowledge in the spatial scenes. Finally, mask trajectories are obtained from the VITA model. Courtesy of [88]\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces (a) MinVIS trained on query-based segmentation individually for every frame. (b) Inference of the video instance segmentation from the segmented image using bipartite matching of the query embeddings. Courtesy of [89] \relax }}{33}{figure.caption.32}\protected@file@percent }
\newlabel{fig:minVIS}{{4.4}{33}{(a) MinVIS trained on query-based segmentation individually for every frame. (b) Inference of the video instance segmentation from the segmented image using bipartite matching of the query embeddings. Courtesy of [89] \relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces  A mask2former with video instance segmentation. Courtesy of [90]\relax }}{34}{figure.caption.33}\protected@file@percent }
\newlabel{fig:mask2former}{{4.5}{34}{A mask2former with video instance segmentation. Courtesy of [90]\relax }{figure.caption.33}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}RQ1 Conclusion}{34}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}RQ2: How are the results from RQ1 compared with each other to perform temporal fusion?}{34}{section.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.6}RQ2 Conclusion}{34}{section.4.6}\protected@file@percent }
\citation{85_kag_challenge}
\citation{79_dai2017scannet}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Comparison of temporal fusion model performance with respect to Youtube VIS dataset\relax }}{35}{table.caption.34}\protected@file@percent }
\newlabel{tab:sota_ytube_vis}{{4.1}{35}{Comparison of temporal fusion model performance with respect to Youtube VIS dataset\relax }{table.caption.34}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}RQ3: How to cross-transfer the temporal fusion technique to semantic segmentation?}{35}{section.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.1}Combining scannet dataset classes for experiment}{35}{subsection.4.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Per class pixel distribution of the entire scannet dataset\relax }}{36}{figure.caption.35}\protected@file@percent }
\newlabel{fig:scannet_class}{{4.6}{36}{Per class pixel distribution of the entire scannet dataset\relax }{figure.caption.35}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.2}Distance and Kernel matrix}{36}{subsection.4.7.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Pixel distribution for the scannet data containing all the classes\relax }}{37}{figure.caption.36}\protected@file@percent }
\newlabel{fig:scannet_all_classes}{{4.7}{37}{Pixel distribution for the scannet data containing all the classes\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Pixel distribution for the scannet data for two classes\relax }}{37}{figure.caption.37}\protected@file@percent }
\newlabel{fig:scannet_two_classes}{{4.8}{37}{Pixel distribution for the scannet data for two classes\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Pixel distribution for the scannet data for three classes\relax }}{37}{figure.caption.38}\protected@file@percent }
\newlabel{fig:scannet_three_classes}{{4.9}{37}{Pixel distribution for the scannet data for three classes\relax }{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Ordered and Unordered set of images\relax }}{38}{figure.caption.39}\protected@file@percent }
\newlabel{fig:ordered_and_unordered_images}{{4.10}{38}{Ordered and Unordered set of images\relax }{figure.caption.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.3}Experiment1.1: U-Net Vanilla, GP and LSTM model considering all the classes}{38}{subsection.4.7.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Distance matrix and Kernel matrix for ordered set of images\relax }}{39}{figure.caption.40}\protected@file@percent }
\newlabel{fig:ordered_D_and_K}{{4.11}{39}{Distance matrix and Kernel matrix for ordered set of images\relax }{figure.caption.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces Distance matrix and Kernel matrix for unordered set of images\relax }}{39}{figure.caption.41}\protected@file@percent }
\newlabel{fig:unordered_D_and_K}{{4.12}{39}{Distance matrix and Kernel matrix for unordered set of images\relax }{figure.caption.41}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Vanilla model performance with respect to all the metrics. Higher values means top performing model.\relax }}{40}{table.caption.42}\protected@file@percent }
\newlabel{tab:caption}{{4.2}{40}{Vanilla model performance with respect to all the metrics. Higher values means top performing model.\relax }{table.caption.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces Pixel distribution for the ground truth and predicted scannet data for vanilla unet model\relax }}{40}{figure.caption.43}\protected@file@percent }
\newlabel{fig:y_gt_pred_vanilla}{{4.13}{40}{Pixel distribution for the ground truth and predicted scannet data for vanilla unet model\relax }{figure.caption.43}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces GP model performance with respect to all the metrics. Higher values means top performing model.\relax }}{41}{table.caption.44}\protected@file@percent }
\newlabel{tab:caption}{{4.3}{41}{GP model performance with respect to all the metrics. Higher values means top performing model.\relax }{table.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces Per class pixel distribution of the predicted pixel class label for gp model\relax }}{41}{figure.caption.45}\protected@file@percent }
\newlabel{fig:y_gt_and_predic_gp}{{4.14}{41}{Per class pixel distribution of the predicted pixel class label for gp model\relax }{figure.caption.45}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces LSTM model performance with respect to all the metrics. Higher values means top performing model.\relax }}{42}{table.caption.46}\protected@file@percent }
\newlabel{tab:caption}{{4.4}{42}{LSTM model performance with respect to all the metrics. Higher values means top performing model.\relax }{table.caption.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces Per class pixel distribution of the predicted pixel class label for lstm model\relax }}{42}{figure.caption.47}\protected@file@percent }
\newlabel{fig:y_gt_and_predic_lstm}{{4.15}{42}{Per class pixel distribution of the predicted pixel class label for lstm model\relax }{figure.caption.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.16}{\ignorespaces Comparison of VANILLA, GP and LSTM model performance based on accuracy metric. Higher the value means top performing model.\relax }}{43}{figure.caption.48}\protected@file@percent }
\newlabel{fig:unet_model_metric_comparison_all_classes}{{4.16}{43}{Comparison of VANILLA, GP and LSTM model performance based on accuracy metric. Higher the value means top performing model.\relax }{figure.caption.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.17}{\ignorespaces Comparison of VANILLA, GP and LSTM model performance based on mean accuracy metric. Higher the value means top performing model.\relax }}{44}{figure.caption.49}\protected@file@percent }
\newlabel{fig:unet_model_metric_comparison_all_classes}{{4.17}{44}{Comparison of VANILLA, GP and LSTM model performance based on mean accuracy metric. Higher the value means top performing model.\relax }{figure.caption.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.18}{\ignorespaces Comparison of VANILLA, GP and LSTM model performance based on meanIoU metric. Higher the value means top performing model.\relax }}{45}{figure.caption.50}\protected@file@percent }
\newlabel{fig:unet_model_metric_comparison_all_classes}{{4.18}{45}{Comparison of VANILLA, GP and LSTM model performance based on meanIoU metric. Higher the value means top performing model.\relax }{figure.caption.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.19}{\ignorespaces Comparison of VANILLA, GP and LSTM model performance based on FwIoU metric. Higher the value means top performing model.\relax }}{46}{figure.caption.51}\protected@file@percent }
\newlabel{fig:unet_model_metric_comparison_all_classes}{{4.19}{46}{Comparison of VANILLA, GP and LSTM model performance based on FwIoU metric. Higher the value means top performing model.\relax }{figure.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.20}{\ignorespaces Plotting of raw input image, ground truth and predicted output for vanilla, gp and lstm model\relax }}{46}{figure.caption.52}\protected@file@percent }
\newlabel{fig:unet_model}{{4.20}{46}{Plotting of raw input image, ground truth and predicted output for vanilla, gp and lstm model\relax }{figure.caption.52}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.4}Experiment1.2: U-Net Vanilla, Temporally fused gp model and LSTM model considering two classes}{47}{subsection.4.7.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.21}{\ignorespaces Plotting of raw input image, ground truth and predicted output for vanilla, gp and lstm model for two class dataset\relax }}{47}{figure.caption.53}\protected@file@percent }
\newlabel{fig:output_vanilla}{{4.21}{47}{Plotting of raw input image, ground truth and predicted output for vanilla, gp and lstm model for two class dataset\relax }{figure.caption.53}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.5}Experiment1.3: U-Net Vanilla, temporally fused gp and lstm model considering three classes}{47}{subsection.4.7.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.22}{\ignorespaces Comparison of VANILLA, GP and LSTM model performance based on accuracy metric for scannet two classes\relax }}{48}{figure.caption.55}\protected@file@percent }
\newlabel{fig:unet_model_metric_comparison}{{4.22}{48}{Comparison of VANILLA, GP and LSTM model performance based on accuracy metric for scannet two classes\relax }{figure.caption.55}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.23}{\ignorespaces Comparison of VANILLA, GP and LSTM model performance based mean accuracy on metric for scannet two classes\relax }}{49}{figure.caption.56}\protected@file@percent }
\newlabel{fig:unet_model_metric_comparison}{{4.23}{49}{Comparison of VANILLA, GP and LSTM model performance based mean accuracy on metric for scannet two classes\relax }{figure.caption.56}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.24}{\ignorespaces Comparison of VANILLA, GP and LSTM model performance based on meanIoU metric for scannet two classes\relax }}{50}{figure.caption.57}\protected@file@percent }
\newlabel{fig:unet_model_metric_comparison}{{4.24}{50}{Comparison of VANILLA, GP and LSTM model performance based on meanIoU metric for scannet two classes\relax }{figure.caption.57}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.25}{\ignorespaces Comparison of VANILLA, GP and LSTM model performance based on FwIoU metric for scannet two classes\relax }}{51}{figure.caption.58}\protected@file@percent }
\newlabel{fig:unet_model_metric_comparison}{{4.25}{51}{Comparison of VANILLA, GP and LSTM model performance based on FwIoU metric for scannet two classes\relax }{figure.caption.58}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Performance of Vanilla model with respect to different metric and two classes. Higher the value means top performing model.\relax }}{52}{table.caption.54}\protected@file@percent }
\newlabel{table:unet_two_classes}{{4.5}{52}{Performance of Vanilla model with respect to different metric and two classes. Higher the value means top performing model.\relax }{table.caption.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.26}{\ignorespaces Plotting of raw input image, ground truth and predicted output for vanilla, gp and lstm model\relax }}{52}{figure.caption.59}\protected@file@percent }
\newlabel{fig:performance_metric_three_classes_scannet}{{4.26}{52}{Plotting of raw input image, ground truth and predicted output for vanilla, gp and lstm model\relax }{figure.caption.59}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.27}{\ignorespaces Comparison of VANILLA, GP and LSTM model performance based on accuracy metric for scannet three classes. Higher the value means top performing model.\relax }}{53}{figure.caption.61}\protected@file@percent }
\newlabel{fig:performance_metric_three_classes_unet}{{4.27}{53}{Comparison of VANILLA, GP and LSTM model performance based on accuracy metric for scannet three classes. Higher the value means top performing model.\relax }{figure.caption.61}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.28}{\ignorespaces Comparison of VANILLA, GP and LSTM model performance based on mean accuracy metric for scannet three classes. Higher the value means top performing model.\relax }}{54}{figure.caption.62}\protected@file@percent }
\newlabel{fig:performance_metric_three_classes_unet}{{4.28}{54}{Comparison of VANILLA, GP and LSTM model performance based on mean accuracy metric for scannet three classes. Higher the value means top performing model.\relax }{figure.caption.62}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.29}{\ignorespaces Comparison of VANILLA, GP and LSTM model performance based on meanIoU metric for scannet three classes. Higher the value means top performing model.\relax }}{55}{figure.caption.63}\protected@file@percent }
\newlabel{fig:performance_metric_three_classes_unet}{{4.29}{55}{Comparison of VANILLA, GP and LSTM model performance based on meanIoU metric for scannet three classes. Higher the value means top performing model.\relax }{figure.caption.63}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.30}{\ignorespaces Comparison of VANILLA, GP and LSTM model performance based FwIoU on metric for scannet three classes. Higher the value means top performing model.\relax }}{56}{figure.caption.64}\protected@file@percent }
\newlabel{fig:performance_metric_three_classes_unet}{{4.30}{56}{Comparison of VANILLA, GP and LSTM model performance based FwIoU on metric for scannet three classes. Higher the value means top performing model.\relax }{figure.caption.64}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces Performance of Vanilla, GP and LSTM model with respect to different metric and two classes. Higher the value means top performing model.\relax }}{57}{table.caption.60}\protected@file@percent }
\newlabel{table:unet_scannet_three_classes}{{4.6}{57}{Performance of Vanilla, GP and LSTM model with respect to different metric and two classes. Higher the value means top performing model.\relax }{table.caption.60}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Experiment 2.1: Experiment with vkitti dataset with fog and morning as the testing data}{57}{section.4.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.31}{\ignorespaces Plotting of raw input image, ground truth and predicted output for vanilla, gp and lstm for vkitti dataset\relax }}{58}{figure.caption.65}\protected@file@percent }
\newlabel{fig:vkitti_unet_two}{{4.31}{58}{Plotting of raw input image, ground truth and predicted output for vanilla, gp and lstm for vkitti dataset\relax }{figure.caption.65}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.1}Experiment 2.1.2: Impact of training data batch size on the evaluation dataset performance}{58}{subsection.4.8.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces Performance of Vanilla model with respect to different metric. Higher the value means top performing model.\relax }}{58}{table.caption.66}\protected@file@percent }
\newlabel{table:unet_vkitti_two_classes}{{4.7}{58}{Performance of Vanilla model with respect to different metric. Higher the value means top performing model.\relax }{table.caption.66}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.32}{\ignorespaces Performance comparison of the VANILLA, GP and LSTM model based on accuracy metrics. Higher the value means top performing model.\relax }}{59}{figure.caption.67}\protected@file@percent }
\newlabel{fig:performance_metric_vkitti_two_class_box_plot}{{4.32}{59}{Performance comparison of the VANILLA, GP and LSTM model based on accuracy metrics. Higher the value means top performing model.\relax }{figure.caption.67}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.33}{\ignorespaces Performance comparison of the VANILLA, GP and LSTM model based on mean accuracy metrics. Higher the value means top performing model.\relax }}{59}{figure.caption.68}\protected@file@percent }
\newlabel{fig:performance_metric_vkitti_two_class_box_plot}{{4.33}{59}{Performance comparison of the VANILLA, GP and LSTM model based on mean accuracy metrics. Higher the value means top performing model.\relax }{figure.caption.68}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.34}{\ignorespaces Performance comparison of the VANILLA, GP and LSTM model based on mIoU metrics. Higher the value means top performing model.\relax }}{60}{figure.caption.69}\protected@file@percent }
\newlabel{fig:performance_metric_vkitti_two_class_box_plot}{{4.34}{60}{Performance comparison of the VANILLA, GP and LSTM model based on mIoU metrics. Higher the value means top performing model.\relax }{figure.caption.69}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.35}{\ignorespaces Performance comparison of the VANILLA, GP and LSTM model based on FwIoU metrics. Higher the value means top performing model.\relax }}{60}{figure.caption.70}\protected@file@percent }
\newlabel{fig:performance_metric_vkitti_two_class_box_plot}{{4.35}{60}{Performance comparison of the VANILLA, GP and LSTM model based on FwIoU metrics. Higher the value means top performing model.\relax }{figure.caption.70}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.36}{\ignorespaces IoU predictions for all the classes for VANILLA model. Higher the value means top performing model.\relax }}{61}{figure.caption.71}\protected@file@percent }
\newlabel{fig:performance_metric_three_classes}{{4.36}{61}{IoU predictions for all the classes for VANILLA model. Higher the value means top performing model.\relax }{figure.caption.71}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.37}{\ignorespaces IoU predictions for all the classes for GP model. Higher the value means top performing model.\relax }}{62}{figure.caption.72}\protected@file@percent }
\newlabel{fig:performance_metric_three_classes}{{4.37}{62}{IoU predictions for all the classes for GP model. Higher the value means top performing model.\relax }{figure.caption.72}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.38}{\ignorespaces IoU predictions for all the classes for LSTM model. Higher the value means top performing model.\relax }}{63}{figure.caption.73}\protected@file@percent }
\newlabel{fig:performance_metric_three_classes}{{4.38}{63}{IoU predictions for all the classes for LSTM model. Higher the value means top performing model.\relax }{figure.caption.73}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.39}{\ignorespaces Side by side comparison of models predictions for continuous sequence data from frame 1 to 4\relax }}{64}{figure.caption.74}\protected@file@percent }
\newlabel{fig:performance_metric_three_classes}{{4.39}{64}{Side by side comparison of models predictions for continuous sequence data from frame 1 to 4\relax }{figure.caption.74}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.40}{\ignorespaces Side by side comparison of models predictions for continuous sequence data from frame 5 to 8\relax }}{65}{figure.caption.75}\protected@file@percent }
\newlabel{fig:performance_metric_three_classes}{{4.40}{65}{Side by side comparison of models predictions for continuous sequence data from frame 5 to 8\relax }{figure.caption.75}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.41}{\ignorespaces Impact of training batch size on the evaluation dataset prediction. Higher the value means top performing model.\relax }}{66}{figure.caption.76}\protected@file@percent }
\newlabel{fig:batch_wise_performance}{{4.41}{66}{Impact of training batch size on the evaluation dataset prediction. Higher the value means top performing model.\relax }{figure.caption.76}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.9}Experiment 2.2: Experiment with vkitti dataset with clone, sunset, rain, 15-deg-right, and 30-deg-left as the testing data}{66}{section.4.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.42}{\ignorespaces Plotting of raw input image, ground truth and predicted output for vanilla, gp and lstm\relax }}{67}{figure.caption.77}\protected@file@percent }
\newlabel{fig:unet_side_by_side_five_classes}{{4.42}{67}{Plotting of raw input image, ground truth and predicted output for vanilla, gp and lstm\relax }{figure.caption.77}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.8}{\ignorespaces Performance of Vanilla model with respect to different metric and two classes. Higher the value means top performing model.\relax }}{67}{table.caption.78}\protected@file@percent }
\newlabel{table:Vanilla_conti_seq}{{4.8}{67}{Performance of Vanilla model with respect to different metric and two classes. Higher the value means top performing model.\relax }{table.caption.78}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.43}{\ignorespaces Comparison of model performance based on accuracy metrics. Higher the value means top performing model.\relax }}{68}{figure.caption.79}\protected@file@percent }
\newlabel{fig:performance_metric_unet}{{4.43}{68}{Comparison of model performance based on accuracy metrics. Higher the value means top performing model.\relax }{figure.caption.79}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.10}RQ3: Conclusion}{68}{section.4.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.44}{\ignorespaces Comparison of model performance based on mean accuracy metrics. Higher the value means top performing model.\relax }}{69}{figure.caption.80}\protected@file@percent }
\newlabel{fig:performance_metric_unet}{{4.44}{69}{Comparison of model performance based on mean accuracy metrics. Higher the value means top performing model.\relax }{figure.caption.80}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.11}Experiment hyper-parameter and configuration detail }{69}{section.4.11}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.9}{\ignorespaces Hyperparameter used to train the model.\relax }}{69}{table.caption.84}\protected@file@percent }
\newlabel{table:hype}{{4.9}{69}{Hyperparameter used to train the model.\relax }{table.caption.84}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.45}{\ignorespaces Comparison of model performance based on mIoU metrics. Higher the value means top performing model.\relax }}{70}{figure.caption.81}\protected@file@percent }
\newlabel{fig:performance_metric_unet}{{4.45}{70}{Comparison of model performance based on mIoU metrics. Higher the value means top performing model.\relax }{figure.caption.81}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.46}{\ignorespaces Comparison of model performance based on FwIoU metrics. Higher the value means top performing model.\relax }}{70}{figure.caption.82}\protected@file@percent }
\newlabel{fig:performance_metric_unet}{{4.46}{70}{Comparison of model performance based on FwIoU metrics. Higher the value means top performing model.\relax }{figure.caption.82}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.47}{\ignorespaces IoU for all the classes in the dataset with respect to different model predictions. Higher the value means top performing model.\relax }}{71}{figure.caption.83}\protected@file@percent }
\newlabel{fig:performance_metric_three_classes_unet_five}{{4.47}{71}{IoU for all the classes in the dataset with respect to different model predictions. Higher the value means top performing model.\relax }{figure.caption.83}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Android Deployment}{73}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:androiddeploy}{{5}{73}{Android Deployment}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Framework}{73}{section.5.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Performance of Vanilla model with respect to different metric and two classes\relax }}{73}{table.caption.86}\protected@file@percent }
\newlabel{table:android_spec}{{5.1}{73}{Performance of Vanilla model with respect to different metric and two classes\relax }{table.caption.86}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Training and evaluation procedure\relax }}{74}{figure.caption.85}\protected@file@percent }
\newlabel{fig:android_pipeline}{{5.1}{74}{Training and evaluation procedure\relax }{figure.caption.85}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Android deployment results\relax }}{75}{figure.caption.87}\protected@file@percent }
\newlabel{fig:android_result}{{5.2}{75}{Android deployment results\relax }{figure.caption.87}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Display of results}{75}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Runtime}{76}{section.5.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Runtime detail for processing of single image\relax }}{76}{table.caption.88}\protected@file@percent }
\newlabel{table:android_spec}{{5.2}{76}{Runtime detail for processing of single image\relax }{table.caption.88}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusions}{77}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:conclusion}{{6}{77}{Conclusions}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Contributions}{78}{section.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Limitation}{79}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Future work}{79}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Appendix \numberline {A}Design Details}{81}{appendix.a.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces Unet vanilla model architecture\relax }}{82}{table.caption.89}\protected@file@percent }
\newlabel{table:unet_vanilla_model_1}{{A.1}{82}{Unet vanilla model architecture\relax }{table.caption.89}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.2}{\ignorespaces Unet vanilla model architecture\relax }}{83}{table.caption.90}\protected@file@percent }
\newlabel{table:unet_vanilla_model_2}{{A.2}{83}{Unet vanilla model architecture\relax }{table.caption.90}{}}
\@writefile{toc}{\contentsline {chapter}{Appendix \numberline {B}Parameters}{85}{appendix.a.B}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces Classes and ids of the Scannet dataset\relax }}{86}{table.caption.91}\protected@file@percent }
\newlabel{table:Classes in scannet_1}{{B.1}{86}{Classes and ids of the Scannet dataset\relax }{table.caption.91}{}}
\@writefile{lot}{\contentsline {table}{\numberline {B.2}{\ignorespaces Classes and ids of the Scannet dataset\relax }}{87}{table.caption.92}\protected@file@percent }
\newlabel{table:Classes in scannet_2}{{B.2}{87}{Classes and ids of the Scannet dataset\relax }{table.caption.92}{}}
\@writefile{lot}{\contentsline {table}{\numberline {B.3}{\ignorespaces Classes and ids of the Scannet dataset\relax }}{88}{table.caption.93}\protected@file@percent }
\newlabel{table:Classes in scannet_3}{{B.3}{88}{Classes and ids of the Scannet dataset\relax }{table.caption.93}{}}
\@writefile{toc}{\contentsline {chapter}{Appendix \numberline {C}Training details}{89}{appendix.a.C}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {C.1}{\ignorespaces Scannet step losses for vanilla\relax }}{89}{figure.caption.94}\protected@file@percent }
\newlabel{fig:android_result}{{C.1}{89}{Scannet step losses for vanilla\relax }{figure.caption.94}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {C.2}{\ignorespaces Scannet epoch losses for vanilla\relax }}{90}{figure.caption.95}\protected@file@percent }
\newlabel{fig:android_result}{{C.2}{90}{Scannet epoch losses for vanilla\relax }{figure.caption.95}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {C.3}{\ignorespaces Scannet step losses for GP\relax }}{90}{figure.caption.96}\protected@file@percent }
\newlabel{fig:android_result}{{C.3}{90}{Scannet step losses for GP\relax }{figure.caption.96}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {C.4}{\ignorespaces Scannet epoch losses for GP\relax }}{90}{figure.caption.97}\protected@file@percent }
\newlabel{fig:android_result}{{C.4}{90}{Scannet epoch losses for GP\relax }{figure.caption.97}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {C.5}{\ignorespaces Scannet step losses for lstm\relax }}{91}{figure.caption.98}\protected@file@percent }
\newlabel{fig:android_result}{{C.5}{91}{Scannet step losses for lstm\relax }{figure.caption.98}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {C.6}{\ignorespaces Scannet epoch losses for lstm\relax }}{91}{figure.caption.99}\protected@file@percent }
\newlabel{fig:android_result}{{C.6}{91}{Scannet epoch losses for lstm\relax }{figure.caption.99}{}}
\bibstyle{unsrt}
\bibdata{references}
\bibcite{35_mldl}{1}
\bibcite{52_hou2019multi}{2}
\bibcite{55_WinNT}{3}
\bibcite{60_minaee2021image}{4}
\bibcite{64_noh2015learning}{5}
\bibcite{62_badrinarayanan2017segnet}{6}
\bibcite{70_ronneberger2015u}{7}
\bibcite{78_hu2020temporally}{8}
\bibcite{82_iou}{9}
\bibcite{01_mandic2005data}{10}
\bibcite{06_castanedo2013review}{11}
\bibcite{02_lim2021temporal}{12}
\@writefile{toc}{\contentsline {chapter}{References}{93}{appendix*.100}\protected@file@percent }
\bibcite{03_duzceker2021deepvideomvs}{13}
\bibcite{04_li2021spatial}{14}
\bibcite{005_hsiao2005temporal}{15}
\bibcite{07_hsiao2005temporal}{16}
\bibcite{08_krause2003unsupervised}{17}
\bibcite{09_lee2003line}{18}
\bibcite{10_han2016seq}{19}
\bibcite{11_kang2017t}{20}
\bibcite{12_ning2017spatially}{21}
\bibcite{13_lu2020retinatrack}{22}
\bibcite{14_kopuklu2019you}{23}
\bibcite{15_feichtenhofer2016convolutional}{24}
\bibcite{16_wang2016temporal}{25}
\bibcite{17_erccelik2021temp}{26}
\bibcite{18_zhu2009spatial}{27}
\bibcite{19_wu2003multi}{28}
\bibcite{20_teutsch2012spatio}{29}
\bibcite{21_forsyth2011computer}{30}
\bibcite{22_dai2016instance}{31}
\bibcite{23_fu1981survey}{32}
\bibcite{24_ladys1994colour}{33}
\bibcite{25_minaee2021image}{34}
\bibcite{26_ronneberger2015u}{35}
\bibcite{27_lin2017refinenet}{36}
\bibcite{28_jegou2017one}{37}
\bibcite{29_iandola2014densenet}{38}
\bibcite{30_yang2018denseaspp}{39}
\bibcite{31_chen2014semantic}{40}
\bibcite{32_zhao2018icnet}{41}
\bibcite{33_deng2020lightweight}{42}
\bibcite{34_hsieh2010real}{43}
\bibcite{36_lecun2015deep}{44}
\bibcite{37_farabet2012learning}{45}
\bibcite{38_hinton2012deep}{46}
\bibcite{39_patel2020machine}{47}
\bibcite{40_ciodaro2012online}{48}
\bibcite{41_zhang2021deep}{49}
\bibcite{42_hirschberg2015advances}{50}
\bibcite{46_o2019deep}{51}
\bibcite{44_mohanty2016using}{52}
\bibcite{45_han2021ecological}{53}
\bibcite{43_srivastava2021comparative}{54}
\bibcite{47_minaee2021image}{55}
\bibcite{48_jensen1999temporal}{56}
\bibcite{49_atluri2018spatio}{57}
\bibcite{50_lim2021temporal}{58}
\bibcite{51_meng2021adafuse}{59}
\bibcite{53_duzceker2021deepvideomvs}{60}
\bibcite{54_zhang2021multiple}{61}
\bibcite{56_otsu1979threshold}{62}
\bibcite{57_otsu1979threshold}{63}
\bibcite{58_boykov2001fast}{64}
\bibcite{59_starck2005image}{65}
\bibcite{61_chen2017rethinking}{66}
\bibcite{63_goodfellow2014generative}{67}
\bibcite{65_kendall2015bayesian}{68}
\bibcite{66_sun2019high}{69}
\bibcite{67_fu2019stacked}{70}
\bibcite{68_hu2018learning}{71}
\bibcite{69_xia2017w}{72}
\bibcite{71_milletari2016v}{73}
\bibcite{72_jin2017video}{74}
\bibcite{73_gadde2017semantic}{75}
\bibcite{74_jin2017video}{76}
\bibcite{75_nilsson2018semantic}{77}
\bibcite{76_jain2019accel}{78}
\bibcite{77_mahasseni2017budget}{79}
\bibcite{79_dai2017scannet}{80}
\bibcite{80_cabon2020vkitti2}{81}
\bibcite{81_minodaRAL2021}{82}
\bibcite{84_ulku2022survey}{83}
\bibcite{83_iou}{84}
\bibcite{85_kag_challenge}{85}
\bibcite{05_hsiao2005temporal}{86}
\bibcite{86_li2022hybrid}{87}
\bibcite{87_heo2022vita}{88}
\bibcite{88_huang2022minvis}{89}
\bibcite{89_cheng2021mask2former}{90}
\bibcite{90_dai2017scannet}{91}
\bibcite{91_ScanNet}{92}
\bibcite{92_cabon2020virtual}{93}
\bibcite{93_ScanNet}{94}
\bibcite{94_ScanNet}{95}
\bibcite{95_mazzotti2016measure}{96}
\bibcite{96_GPML}{97}
\bibcite{97_williams2006gaussian}{98}
\bibcite{98_ConvLSTM}{99}
\bibcite{99_shi2015convolutional}{100}
\citation{*}
