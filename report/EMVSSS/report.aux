\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{35_mldl}
\citation{52_hou2019multi}
\citation{55_WinNT}
\citation{60_minaee2021image}
\citation{64_noh2015learning}
\citation{62_badrinarayanan2017segnet}
\citation{70_ronneberger2015u}
\citation{78_hu2020temporally}
\citation{82_iou}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{xv}{chapter*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{List of Tables}{xvii}{chapter*.5}\protected@file@percent }
\citation{01_mandic2005data}
\citation{06_castanedo2013review}
\citation{02_lim2021temporal}
\citation{03_duzceker2021deepvideomvs}
\citation{04_li2021spatial}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation}{1}{section.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Data fusion categories\relax }}{1}{figure.caption.6}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:3D_reconstruction}{{1.1}{1}{Data fusion categories\relax }{figure.caption.6}{}}
\citation{005_hsiao2005temporal}
\citation{07_hsiao2005temporal}
\citation{07_hsiao2005temporal}
\citation{08_krause2003unsupervised}
\citation{09_lee2003line}
\citation{10_han2016seq}
\citation{11_kang2017t}
\citation{12_ning2017spatially}
\citation{13_lu2020retinatrack}
\citation{14_kopuklu2019you}
\citation{15_feichtenhofer2016convolutional}
\citation{16_wang2016temporal}
\citation{17_erccelik2021temp}
\citation{18_zhu2009spatial}
\citation{19_wu2003multi}
\citation{20_teutsch2012spatio}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Temporal fusion}{2}{subsection.1.1.1}\protected@file@percent }
\citation{21_forsyth2011computer}
\citation{22_dai2016instance}
\citation{23_fu1981survey}
\citation{24_ladys1994colour}
\citation{25_minaee2021image}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Semantic segmentation}{3}{subsection.1.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Challenges and Difficulties}{3}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Dataset}{3}{subsection.1.2.1}\protected@file@percent }
\citation{26_ronneberger2015u}
\citation{27_lin2017refinenet}
\citation{28_jegou2017one}
\citation{29_iandola2014densenet}
\citation{30_yang2018denseaspp}
\citation{25_minaee2021image}
\citation{31_chen2014semantic}
\citation{32_zhao2018icnet}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Fusion architecture}{4}{subsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Computation cost}{4}{subsection.1.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Real time inference for various application areas}{4}{subsection.1.2.4}\protected@file@percent }
\citation{33_deng2020lightweight}
\citation{34_hsieh2010real}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Use cases}{5}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Autonomous driving and Robotics}{5}{subsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Weed mapping using Unmanned Aerial Vehicle (UAV)}{5}{subsection.1.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Real-Time Hand Gesture Recognition}{5}{subsection.1.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Problem Statement and Contribution}{6}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Research question}{6}{subsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Contribution}{6}{subsection.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Report outline}{6}{section.1.5}\protected@file@percent }
\citation{35_mldl}
\citation{35_mldl}
\citation{36_lecun2015deep}
\citation{36_lecun2015deep}
\citation{36_lecun2015deep}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}State of the Art}{7}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:stateofart}{{2}{7}{State of the Art}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Deep Learning}{7}{section.2.1}\protected@file@percent }
\newlabel{sec:deeplearn}{{2.1}{7}{Deep Learning}{section.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Deep learning in the artificial intelligence domain. Courtesy of \cite  {35_mldl}\relax }}{7}{figure.caption.7}\protected@file@percent }
\newlabel{fig:DLAI}{{2.1}{7}{Deep learning in the artificial intelligence domain. Courtesy of \cite {35_mldl}\relax }{figure.caption.7}{}}
\citation{37_farabet2012learning}
\citation{38_hinton2012deep}
\citation{39_patel2020machine}
\citation{40_ciodaro2012online}
\citation{41_zhang2021deep}
\citation{42_hirschberg2015advances}
\citation{46_o2019deep}
\citation{44_mohanty2016using}
\citation{45_han2021ecological}
\citation{43_srivastava2021comparative}
\citation{47_minaee2021image}
\citation{48_jensen1999temporal}
\citation{49_atluri2018spatio}
\citation{50_lim2021temporal}
\citation{51_meng2021adafuse}
\citation{52_hou2019multi}
\citation{53_duzceker2021deepvideomvs}
\citation{54_zhang2021multiple}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Temporal Fusion}{8}{section.2.2}\protected@file@percent }
\newlabel{sec:tempfuse}{{2.2}{8}{Temporal Fusion}{section.2.2}{}}
\citation{52_hou2019multi}
\citation{52_hou2019multi}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Mulit view stereo architecture for depth estimation. Courtesy of \cite  {52_hou2019multi}\relax }}{9}{figure.caption.8}\protected@file@percent }
\newlabel{fig:mvs}{{2.2}{9}{Mulit view stereo architecture for depth estimation. Courtesy of \cite {52_hou2019multi}\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Semantic Segmentation}{9}{section.2.3}\protected@file@percent }
\newlabel{sec:semseg}{{2.3}{9}{Semantic Segmentation}{section.2.3}{}}
\citation{55_WinNT}
\citation{55_WinNT}
\citation{56_otsu1979threshold}
\citation{57_otsu1979threshold}
\citation{58_boykov2001fast}
\citation{59_starck2005image}
\citation{60_minaee2021image}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Semantic and Instance segmentation example. Courtesy of \cite  {55_WinNT}\relax }}{10}{figure.caption.9}\protected@file@percent }
\newlabel{fig:SS}{{2.3}{10}{Semantic and Instance segmentation example. Courtesy of \cite {55_WinNT}\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Classical Semantic Segmentation}{10}{subsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Deep Learning based Semantic Segmentation}{10}{subsection.2.3.2}\protected@file@percent }
\citation{61_chen2017rethinking}
\citation{62_badrinarayanan2017segnet}
\citation{60_minaee2021image}
\citation{63_goodfellow2014generative}
\citation{60_minaee2021image}
\citation{60_minaee2021image}
\citation{64_noh2015learning}
\citation{64_noh2015learning}
\citation{64_noh2015learning}
\citation{62_badrinarayanan2017segnet}
\citation{62_badrinarayanan2017segnet}
\citation{62_badrinarayanan2017segnet}
\citation{65_kendall2015bayesian}
\citation{66_sun2019high}
\citation{67_fu2019stacked}
\citation{68_hu2018learning}
\citation{69_xia2017w}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Simple encoder-decoder architecture. Courtesy of \cite  {60_minaee2021image}\relax }}{11}{figure.caption.10}\protected@file@percent }
\newlabel{fig:en_de}{{2.4}{11}{Simple encoder-decoder architecture. Courtesy of \cite {60_minaee2021image}\relax }{figure.caption.10}{}}
\citation{70_ronneberger2015u}
\citation{71_milletari2016v}
\citation{70_ronneberger2015u}
\citation{70_ronneberger2015u}
\citation{70_ronneberger2015u}
\citation{70_ronneberger2015u}
\citation{60_minaee2021image}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Simple encoder-decoder architecture. Courtesy of \cite  {64_noh2015learning}\relax }}{12}{figure.caption.11}\protected@file@percent }
\newlabel{fig:general_seg}{{2.5}{12}{Simple encoder-decoder architecture. Courtesy of \cite {64_noh2015learning}\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces SegNet architecture. Courtesy of \cite  {62_badrinarayanan2017segnet}\relax }}{12}{figure.caption.12}\protected@file@percent }
\newlabel{fig:segnet}{{2.6}{12}{SegNet architecture. Courtesy of \cite {62_badrinarayanan2017segnet}\relax }{figure.caption.12}{}}
\citation{72_jin2017video}
\citation{73_gadde2017semantic}
\citation{74_jin2017video}
\citation{75_nilsson2018semantic}
\citation{76_jain2019accel}
\citation{77_mahasseni2017budget}
\citation{78_hu2020temporally}
\citation{78_hu2020temporally}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Unet architecture. Courtesy of \cite  {70_ronneberger2015u}\relax }}{13}{figure.caption.13}\protected@file@percent }
\newlabel{fig:unet}{{2.7}{13}{Unet architecture. Courtesy of \cite {70_ronneberger2015u}\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Temporal Fusion in Semantic Segmentation}{13}{section.2.4}\protected@file@percent }
\citation{78_hu2020temporally}
\citation{78_hu2020temporally}
\citation{52_hou2019multi}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces TDNet. Courtesy of \cite  {78_hu2020temporally}\relax }}{14}{figure.caption.14}\protected@file@percent }
\newlabel{fig:TDNet}{{2.8}{14}{TDNet. Courtesy of \cite {78_hu2020temporally}\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Limitations of Previous Work}{14}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methodology}{15}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:methodology}{{3}{15}{Methodology}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Dataset}{15}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}ScanNet [91]}{15}{subsection.3.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Sample of Scannet dataset rgb and semantic label\relax }}{15}{figure.caption.15}\protected@file@percent }
\newlabel{fig:sample_rgb_seg_scannet}{{3.1}{15}{Sample of Scannet dataset rgb and semantic label\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Sample of Scannet dataset pose\relax }}{16}{figure.caption.16}\protected@file@percent }
\newlabel{fig:sample_pose_scannet}{{3.2}{16}{Sample of Scannet dataset pose\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Virtual KITTI 2 [93]}{16}{subsection.3.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Scannet dataset class distribution\relax }}{17}{figure.caption.17}\protected@file@percent }
\newlabel{fig:scannet_class_distribution}{{3.3}{17}{Scannet dataset class distribution\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Sample of Virtual Kitti 2 dataset\relax }}{17}{figure.caption.18}\protected@file@percent }
\newlabel{fig:sample_scannet_vkitti_2}{{3.4}{17}{Sample of Virtual Kitti 2 dataset\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Sample of Virtual Kitti 2 dataset pose\relax }}{18}{figure.caption.19}\protected@file@percent }
\newlabel{fig:sample_pose_scannet_vkitti_2}{{3.5}{18}{Sample of Virtual Kitti 2 dataset pose\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Data Collection and Preprocessing}{18}{section.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces RGB, Label and Pose dataset sample of scannet and vkitti data\relax }}{19}{figure.caption.20}\protected@file@percent }
\newlabel{fig:scannet_vkitti}{{3.6}{19}{RGB, Label and Pose dataset sample of scannet and vkitti data\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Experimental Design}{19}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}U-Net Vanilla model}{19}{subsection.3.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Scannet data distribution\relax }}{20}{figure.caption.21}\protected@file@percent }
\newlabel{fig:scannet_vkitti}{{3.7}{20}{Scannet data distribution\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}U-Net with Gaussian process}{20}{subsection.3.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Vkitti data distribution\relax }}{21}{figure.caption.22}\protected@file@percent }
\newlabel{fig:scannet_vkitti}{{3.8}{21}{Vkitti data distribution\relax }{figure.caption.22}{}}
\newlabel{eq:distance_matrix}{{3.1}{21}{U-Net with Gaussian process}{equation.3.3.1}{}}
\newlabel{eq:covariance_matrix}{{3.2}{21}{U-Net with Gaussian process}{equation.3.3.2}{}}
\newlabel{eq:gp}{{3.3}{21}{U-Net with Gaussian process}{equation.3.3.3}{}}
\newlabel{eq:encode_output}{{3.4}{21}{U-Net with Gaussian process}{equation.3.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Unet model architecture\relax }}{22}{figure.caption.23}\protected@file@percent }
\newlabel{fig:unet_model}{{3.9}{22}{Unet model architecture\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}U-Net with Long Short Term Memory (LSTM)}{22}{subsection.3.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Unet model architecture with temporal fusion in latent space using Gaussian Process\relax }}{23}{figure.caption.24}\protected@file@percent }
\newlabel{fig:unet_gp}{{3.10}{23}{Unet model architecture with temporal fusion in latent space using Gaussian Process\relax }{figure.caption.24}{}}
\newlabel{eq:it}{{3.5}{23}{U-Net with Long Short Term Memory (LSTM)}{equation.3.3.5}{}}
\newlabel{eq:ft}{{3.6}{23}{U-Net with Long Short Term Memory (LSTM)}{equation.3.3.6}{}}
\newlabel{eq:ot}{{3.7}{23}{U-Net with Long Short Term Memory (LSTM)}{equation.3.3.7}{}}
\newlabel{eq:gt}{{3.8}{23}{U-Net with Long Short Term Memory (LSTM)}{equation.3.3.8}{}}
\newlabel{eq:Ct}{{3.9}{23}{U-Net with Long Short Term Memory (LSTM)}{equation.3.3.9}{}}
\newlabel{eq:Ht}{{3.10}{23}{U-Net with Long Short Term Memory (LSTM)}{equation.3.3.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Training and Evaluation Pipeline}{23}{section.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Unet model architecture with temporal fusion in latent space using the ConvLSTM cell\relax }}{24}{figure.caption.25}\protected@file@percent }
\newlabel{fig:unet_lstm}{{3.11}{24}{Unet model architecture with temporal fusion in latent space using the ConvLSTM cell\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Training pipeline\relax }}{25}{figure.caption.26}\protected@file@percent }
\newlabel{fig:unet_training}{{3.12}{25}{Training pipeline\relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Hardware Configuration}{25}{section.3.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Evaluation pipeline\relax }}{26}{figure.caption.27}\protected@file@percent }
\newlabel{fig:unet_evaluation}{{3.13}{26}{Evaluation pipeline\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Pictorial representation of training and evaluation procedure\relax }}{27}{figure.caption.28}\protected@file@percent }
\newlabel{fig:unet_evaluation}{{3.14}{27}{Pictorial representation of training and evaluation procedure\relax }{figure.caption.28}{}}
\citation{79_dai2017scannet}
\citation{80_cabon2020vkitti2}
\citation{81_minodaRAL2021}
\citation{84_ulku2022survey}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Evaluation and Experimental Result}{29}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:evaluationandresult}{{4}{29}{Evaluation and Experimental Result}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Evaluation Metric}{29}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Pixel Accuracy}{29}{subsection.4.1.1}\protected@file@percent }
\citation{83_iou}
\citation{82_iou}
\citation{82_iou}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}IoU}{30}{subsection.4.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces IoU. Courtesy of \cite  {82_iou}\relax }}{30}{figure.caption.29}\protected@file@percent }
\newlabel{fig:IoU}{{4.1}{30}{IoU. Courtesy of \cite {82_iou}\relax }{figure.caption.29}{}}
\citation{84_ulku2022survey}
\citation{84_ulku2022survey}
\citation{78_hu2020temporally}
\citation{52_hou2019multi}
\citation{03_duzceker2021deepvideomvs}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Hypothesis}{31}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}RQ1: What are the works on state-of-the-art temporal fusion in semantic segmentation?}{31}{section.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces VIS proposed framework. Courtesy of [87]\relax }}{32}{figure.caption.30}\protected@file@percent }
\newlabel{fig:vis}{{4.2}{32}{VIS proposed framework. Courtesy of [87]\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces The frame level detector takes frame queries and mask features, generates the embeddings, and pass onto the VITA model for mask prediction. Constructing the temporal interactions between the frame queries captures the object-aware knowledge in the spatial scenes. Finally, mask trajectories are obtained from the VITA model. Courtesy of [88]\relax }}{33}{figure.caption.31}\protected@file@percent }
\newlabel{fig:vita}{{4.3}{33}{The frame level detector takes frame queries and mask features, generates the embeddings, and pass onto the VITA model for mask prediction. Constructing the temporal interactions between the frame queries captures the object-aware knowledge in the spatial scenes. Finally, mask trajectories are obtained from the VITA model. Courtesy of [88]\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces (a) MinVIS trained on query-based segmentation individually for every frame. (b) Inference of the video instance segmentation from the segmented image using bipartite matching of the query embeddings. Courtesy of [89] \relax }}{33}{figure.caption.32}\protected@file@percent }
\newlabel{fig:minVIS}{{4.4}{33}{(a) MinVIS trained on query-based segmentation individually for every frame. (b) Inference of the video instance segmentation from the segmented image using bipartite matching of the query embeddings. Courtesy of [89] \relax }{figure.caption.32}{}}
\citation{85_kag_challenge}
\citation{79_dai2017scannet}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces  A mask2former with video instance segmentation. Courtesy of [90]\relax }}{34}{figure.caption.33}\protected@file@percent }
\newlabel{fig:mask2former}{{4.5}{34}{A mask2former with video instance segmentation. Courtesy of [90]\relax }{figure.caption.33}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}RQ2: How are the results from RQ1 compared with each other to perform temporal fusion?}{34}{section.4.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Comparison of temporal fusion model performance with respect to Youtube VIS dataset\relax }}{34}{table.caption.34}\protected@file@percent }
\newlabel{tab:sota_ytube_vis}{{4.1}{34}{Comparison of temporal fusion model performance with respect to Youtube VIS dataset\relax }{table.caption.34}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}RQ3: How to cross-transfer the temporal fusion technique to semantic segmentation?}{35}{section.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Combining scannet dataset classes for experiment}{35}{subsection.4.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Distance and Kernel matrix}{35}{subsection.4.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Per class pixel distribution of the entire scannet dataset\relax }}{36}{figure.caption.35}\protected@file@percent }
\newlabel{fig:scannet_class}{{4.6}{36}{Per class pixel distribution of the entire scannet dataset\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Pixel distribution for the scannet data containing all the classes\relax }}{36}{figure.caption.36}\protected@file@percent }
\newlabel{fig:scannet_all_classes}{{4.7}{36}{Pixel distribution for the scannet data containing all the classes\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Pixel distribution for the scannet data for two classes\relax }}{37}{figure.caption.37}\protected@file@percent }
\newlabel{fig:scannet_two_classes}{{4.8}{37}{Pixel distribution for the scannet data for two classes\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Pixel distribution for the scannet data for three classes\relax }}{37}{figure.caption.38}\protected@file@percent }
\newlabel{fig:scannet_three_classes}{{4.9}{37}{Pixel distribution for the scannet data for three classes\relax }{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Ordered and Unordered set of images\relax }}{38}{figure.caption.39}\protected@file@percent }
\newlabel{fig:ordered_and_unordered_images}{{4.10}{38}{Ordered and Unordered set of images\relax }{figure.caption.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Experiment1.1: U-Net Vanilla, GP and LSTM model considering all the classes}{38}{subsection.4.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Distance matrix and Kernel matrix for ordered set of images\relax }}{39}{figure.caption.40}\protected@file@percent }
\newlabel{fig:ordered_D_and_K}{{4.11}{39}{Distance matrix and Kernel matrix for ordered set of images\relax }{figure.caption.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces Distance matrix and Kernel matrix for unordered set of images\relax }}{39}{figure.caption.41}\protected@file@percent }
\newlabel{fig:unordered_D_and_K}{{4.12}{39}{Distance matrix and Kernel matrix for unordered set of images\relax }{figure.caption.41}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces A normal caption\relax }}{40}{table.caption.42}\protected@file@percent }
\newlabel{tab:caption}{{4.2}{40}{A normal caption\relax }{table.caption.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces Pixel distribution for the ground truth and predicted scannet data for vanilla unet model\relax }}{40}{figure.caption.43}\protected@file@percent }
\newlabel{fig:y_gt_pred_vanilla}{{4.13}{40}{Pixel distribution for the ground truth and predicted scannet data for vanilla unet model\relax }{figure.caption.43}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces A normal caption\relax }}{41}{table.caption.44}\protected@file@percent }
\newlabel{tab:caption}{{4.3}{41}{A normal caption\relax }{table.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces Per class pixel distribution of the predicted pixel class label for gp model\relax }}{41}{figure.caption.45}\protected@file@percent }
\newlabel{fig:y_gt_and_predic_gp}{{4.14}{41}{Per class pixel distribution of the predicted pixel class label for gp model\relax }{figure.caption.45}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces A normal caption\relax }}{42}{table.caption.46}\protected@file@percent }
\newlabel{tab:caption}{{4.4}{42}{A normal caption\relax }{table.caption.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces Per class pixel distribution of the predicted pixel class label for lstm model\relax }}{42}{figure.caption.47}\protected@file@percent }
\newlabel{fig:y_gt_and_predic_lstm}{{4.15}{42}{Per class pixel distribution of the predicted pixel class label for lstm model\relax }{figure.caption.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.16}{\ignorespaces Comparison of VANILLA, GP and LSTM model performance based on metric\relax }}{43}{figure.caption.48}\protected@file@percent }
\newlabel{fig:unet_model_metric_comparison_all_classes}{{4.16}{43}{Comparison of VANILLA, GP and LSTM model performance based on metric\relax }{figure.caption.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.17}{\ignorespaces Plotting of raw input image, ground truth and predicted output for vanilla, gp and lstm model\relax }}{44}{figure.caption.49}\protected@file@percent }
\newlabel{fig:unet_model}{{4.17}{44}{Plotting of raw input image, ground truth and predicted output for vanilla, gp and lstm model\relax }{figure.caption.49}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.4}Experiment1.2: U-Net Vanilla, Temporally fused gp model and LSTM model considering two classes}{44}{subsection.4.5.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Performance of Vanilla model with respect to different metric and two classes\relax }}{44}{table.caption.51}\protected@file@percent }
\newlabel{table:unet_two_classes}{{4.5}{44}{Performance of Vanilla model with respect to different metric and two classes\relax }{table.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.18}{\ignorespaces Plotting of raw input image, ground truth and predicted output for vanilla, gp and lstm model for two class dataset\relax }}{45}{figure.caption.50}\protected@file@percent }
\newlabel{fig:output_vanilla}{{4.18}{45}{Plotting of raw input image, ground truth and predicted output for vanilla, gp and lstm model for two class dataset\relax }{figure.caption.50}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.5}Experiment1.3: U-Net Vanilla, temporally fused gp and lstm model considering three classes}{45}{subsection.4.5.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces Performance of Vanilla model with respect to different metric and two classes\relax }}{45}{table.caption.54}\protected@file@percent }
\newlabel{table:unet_scannet_three_classes}{{4.6}{45}{Performance of Vanilla model with respect to different metric and two classes\relax }{table.caption.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.19}{\ignorespaces Comparison of VANILLA, GP and LSTM model performance based on metric for scannet two classes\relax }}{46}{figure.caption.52}\protected@file@percent }
\newlabel{fig:unet_model_metric_comparison}{{4.19}{46}{Comparison of VANILLA, GP and LSTM model performance based on metric for scannet two classes\relax }{figure.caption.52}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.20}{\ignorespaces Plotting of raw input image, ground truth and predicted output for vanilla, gp and lstm model\relax }}{46}{figure.caption.53}\protected@file@percent }
\newlabel{fig:performance_metric_three_classes_scannet}{{4.20}{46}{Plotting of raw input image, ground truth and predicted output for vanilla, gp and lstm model\relax }{figure.caption.53}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.21}{\ignorespaces Comparison of VANILLA, GP and LSTM model performance based on metric for scannet three classes\relax }}{47}{figure.caption.55}\protected@file@percent }
\newlabel{fig:performance_metric_three_classes_unet}{{4.21}{47}{Comparison of VANILLA, GP and LSTM model performance based on metric for scannet three classes\relax }{figure.caption.55}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces Performance of Vanilla model with respect to different metric and two classes\relax }}{48}{table.caption.57}\protected@file@percent }
\newlabel{table:unet_vkitti_two_classes}{{4.7}{48}{Performance of Vanilla model with respect to different metric and two classes\relax }{table.caption.57}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Experiment 2.1: Experiment with vkitti dataset with fog and morning as the testing data}{48}{section.4.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.22}{\ignorespaces Plotting of raw input image, ground truth and predicted output for vanilla, gp and lstm for vkitti dataset\relax }}{49}{figure.caption.56}\protected@file@percent }
\newlabel{fig:vkitti_unet_two}{{4.22}{49}{Plotting of raw input image, ground truth and predicted output for vanilla, gp and lstm for vkitti dataset\relax }{figure.caption.56}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Experiment 2.1.2: Impact of training data batch size on the evaluation dataset performance}{49}{subsection.4.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.23}{\ignorespaces Performance comparison of the VANILLA, GP and LSTM model based on different metrics\relax }}{50}{figure.caption.58}\protected@file@percent }
\newlabel{fig:performance_metric_vkitti_two_class_box_plot}{{4.23}{50}{Performance comparison of the VANILLA, GP and LSTM model based on different metrics\relax }{figure.caption.58}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.24}{\ignorespaces IoU predictions for all the classes for VANILLA, GP and LSTM model\relax }}{50}{figure.caption.59}\protected@file@percent }
\newlabel{fig:performance_metric_three_classes}{{4.24}{50}{IoU predictions for all the classes for VANILLA, GP and LSTM model\relax }{figure.caption.59}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.25}{\ignorespaces Side by side comparison of models predictions for continuous sequence data from frame 1 to 4\relax }}{51}{figure.caption.60}\protected@file@percent }
\newlabel{fig:performance_metric_three_classes}{{4.25}{51}{Side by side comparison of models predictions for continuous sequence data from frame 1 to 4\relax }{figure.caption.60}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.26}{\ignorespaces Side by side comparison of models predictions for continuous sequence data from frame 5 to 8\relax }}{52}{figure.caption.61}\protected@file@percent }
\newlabel{fig:performance_metric_three_classes}{{4.26}{52}{Side by side comparison of models predictions for continuous sequence data from frame 5 to 8\relax }{figure.caption.61}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.27}{\ignorespaces Impact of training batch size on the evaluation dataset prediction\relax }}{53}{figure.caption.62}\protected@file@percent }
\newlabel{fig:batch_wise_performance}{{4.27}{53}{Impact of training batch size on the evaluation dataset prediction\relax }{figure.caption.62}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.28}{\ignorespaces Plotting of raw input image, ground truth and predicted output for vanilla, gp and lstm\relax }}{53}{figure.caption.63}\protected@file@percent }
\newlabel{fig:unet_side_by_side_five_classes}{{4.28}{53}{Plotting of raw input image, ground truth and predicted output for vanilla, gp and lstm\relax }{figure.caption.63}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Experiment 2.2: Experiment with vkitti dataset with clone, sunset, rain, 15-deg-right, and 30-deg-left as the testing data}{53}{section.4.7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.8}{\ignorespaces Performance of Vanilla model with respect to different metric and two classes\relax }}{54}{table.caption.64}\protected@file@percent }
\newlabel{table:Vanilla_conti_seq}{{4.8}{54}{Performance of Vanilla model with respect to different metric and two classes\relax }{table.caption.64}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.29}{\ignorespaces Comparison of model performance based on different metrics\relax }}{54}{figure.caption.65}\protected@file@percent }
\newlabel{fig:performance_metric_unet}{{4.29}{54}{Comparison of model performance based on different metrics\relax }{figure.caption.65}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Comments and Discussion}{54}{section.4.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.30}{\ignorespaces IoU for all the classes in the dataset with respect to different model predictions\relax }}{55}{figure.caption.66}\protected@file@percent }
\newlabel{fig:performance_metric_three_classes_unet_five}{{4.30}{55}{IoU for all the classes in the dataset with respect to different model predictions\relax }{figure.caption.66}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Android Deployment}{57}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:androiddeploy}{{5}{57}{Android Deployment}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Framework}{57}{section.5.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Performance of Vanilla model with respect to different metric and two classes\relax }}{57}{table.caption.68}\protected@file@percent }
\newlabel{table:android_spec}{{5.1}{57}{Performance of Vanilla model with respect to different metric and two classes\relax }{table.caption.68}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Training and evaluation procedure\relax }}{58}{figure.caption.67}\protected@file@percent }
\newlabel{fig:android_pipeline}{{5.1}{58}{Training and evaluation procedure\relax }{figure.caption.67}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Android deployment results\relax }}{59}{figure.caption.69}\protected@file@percent }
\newlabel{fig:android_result}{{5.2}{59}{Android deployment results\relax }{figure.caption.69}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Display of results}{59}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusions}{61}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:conclusion}{{6}{61}{Conclusions}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Contributions}{62}{section.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Lessons learned}{62}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Future work}{62}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Appendix \numberline {A}Design Details}{63}{appendix.a.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces Unet vanilla model architecture\relax }}{64}{table.caption.70}\protected@file@percent }
\newlabel{table:unet_vanilla_model_1}{{A.1}{64}{Unet vanilla model architecture\relax }{table.caption.70}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.2}{\ignorespaces Unet vanilla model architecture\relax }}{65}{table.caption.71}\protected@file@percent }
\newlabel{table:unet_vanilla_model_2}{{A.2}{65}{Unet vanilla model architecture\relax }{table.caption.71}{}}
\@writefile{toc}{\contentsline {chapter}{Appendix \numberline {B}Parameters}{67}{appendix.a.B}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces Classes and ids of the Scannet dataset\relax }}{68}{table.caption.72}\protected@file@percent }
\newlabel{table:Classes in scannet_1}{{B.1}{68}{Classes and ids of the Scannet dataset\relax }{table.caption.72}{}}
\@writefile{lot}{\contentsline {table}{\numberline {B.2}{\ignorespaces Classes and ids of the Scannet dataset\relax }}{69}{table.caption.73}\protected@file@percent }
\newlabel{table:Classes in scannet_2}{{B.2}{69}{Classes and ids of the Scannet dataset\relax }{table.caption.73}{}}
\@writefile{lot}{\contentsline {table}{\numberline {B.3}{\ignorespaces Classes and ids of the Scannet dataset\relax }}{70}{table.caption.74}\protected@file@percent }
\newlabel{table:Classes in scannet_3}{{B.3}{70}{Classes and ids of the Scannet dataset\relax }{table.caption.74}{}}
\bibstyle{unsrt}
\bibdata{references}
\bibcite{35_mldl}{1}
\bibcite{52_hou2019multi}{2}
\bibcite{55_WinNT}{3}
\bibcite{60_minaee2021image}{4}
\bibcite{64_noh2015learning}{5}
\bibcite{62_badrinarayanan2017segnet}{6}
\bibcite{70_ronneberger2015u}{7}
\bibcite{78_hu2020temporally}{8}
\bibcite{82_iou}{9}
\bibcite{01_mandic2005data}{10}
\bibcite{06_castanedo2013review}{11}
\bibcite{02_lim2021temporal}{12}
\@writefile{toc}{\contentsline {chapter}{References}{71}{appendix*.75}\protected@file@percent }
\bibcite{03_duzceker2021deepvideomvs}{13}
\bibcite{04_li2021spatial}{14}
\bibcite{005_hsiao2005temporal}{15}
\bibcite{07_hsiao2005temporal}{16}
\bibcite{08_krause2003unsupervised}{17}
\bibcite{09_lee2003line}{18}
\bibcite{10_han2016seq}{19}
\bibcite{11_kang2017t}{20}
\bibcite{12_ning2017spatially}{21}
\bibcite{13_lu2020retinatrack}{22}
\bibcite{14_kopuklu2019you}{23}
\bibcite{15_feichtenhofer2016convolutional}{24}
\bibcite{16_wang2016temporal}{25}
\bibcite{17_erccelik2021temp}{26}
\bibcite{18_zhu2009spatial}{27}
\bibcite{19_wu2003multi}{28}
\bibcite{20_teutsch2012spatio}{29}
\bibcite{21_forsyth2011computer}{30}
\bibcite{22_dai2016instance}{31}
\bibcite{23_fu1981survey}{32}
\bibcite{24_ladys1994colour}{33}
\bibcite{25_minaee2021image}{34}
\bibcite{26_ronneberger2015u}{35}
\bibcite{27_lin2017refinenet}{36}
\bibcite{28_jegou2017one}{37}
\bibcite{29_iandola2014densenet}{38}
\bibcite{30_yang2018denseaspp}{39}
\bibcite{31_chen2014semantic}{40}
\bibcite{32_zhao2018icnet}{41}
\bibcite{33_deng2020lightweight}{42}
\bibcite{34_hsieh2010real}{43}
\bibcite{36_lecun2015deep}{44}
\bibcite{37_farabet2012learning}{45}
\bibcite{38_hinton2012deep}{46}
\bibcite{39_patel2020machine}{47}
\bibcite{40_ciodaro2012online}{48}
\bibcite{41_zhang2021deep}{49}
\bibcite{42_hirschberg2015advances}{50}
\bibcite{46_o2019deep}{51}
\bibcite{44_mohanty2016using}{52}
\bibcite{45_han2021ecological}{53}
\bibcite{43_srivastava2021comparative}{54}
\bibcite{47_minaee2021image}{55}
\bibcite{48_jensen1999temporal}{56}
\bibcite{49_atluri2018spatio}{57}
\bibcite{50_lim2021temporal}{58}
\bibcite{51_meng2021adafuse}{59}
\bibcite{53_duzceker2021deepvideomvs}{60}
\bibcite{54_zhang2021multiple}{61}
\bibcite{56_otsu1979threshold}{62}
\bibcite{57_otsu1979threshold}{63}
\bibcite{58_boykov2001fast}{64}
\bibcite{59_starck2005image}{65}
\bibcite{61_chen2017rethinking}{66}
\bibcite{63_goodfellow2014generative}{67}
\bibcite{65_kendall2015bayesian}{68}
\bibcite{66_sun2019high}{69}
\bibcite{67_fu2019stacked}{70}
\bibcite{68_hu2018learning}{71}
\bibcite{69_xia2017w}{72}
\bibcite{71_milletari2016v}{73}
\bibcite{72_jin2017video}{74}
\bibcite{73_gadde2017semantic}{75}
\bibcite{74_jin2017video}{76}
\bibcite{75_nilsson2018semantic}{77}
\bibcite{76_jain2019accel}{78}
\bibcite{77_mahasseni2017budget}{79}
\bibcite{79_dai2017scannet}{80}
\bibcite{80_cabon2020vkitti2}{81}
\bibcite{81_minodaRAL2021}{82}
\bibcite{84_ulku2022survey}{83}
\bibcite{83_iou}{84}
\bibcite{85_kag_challenge}{85}
\bibcite{05_hsiao2005temporal}{86}
\bibcite{86_li2022hybrid}{87}
\bibcite{87_heo2022vita}{88}
\bibcite{88_huang2022minvis}{89}
\bibcite{89_cheng2021mask2former}{90}
\bibcite{90_dai2017scannet}{91}
\bibcite{91_ScanNet}{92}
\bibcite{92_cabon2020virtual}{93}
\bibcite{93_ScanNet}{94}
\bibcite{94_ScanNet}{95}
\bibcite{95_mazzotti2016measure}{96}
\bibcite{96_GPML}{97}
\bibcite{97_williams2006gaussian}{98}
\bibcite{98_ConvLSTM}{99}
\bibcite{99_shi2015convolutional}{100}
\citation{*}
