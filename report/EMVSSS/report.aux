\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{35_mldl}
\citation{52_hou2019multi}
\citation{55_WinNT}
\citation{60_minaee2021image}
\citation{64_noh2015learning}
\citation{62_badrinarayanan2017segnet}
\citation{70_ronneberger2015u}
\citation{78_hu2020temporally}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{xv}{chapter*.4}\protected@file@percent }
\citation{82_iou}
\@writefile{toc}{\contentsline {chapter}{List of Tables}{xxi}{chapter*.5}\protected@file@percent }
\citation{01_mandic2005data}
\citation{06_castanedo2013review}
\citation{02_lim2021temporal}
\citation{03_duzceker2021deepvideomvs}
\citation{04_li2021spatial}
\citation{005_hsiao2005temporal}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:introduction}{{1}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation}{1}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Temporal Fusion}{1}{subsection.1.1.1}\protected@file@percent }
\citation{07_hsiao2005temporal}
\citation{07_hsiao2005temporal}
\citation{08_krause2003unsupervised}
\citation{09_lee2003line}
\citation{10_han2016seq}
\citation{11_kang2017t}
\citation{12_ning2017spatially}
\citation{13_lu2020retinatrack}
\citation{14_kopuklu2019you}
\citation{15_feichtenhofer2016convolutional}
\citation{16_wang2016temporal}
\citation{17_erccelik2021temp}
\citation{18_zhu2009spatial}
\citation{19_wu2003multi}
\citation{20_teutsch2012spatio}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Data fusion categories based on the type of fusion. Temporal data fusion fuse the temporal data from past sequences, and multi-sensor data fuse the data from multiple sensors collected at the moment. Generated with \href  {https://app.diagrams.net/}{Diagrams.net} \relax }}{2}{figure.caption.6}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:3D_reconstruction}{{1.1}{2}{Data fusion categories based on the type of fusion. Temporal data fusion fuse the temporal data from past sequences, and multi-sensor data fuse the data from multiple sensors collected at the moment. Generated with \href {https://app.diagrams.net/}{Diagrams.net} \relax }{figure.caption.6}{}}
\citation{21_forsyth2011computer}
\citation{22_dai2016instance}
\citation{23_fu1981survey}
\citation{24_ladys1994colour}
\citation{25_minaee2021image}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Semantic Segmentation}{3}{subsection.1.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Challenges and Difficulties}{3}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Dataset}{3}{subsection.1.2.1}\protected@file@percent }
\citation{26_ronneberger2015u}
\citation{27_lin2017refinenet}
\citation{28_jegou2017one}
\citation{29_iandola2014densenet}
\citation{30_yang2018denseaspp}
\citation{25_minaee2021image}
\citation{31_chen2014semantic}
\citation{32_zhao2018icnet}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Fusion Architecture}{4}{subsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Computation Cost}{4}{subsection.1.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Real Time Inference for Various Application Areas}{4}{subsection.1.2.4}\protected@file@percent }
\citation{33_deng2020lightweight}
\citation{34_hsieh2010real}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Use Cases}{5}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Autonomous Driving and Robotics}{5}{subsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Weed Mapping Using Unmanned Aerial Vehicle (UAV)}{5}{subsection.1.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Real-Time Hand Gesture Recognition}{5}{subsection.1.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Problem Statement and Contribution}{6}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Research Questions}{6}{subsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Contributions}{6}{subsection.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Report Outline}{6}{section.1.5}\protected@file@percent }
\citation{35_mldl}
\citation{35_mldl}
\citation{36_lecun2015deep}
\citation{36_lecun2015deep}
\citation{36_lecun2015deep}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}State of the Art}{7}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:stateofart}{{2}{7}{State of the Art}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Deep Learning}{7}{section.2.1}\protected@file@percent }
\newlabel{sec:deeplearn}{{2.1}{7}{Deep Learning}{section.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Deep learning in the artificial intelligence domain. Courtesy of \cite  {35_mldl}\relax }}{7}{figure.caption.7}\protected@file@percent }
\newlabel{fig:DLAI}{{2.1}{7}{Deep learning in the artificial intelligence domain. Courtesy of \cite {35_mldl}\relax }{figure.caption.7}{}}
\citation{37_farabet2012learning}
\citation{38_hinton2012deep}
\citation{39_patel2020machine}
\citation{40_ciodaro2012online}
\citation{41_zhang2021deep}
\citation{42_hirschberg2015advances}
\citation{46_o2019deep}
\citation{44_mohanty2016using}
\citation{45_han2021ecological}
\citation{43_srivastava2021comparative}
\citation{47_minaee2021image}
\citation{48_jensen1999temporal}
\citation{49_atluri2018spatio}
\citation{50_lim2021temporal}
\citation{51_meng2021adafuse}
\citation{52_hou2019multi}
\citation{53_duzceker2021deepvideomvs}
\citation{54_zhang2021multiple}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Temporal Fusion}{8}{section.2.2}\protected@file@percent }
\newlabel{sec:tempfuse}{{2.2}{8}{Temporal Fusion}{section.2.2}{}}
\citation{52_hou2019multi}
\citation{52_hou2019multi}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Multi-View Stereo by Temporal Nonparametric Fusion. Two consecutive frames are used to construct the cost volume and the output is passed onto the encoder to generate latent space encoding. The frames coordinates are transferred to the Gaussian Process to generate the updated latent space encoding. The Gaussian Process take encoder output, frame coordinates, and the propagated latent space encoding. The output of Gaussian Process is forwarded to decoder to generate the depth map. Courtesy of \cite  {52_hou2019multi}\relax }}{9}{figure.caption.8}\protected@file@percent }
\newlabel{fig:mvs}{{2.2}{9}{Multi-View Stereo by Temporal Nonparametric Fusion. Two consecutive frames are used to construct the cost volume and the output is passed onto the encoder to generate latent space encoding. The frames coordinates are transferred to the Gaussian Process to generate the updated latent space encoding. The Gaussian Process take encoder output, frame coordinates, and the propagated latent space encoding. The output of Gaussian Process is forwarded to decoder to generate the depth map. Courtesy of \cite {52_hou2019multi}\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Semantic Segmentation}{9}{section.2.3}\protected@file@percent }
\newlabel{sec:semseg}{{2.3}{9}{Semantic Segmentation}{section.2.3}{}}
\citation{55_WinNT}
\citation{55_WinNT}
\citation{56_otsu1979threshold}
\citation{57_otsu1979threshold}
\citation{58_boykov2001fast}
\citation{59_starck2005image}
\citation{60_minaee2021image}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Picture depicting the semantic, instance and panoptic segmentation examples. Courtesy of \cite  {55_WinNT}\relax }}{10}{figure.caption.9}\protected@file@percent }
\newlabel{fig:SS}{{2.3}{10}{Picture depicting the semantic, instance and panoptic segmentation examples. Courtesy of \cite {55_WinNT}\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Classical Semantic Segmentation}{10}{subsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Deep Learning Based Semantic Segmentation}{10}{subsection.2.3.2}\protected@file@percent }
\citation{61_chen2017rethinking}
\citation{62_badrinarayanan2017segnet}
\citation{60_minaee2021image}
\citation{63_goodfellow2014generative}
\citation{60_minaee2021image}
\citation{60_minaee2021image}
\citation{64_noh2015learning}
\citation{64_noh2015learning}
\citation{64_noh2015learning}
\citation{62_badrinarayanan2017segnet}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Plain encoder-decoder architecture with latent space encoding. Courtesy of \cite  {60_minaee2021image}\relax }}{11}{figure.caption.10}\protected@file@percent }
\newlabel{fig:en_de}{{2.4}{11}{Plain encoder-decoder architecture with latent space encoding. Courtesy of \cite {60_minaee2021image}\relax }{figure.caption.10}{}}
\citation{62_badrinarayanan2017segnet}
\citation{62_badrinarayanan2017segnet}
\citation{65_kendall2015bayesian}
\citation{66_sun2019high}
\citation{67_fu2019stacked}
\citation{68_hu2018learning}
\citation{69_xia2017w}
\citation{70_ronneberger2015u}
\citation{71_milletari2016v}
\citation{70_ronneberger2015u}
\citation{70_ronneberger2015u}
\citation{70_ronneberger2015u}
\citation{70_ronneberger2015u}
\citation{60_minaee2021image}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Simple encoder-decoder architecture with convolution and deconvolution network. Courtesy of \cite  {64_noh2015learning}\relax }}{12}{figure.caption.11}\protected@file@percent }
\newlabel{fig:general_seg}{{2.5}{12}{Simple encoder-decoder architecture with convolution and deconvolution network. Courtesy of \cite {64_noh2015learning}\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces SegNet architecture with novel upsampling of latent space encoding. Courtesy of \cite  {62_badrinarayanan2017segnet}\relax }}{12}{figure.caption.12}\protected@file@percent }
\newlabel{fig:segnet}{{2.6}{12}{SegNet architecture with novel upsampling of latent space encoding. Courtesy of \cite {62_badrinarayanan2017segnet}\relax }{figure.caption.12}{}}
\citation{72_jin2017video}
\citation{73_gadde2017semantic}
\citation{74_jin2017video}
\citation{75_nilsson2018semantic}
\citation{76_jain2019accel}
\citation{77_mahasseni2017budget}
\citation{78_hu2020temporally}
\citation{78_hu2020temporally}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Unet architecture. A semantic segmentation network for biomedical applications with the strong use of augmentation. The network consist of encoder that down samples the features and a decoder for up sampling. Courtesy of \cite  {70_ronneberger2015u}\relax }}{13}{figure.caption.13}\protected@file@percent }
\newlabel{fig:unet}{{2.7}{13}{Unet architecture. A semantic segmentation network for biomedical applications with the strong use of augmentation. The network consist of encoder that down samples the features and a decoder for up sampling. Courtesy of \cite {70_ronneberger2015u}\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Temporal Fusion in Semantic Segmentation}{13}{section.2.4}\protected@file@percent }
\citation{78_hu2020temporally}
\citation{78_hu2020temporally}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces TDNet is a efficient and accurate video segmentation framework. Courtesy of \cite  {78_hu2020temporally}\relax }}{14}{figure.caption.14}\protected@file@percent }
\newlabel{fig:TDNet}{{2.8}{14}{TDNet is a efficient and accurate video segmentation framework. Courtesy of \cite {78_hu2020temporally}\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}RQ1: What Are the Works on State-Of-The-Art Temporal Fusion in Semantic Segmentation?}{14}{section.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces VIS proposed framework.An online video instance segmentation framework with instance aware temporal fusion method. Intra frame attention framework for combining instance code and feature map. Inter-frame attention for fusing the hybrid temporal information from previous prediction stage. Courtesy of [87]\relax }}{15}{figure.caption.15}\protected@file@percent }
\newlabel{fig:vis}{{2.9}{15}{VIS proposed framework.An online video instance segmentation framework with instance aware temporal fusion method. Intra frame attention framework for combining instance code and feature map. Inter-frame attention for fusing the hybrid temporal information from previous prediction stage. Courtesy of [87]\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces The frame level detector takes frame queries and mask features, generates the embeddings, and pass onto the VITA model for mask prediction. Constructing the temporal interactions between the frame queries captures the object-aware knowledge in the spatial scenes. Finally, mask trajectories are obtained from the VITA model. Courtesy of [88]\relax }}{16}{figure.caption.16}\protected@file@percent }
\newlabel{fig:vita}{{2.10}{16}{The frame level detector takes frame queries and mask features, generates the embeddings, and pass onto the VITA model for mask prediction. Constructing the temporal interactions between the frame queries captures the object-aware knowledge in the spatial scenes. Finally, mask trajectories are obtained from the VITA model. Courtesy of [88]\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces (a) MinVIS trained on query-based segmentation individually for every frame. (b) Inference of the video instance segmentation from the segmented image using bipartite matching of the query embeddings. Courtesy of [89] \relax }}{16}{figure.caption.17}\protected@file@percent }
\newlabel{fig:minVIS}{{2.11}{16}{(a) MinVIS trained on query-based segmentation individually for every frame. (b) Inference of the video instance segmentation from the segmented image using bipartite matching of the query embeddings. Courtesy of [89] \relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces  A mask2former with video instance segmentation. The architecture can also tackle the semantic and panoptic segmentation task. Courtesy of [90]\relax }}{17}{figure.caption.18}\protected@file@percent }
\newlabel{fig:mask2former}{{2.12}{17}{A mask2former with video instance segmentation. The architecture can also tackle the semantic and panoptic segmentation task. Courtesy of [90]\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}RQ1 Conclusion}{17}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.7}RQ2: How Are the Results From RQ1 Compared With Each Other to Perform Temporal Fusion?}{17}{section.2.7}\protected@file@percent }
\citation{52_hou2019multi}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Comparison of temporal fusion model performance with respect to Youtube VIS dataset\relax }}{18}{table.caption.19}\protected@file@percent }
\newlabel{tab:sota_ytube_vis}{{2.1}{18}{Comparison of temporal fusion model performance with respect to Youtube VIS dataset\relax }{table.caption.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.8}RQ2 Conclusion}{18}{section.2.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.9}Limitations of Previous Work}{19}{section.2.9}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methodology}{21}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:methodology}{{3}{21}{Methodology}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Dataset}{21}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}ScanNet [91]}{21}{subsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Virtual KITTI 2 [93]}{21}{subsection.3.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Sample of Scannet dataset continuous frame rgb and semantic label. Courtesy of [91] \relax }}{22}{figure.caption.20}\protected@file@percent }
\newlabel{fig:sample_rgb_seg_scannet}{{3.1}{22}{Sample of Scannet dataset continuous frame rgb and semantic label. Courtesy of [91] \relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Sample of Scannet dataset pose. Each point represent the pose of the camera in 3D frame when a particular frame is captured. The representation shows how the camera is moving in the 3D world.\relax }}{22}{figure.caption.21}\protected@file@percent }
\newlabel{fig:sample_pose_scannet}{{3.2}{22}{Sample of Scannet dataset pose. Each point represent the pose of the camera in 3D frame when a particular frame is captured. The representation shows how the camera is moving in the 3D world.\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Scannet dataset class distribution. Horizontal axis represent the classes in Scannet dataset and vertical axis represent the normalized number of pixels per class. There are high number of pixels in entire data belonging to class 1 and class 2.\relax }}{23}{figure.caption.22}\protected@file@percent }
\newlabel{fig:scannet_class_distribution}{{3.3}{23}{Scannet dataset class distribution. Horizontal axis represent the classes in Scannet dataset and vertical axis represent the normalized number of pixels per class. There are high number of pixels in entire data belonging to class 1 and class 2.\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Data Collection and Preprocessing}{23}{section.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Sample of Virtual Kitti 2 dataset. Top row represent the continuous frame number followed by rgb and semantic labels. Courtesy of [93]\relax }}{24}{figure.caption.23}\protected@file@percent }
\newlabel{fig:sample_scannet_vkitti_2}{{3.4}{24}{Sample of Virtual Kitti 2 dataset. Top row represent the continuous frame number followed by rgb and semantic labels. Courtesy of [93]\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Experimental Design}{24}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}U-Net Vanilla Model}{24}{subsection.3.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Sample of Virtual Kitti 2 dataset pose. Each point represent the pose of the camera in 3D frame when a particular frame is captured. The representation shows how the camera is moving in the 3D world.\relax }}{25}{figure.caption.24}\protected@file@percent }
\newlabel{fig:sample_pose_scannet_vkitti_2}{{3.5}{25}{Sample of Virtual Kitti 2 dataset pose. Each point represent the pose of the camera in 3D frame when a particular frame is captured. The representation shows how the camera is moving in the 3D world.\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces RGB, Label and Pose sample of Scannet and Vkitti data\relax }}{26}{figure.caption.25}\protected@file@percent }
\newlabel{fig:scannet_vkitti}{{3.6}{26}{RGB, Label and Pose sample of Scannet and Vkitti data\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Scannet data distribution. Horizontal axis represent the video sequence number and vertical axis represent the number of frames in each video sequence.\relax }}{26}{figure.caption.26}\protected@file@percent }
\newlabel{fig:scannet_vkitti}{{3.7}{26}{Scannet data distribution. Horizontal axis represent the video sequence number and vertical axis represent the number of frames in each video sequence.\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Vkitti data distribution. Horizontal axis represent the scenes and under each scene there is 15-deg-left, fog, overcast, morning, 30-deg-right, 15-deg-right, 30-deg-left, clone, rain and sunset categories.\relax }}{27}{figure.caption.27}\protected@file@percent }
\newlabel{fig:scannet_vkitti}{{3.8}{27}{Vkitti data distribution. Horizontal axis represent the scenes and under each scene there is 15-deg-left, fog, overcast, morning, 30-deg-right, 15-deg-right, 30-deg-left, clone, rain and sunset categories.\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Unet model architecture. Generated with modification \href  {https://github.com/HarisIqbal88/PlotNeuralNet}{PlotNeuralNet}\relax }}{28}{figure.caption.28}\protected@file@percent }
\newlabel{fig:unet_model}{{3.9}{28}{Unet model architecture. Generated with modification \href {https://github.com/HarisIqbal88/PlotNeuralNet}{PlotNeuralNet}\relax }{figure.caption.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}U-Net With Gaussian Process}{29}{subsection.3.3.2}\protected@file@percent }
\newlabel{eq:distance_matrix}{{3.1}{29}{U-Net With Gaussian Process}{equation.3.3.1}{}}
\newlabel{eq:covariance_matrix}{{3.2}{29}{U-Net With Gaussian Process}{equation.3.3.2}{}}
\newlabel{eq:gp}{{3.3}{29}{U-Net With Gaussian Process}{equation.3.3.3}{}}
\newlabel{eq:encode_output}{{3.4}{29}{U-Net With Gaussian Process}{equation.3.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}U-Net with Long Short Term Memory (LSTM)}{29}{subsection.3.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Unet model architecture with temporal fusion in latent space using Gaussian Process.The latent space encoding is propagated forward. During every computation the Gaussian Process takes latent space encoding, previous updated latent space and the current and previous pose to update the current latent space encoding. Generated with \href  {https://app.diagrams.net/}{Diagrams.net}\relax }}{30}{figure.caption.29}\protected@file@percent }
\newlabel{fig:unet_gp}{{3.10}{30}{Unet model architecture with temporal fusion in latent space using Gaussian Process.The latent space encoding is propagated forward. During every computation the Gaussian Process takes latent space encoding, previous updated latent space and the current and previous pose to update the current latent space encoding. Generated with \href {https://app.diagrams.net/}{Diagrams.net}\relax }{figure.caption.29}{}}
\newlabel{eq:it}{{3.5}{30}{U-Net with Long Short Term Memory (LSTM)}{equation.3.3.5}{}}
\newlabel{eq:ft}{{3.6}{30}{U-Net with Long Short Term Memory (LSTM)}{equation.3.3.6}{}}
\newlabel{eq:ot}{{3.7}{30}{U-Net with Long Short Term Memory (LSTM)}{equation.3.3.7}{}}
\newlabel{eq:gt}{{3.8}{30}{U-Net with Long Short Term Memory (LSTM)}{equation.3.3.8}{}}
\newlabel{eq:Ct}{{3.9}{30}{U-Net with Long Short Term Memory (LSTM)}{equation.3.3.9}{}}
\newlabel{eq:Ht}{{3.10}{30}{U-Net with Long Short Term Memory (LSTM)}{equation.3.3.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Unet model architecture with temporal fusion in latent space using the ConvLSTM cell. At every frames semantic label computation the previous convolutional LSTM cell hidden and cell state are passed to the current cell computation. Thereby modeling the temporal dependency in the consecutive frames. Generated with \href  {https://app.diagrams.net/}{Diagrams.net}\relax }}{31}{figure.caption.30}\protected@file@percent }
\newlabel{fig:unet_lstm}{{3.11}{31}{Unet model architecture with temporal fusion in latent space using the ConvLSTM cell. At every frames semantic label computation the previous convolutional LSTM cell hidden and cell state are passed to the current cell computation. Thereby modeling the temporal dependency in the consecutive frames. Generated with \href {https://app.diagrams.net/}{Diagrams.net}\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Training pipeline. Generated with \href  {https://app.diagrams.net/}{Diagrams.net}\relax }}{32}{figure.caption.31}\protected@file@percent }
\newlabel{fig:unet_training}{{3.12}{32}{Training pipeline. Generated with \href {https://app.diagrams.net/}{Diagrams.net}\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Training and Evaluation Pipeline}{32}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Hardware Configuration}{32}{section.3.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Evaluation pipeline. Generated with \href  {https://app.diagrams.net/}{Diagrams.net}\relax }}{33}{figure.caption.32}\protected@file@percent }
\newlabel{fig:unet_evaluation}{{3.13}{33}{Evaluation pipeline. Generated with \href {https://app.diagrams.net/}{Diagrams.net}\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Pictorial representation of training and evaluation procedure. The picture represent the how the training and evaluation is conducted with and without the temporal fusion. In the Vanilla network the model is trained on individual frames without temporal fusion and in the GP and LSTM model temporal data is propagated from one frame to another. Generated with \href  {https://app.diagrams.net/}{Diagrams.net}\relax }}{34}{figure.caption.33}\protected@file@percent }
\newlabel{fig:unet_evaluation}{{3.14}{34}{Pictorial representation of training and evaluation procedure. The picture represent the how the training and evaluation is conducted with and without the temporal fusion. In the Vanilla network the model is trained on individual frames without temporal fusion and in the GP and LSTM model temporal data is propagated from one frame to another. Generated with \href {https://app.diagrams.net/}{Diagrams.net}\relax }{figure.caption.33}{}}
\citation{79_dai2017scannet}
\citation{80_cabon2020vkitti2}
\citation{81_minodaRAL2021}
\citation{84_ulku2022survey}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Evaluation and Experimental Result}{35}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:evaluationandresult}{{4}{35}{Evaluation and Experimental Result}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Evaluation Metric}{35}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Pixel Accuracy (PA), Mean Pixel Accuracy (mPA)}{35}{subsection.4.1.1}\protected@file@percent }
\citation{83_iou}
\citation{82_iou}
\citation{82_iou}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}IoU, mIoU, FwIoU}{36}{subsection.4.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Definition of IoU. Courtesy of \cite  {82_iou}\relax }}{36}{figure.caption.34}\protected@file@percent }
\newlabel{fig:IoU}{{4.1}{36}{Definition of IoU. Courtesy of \cite {82_iou}\relax }{figure.caption.34}{}}
\citation{84_ulku2022survey}
\citation{84_ulku2022survey}
\citation{78_hu2020temporally}
\citation{52_hou2019multi}
\citation{03_duzceker2021deepvideomvs}
\citation{85_kag_challenge}
\citation{79_dai2017scannet}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Hypothesis}{37}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}RQ3: How to Cross-Transfer the Temporal Fusion Technique to Semantic Segmentation?}{37}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Merging Scannet Dataset Classes for Experiment}{38}{subsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Distance and Kernel Matrix}{38}{subsection.4.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Per class pixel distribution of the entire scannet dataset. X-axis represent the labels in the Scannet dataset and Y-axis represent the number of pixels per class. High number of pixels belongs to class 1 and class 2\relax }}{39}{figure.caption.35}\protected@file@percent }
\newlabel{fig:scannet_class}{{4.2}{39}{Per class pixel distribution of the entire scannet dataset. X-axis represent the labels in the Scannet dataset and Y-axis represent the number of pixels per class. High number of pixels belongs to class 1 and class 2\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Pixel distribution for the scannet data containing all the classes\relax }}{39}{figure.caption.36}\protected@file@percent }
\newlabel{fig:scannet_all_classes}{{4.3}{39}{Pixel distribution for the scannet data containing all the classes\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Pixel distribution for the scannet data for two classes. There are only two classes because all the classes except class 1 are combined with the 0th class. Horizontal axis represent the class and vertical axis represent the number of pixels belonging to that particular class.\relax }}{40}{figure.caption.37}\protected@file@percent }
\newlabel{fig:scannet_two_classes}{{4.4}{40}{Pixel distribution for the scannet data for two classes. There are only two classes because all the classes except class 1 are combined with the 0th class. Horizontal axis represent the class and vertical axis represent the number of pixels belonging to that particular class.\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Pixel distribution for the scannet data for three classes. There are only three classes because all the classes except class 1 and 2 are combined with the 0th class. Horizontal axis represent the class and vertical axis represent the number of pixels belonging to that particular class.\relax }}{40}{figure.caption.38}\protected@file@percent }
\newlabel{fig:scannet_three_classes}{{4.5}{40}{Pixel distribution for the scannet data for three classes. There are only three classes because all the classes except class 1 and 2 are combined with the 0th class. Horizontal axis represent the class and vertical axis represent the number of pixels belonging to that particular class.\relax }{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Ordered and Unordered set of images. Ordered set of images are the images from the video sequence without shuffling the frames. In unordered set of images the frames are shuffled. \relax }}{41}{figure.caption.39}\protected@file@percent }
\newlabel{fig:ordered_and_unordered_images}{{4.6}{41}{Ordered and Unordered set of images. Ordered set of images are the images from the video sequence without shuffling the frames. In unordered set of images the frames are shuffled. \relax }{figure.caption.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Experiment 1.1: U-Net Vanilla, and Temporally Fused GP and LSTM Model on Scannet Dataset Without Merging Classes}{41}{subsection.4.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Distance matrix and Kernel matrix for ordered set of images. Distance matrix represent the distance between the consecutive eight frames poses with respect to each other. Kernel matrix represent the covariance between the consecutive eight frame poses.\relax }}{42}{figure.caption.40}\protected@file@percent }
\newlabel{fig:ordered_D_and_K}{{4.7}{42}{Distance matrix and Kernel matrix for ordered set of images. Distance matrix represent the distance between the consecutive eight frames poses with respect to each other. Kernel matrix represent the covariance between the consecutive eight frame poses.\relax }{figure.caption.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Distance matrix and Kernel matrix for unordered set of images. Distance matrix represent the distance between the shuffled eight frames poses with respect to each other. Kernel matrix represent the covariance between the shuffled eight frame poses.\relax }}{42}{figure.caption.41}\protected@file@percent }
\newlabel{fig:unordered_D_and_K}{{4.8}{42}{Distance matrix and Kernel matrix for unordered set of images. Distance matrix represent the distance between the shuffled eight frames poses with respect to each other. Kernel matrix represent the covariance between the shuffled eight frame poses.\relax }{figure.caption.41}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Vanilla model performance with respect to all the metrics. Higher values means top performing model.\relax }}{43}{table.caption.42}\protected@file@percent }
\newlabel{tab:caption}{{4.1}{43}{Vanilla model performance with respect to all the metrics. Higher values means top performing model.\relax }{table.caption.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces IoU for Vanilla model considering all classes in dataset\relax }}{43}{figure.caption.43}\protected@file@percent }
\newlabel{fig:scannet_class}{{4.9}{43}{IoU for Vanilla model considering all classes in dataset\relax }{figure.caption.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Pixel distribution for the ground truth and predicted scannet data for Vanilla unet model\relax }}{43}{figure.caption.44}\protected@file@percent }
\newlabel{fig:y_gt_pred_vanilla}{{4.10}{43}{Pixel distribution for the ground truth and predicted scannet data for Vanilla unet model\relax }{figure.caption.44}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces GP model performance with respect to all the metrics. Higher values means top performing model.\relax }}{44}{table.caption.45}\protected@file@percent }
\newlabel{tab:caption}{{4.2}{44}{GP model performance with respect to all the metrics. Higher values means top performing model.\relax }{table.caption.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces IoU for GP model considering all classes in dataset\relax }}{44}{figure.caption.46}\protected@file@percent }
\newlabel{fig:scannet_class}{{4.11}{44}{IoU for GP model considering all classes in dataset\relax }{figure.caption.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces Per class pixel distribution of the predicted pixel class label for GP model\relax }}{45}{figure.caption.47}\protected@file@percent }
\newlabel{fig:y_gt_and_predic_gp}{{4.12}{45}{Per class pixel distribution of the predicted pixel class label for GP model\relax }{figure.caption.47}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces LSTM model performance with respect to all the metrics. Higher values means top performing model.\relax }}{45}{table.caption.48}\protected@file@percent }
\newlabel{tab:unet_lstm}{{4.3}{45}{LSTM model performance with respect to all the metrics. Higher values means top performing model.\relax }{table.caption.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces IoU for LSTM model considering all classes in dataset\relax }}{46}{figure.caption.49}\protected@file@percent }
\newlabel{fig:scannet_class}{{4.13}{46}{IoU for LSTM model considering all classes in dataset\relax }{figure.caption.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces Per class pixel distribution of the predicted pixel class label for LSTM model\relax }}{46}{figure.caption.50}\protected@file@percent }
\newlabel{fig:y_gt_and_predic_lstm}{{4.14}{46}{Per class pixel distribution of the predicted pixel class label for LSTM model\relax }{figure.caption.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces Comparison of Vanilla, GP and LSTM model performance based on accuracy metric. Higher the value means top performing model.\relax }}{47}{figure.caption.51}\protected@file@percent }
\newlabel{fig:unet_model_metric_comparison_all_classes}{{4.15}{47}{Comparison of Vanilla, GP and LSTM model performance based on accuracy metric. Higher the value means top performing model.\relax }{figure.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.16}{\ignorespaces Comparison of Vanilla, GP and LSTM model performance based on mean accuracy metric. Higher the value means top performing model.\relax }}{48}{figure.caption.52}\protected@file@percent }
\newlabel{fig:unet_model_metric_comparison_all_classes}{{4.16}{48}{Comparison of Vanilla, GP and LSTM model performance based on mean accuracy metric. Higher the value means top performing model.\relax }{figure.caption.52}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.17}{\ignorespaces Comparison of Vanilla, GP and LSTM model performance based on meanIoU metric. Higher the value means top performing model.\relax }}{49}{figure.caption.53}\protected@file@percent }
\newlabel{fig:unet_model_metric_comparison_all_classes}{{4.17}{49}{Comparison of Vanilla, GP and LSTM model performance based on meanIoU metric. Higher the value means top performing model.\relax }{figure.caption.53}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.18}{\ignorespaces Comparison of Vanilla, GP and LSTM model performance based on FwIoU metric. Higher the value means top performing model.\relax }}{50}{figure.caption.54}\protected@file@percent }
\newlabel{fig:unet_model_metric_comparison_all_classes}{{4.18}{50}{Comparison of Vanilla, GP and LSTM model performance based on FwIoU metric. Higher the value means top performing model.\relax }{figure.caption.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.19}{\ignorespaces Plotting of raw input image, ground truth and predicted output for Vanilla, GP and LSTM model\relax }}{51}{figure.caption.55}\protected@file@percent }
\newlabel{fig:unet_model}{{4.19}{51}{Plotting of raw input image, ground truth and predicted output for Vanilla, GP and LSTM model\relax }{figure.caption.55}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Experiment 1.2: U-Net Vanilla, and Temporally Fused GP and Lstm Model on Scannet Dataset With Merging Into Two Classes}{52}{subsection.4.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.20}{\ignorespaces Plotting of raw input image, ground truth and predicted output for Vanilla, GP and LSTM model for two class scannet dataset\relax }}{52}{figure.caption.56}\protected@file@percent }
\newlabel{fig:output_vanilla}{{4.20}{52}{Plotting of raw input image, ground truth and predicted output for Vanilla, GP and LSTM model for two class scannet dataset\relax }{figure.caption.56}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Performance of Vanilla model with respect to different metric and two classes. Higher the value means top performing model.\relax }}{52}{table.caption.57}\protected@file@percent }
\newlabel{table:unet_two_classes}{{4.4}{52}{Performance of Vanilla model with respect to different metric and two classes. Higher the value means top performing model.\relax }{table.caption.57}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.21}{\ignorespaces IoU for Vanilla, GP and LSTM comparison side by side for two classes in scannet dataset\relax }}{53}{figure.caption.58}\protected@file@percent }
\newlabel{fig:unet_model_metric_comparison}{{4.21}{53}{IoU for Vanilla, GP and LSTM comparison side by side for two classes in scannet dataset\relax }{figure.caption.58}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.22}{\ignorespaces Comparison of Vanilla, GP and LSTM model performance based on accuracy metric for scannet two classes\relax }}{53}{figure.caption.59}\protected@file@percent }
\newlabel{fig:unet_model_metric_comparison}{{4.22}{53}{Comparison of Vanilla, GP and LSTM model performance based on accuracy metric for scannet two classes\relax }{figure.caption.59}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.23}{\ignorespaces Comparison of Vanilla, GP and LSTM model performance based mean accuracy on metric for scannet two classes\relax }}{54}{figure.caption.60}\protected@file@percent }
\newlabel{fig:unet_model_metric_comparison}{{4.23}{54}{Comparison of Vanilla, GP and LSTM model performance based mean accuracy on metric for scannet two classes\relax }{figure.caption.60}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.24}{\ignorespaces Comparison of Vanilla, GP and LSTM model performance based on meanIoU metric for scannet two classes\relax }}{55}{figure.caption.61}\protected@file@percent }
\newlabel{fig:unet_model_metric_comparison}{{4.24}{55}{Comparison of Vanilla, GP and LSTM model performance based on meanIoU metric for scannet two classes\relax }{figure.caption.61}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.25}{\ignorespaces Comparison of Vanilla, GP and LSTM model performance based on FwIoU metric for scannet two classes\relax }}{56}{figure.caption.62}\protected@file@percent }
\newlabel{fig:unet_model_metric_comparison}{{4.25}{56}{Comparison of Vanilla, GP and LSTM model performance based on FwIoU metric for scannet two classes\relax }{figure.caption.62}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.26}{\ignorespaces Plotting of raw input image, ground truth and predicted output for Vanilla, GP and LSTM model for three class scannet dataset\relax }}{57}{figure.caption.63}\protected@file@percent }
\newlabel{fig:performance_metric_three_classes_scannet}{{4.26}{57}{Plotting of raw input image, ground truth and predicted output for Vanilla, GP and LSTM model for three class scannet dataset\relax }{figure.caption.63}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.5}Experiment 1.3: U-Net Vanilla, and Temporally Fused GP and LSTM Model on Scannet Dataset With Merging Into Three Classes}{57}{subsection.4.3.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Performance of Vanilla, GP and LSTM model with respect to different metric and three class scannet dataset. Higher the value means top performing model.\relax }}{57}{table.caption.64}\protected@file@percent }
\newlabel{table:unet_scannet_three_classes}{{4.5}{57}{Performance of Vanilla, GP and LSTM model with respect to different metric and three class scannet dataset. Higher the value means top performing model.\relax }{table.caption.64}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.27}{\ignorespaces IoU for Vanilla, GP and LSTM comparison side by side for three classes in scannet dataset\relax }}{58}{figure.caption.65}\protected@file@percent }
\newlabel{fig:performance_metric_three_classes_unet}{{4.27}{58}{IoU for Vanilla, GP and LSTM comparison side by side for three classes in scannet dataset\relax }{figure.caption.65}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.28}{\ignorespaces Comparison of Vanilla, GP and LSTM model performance based on accuracy metric for scannet three class scannet dataset. Higher the value means top performing model.\relax }}{58}{figure.caption.66}\protected@file@percent }
\newlabel{fig:performance_metric_three_classes_unet}{{4.28}{58}{Comparison of Vanilla, GP and LSTM model performance based on accuracy metric for scannet three class scannet dataset. Higher the value means top performing model.\relax }{figure.caption.66}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.29}{\ignorespaces Comparison of Vanilla, GP and LSTM model performance based on mean accuracy metric for three class scannet dataset. Higher the value means top performing model.\relax }}{59}{figure.caption.67}\protected@file@percent }
\newlabel{fig:performance_metric_three_classes_unet}{{4.29}{59}{Comparison of Vanilla, GP and LSTM model performance based on mean accuracy metric for three class scannet dataset. Higher the value means top performing model.\relax }{figure.caption.67}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.30}{\ignorespaces Comparison of Vanilla, GP and LSTM model performance based on meanIoU metric for three class scannet dataset. Higher the value means top performing model.\relax }}{60}{figure.caption.68}\protected@file@percent }
\newlabel{fig:performance_metric_three_classes_unet}{{4.30}{60}{Comparison of Vanilla, GP and LSTM model performance based on meanIoU metric for three class scannet dataset. Higher the value means top performing model.\relax }{figure.caption.68}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.31}{\ignorespaces Comparison of Vanilla, GP and LSTM model performance based FwIoU on metric for three class scannet dataset. Higher the value means top performing model.\relax }}{61}{figure.caption.69}\protected@file@percent }
\newlabel{fig:performance_metric_three_classes_unet}{{4.31}{61}{Comparison of Vanilla, GP and LSTM model performance based FwIoU on metric for three class scannet dataset. Higher the value means top performing model.\relax }{figure.caption.69}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces Performance of Vanilla, GP and LSTM model with respect to different metric. Higher the value means top performing model.\relax }}{62}{table.caption.71}\protected@file@percent }
\newlabel{table:unet_vkitti_two_classes}{{4.6}{62}{Performance of Vanilla, GP and LSTM model with respect to different metric. Higher the value means top performing model.\relax }{table.caption.71}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.6}Experiment 1.4: U-Net Vanilla, and Temporally Fused GP and LSTM Model on Vkitti Dataset With Fog and Morning Categories as Testing Data}{62}{subsection.4.3.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.32}{\ignorespaces Plotting of raw input image, ground truth and predicted output for Vanilla, GP and LSTM for vkitti dataset with fog and morning as the validation dataset\relax }}{63}{figure.caption.70}\protected@file@percent }
\newlabel{fig:vkitti_unet_two}{{4.32}{63}{Plotting of raw input image, ground truth and predicted output for Vanilla, GP and LSTM for vkitti dataset with fog and morning as the validation dataset\relax }{figure.caption.70}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.7}Experiment 1.5: Impact of Training Data Batch Size on the Model Performance}{63}{subsection.4.3.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.33}{\ignorespaces IoU for Vanilla model tested with vkitti dataset with fog and morning as the validation data\relax }}{64}{figure.caption.72}\protected@file@percent }
\newlabel{fig:performance_metric_vkitti_two_class_box_plot}{{4.33}{64}{IoU for Vanilla model tested with vkitti dataset with fog and morning as the validation data\relax }{figure.caption.72}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.34}{\ignorespaces IoU for GP model tested with vkitti dataset with fog and morning as the validation data\relax }}{64}{figure.caption.73}\protected@file@percent }
\newlabel{fig:performance_metric_vkitti_two_class_box_plot}{{4.34}{64}{IoU for GP model tested with vkitti dataset with fog and morning as the validation data\relax }{figure.caption.73}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.35}{\ignorespaces IoU for LSTM model tested with vkitti dataset with fog and morning as the validation data\relax }}{65}{figure.caption.74}\protected@file@percent }
\newlabel{fig:performance_metric_vkitti_two_class_box_plot}{{4.35}{65}{IoU for LSTM model tested with vkitti dataset with fog and morning as the validation data\relax }{figure.caption.74}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.36}{\ignorespaces Performance comparison of the Vanilla, GP and LSTM model for fog and morning as the validation data. Higher the value means top performing model.\relax }}{65}{figure.caption.75}\protected@file@percent }
\newlabel{fig:performance_metric_vkitti_two_class_box_plot}{{4.36}{65}{Performance comparison of the Vanilla, GP and LSTM model for fog and morning as the validation data. Higher the value means top performing model.\relax }{figure.caption.75}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.37}{\ignorespaces Side by side comparison of models predictions for continuous sequence data from frame 0 to 3\relax }}{66}{figure.caption.76}\protected@file@percent }
\newlabel{fig:performance_metric_three_classes}{{4.37}{66}{Side by side comparison of models predictions for continuous sequence data from frame 0 to 3\relax }{figure.caption.76}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.38}{\ignorespaces Side by side comparison of models predictions for continuous sequence data from frame 4 to 7\relax }}{67}{figure.caption.77}\protected@file@percent }
\newlabel{fig:performance_metric_three_classes}{{4.38}{67}{Side by side comparison of models predictions for continuous sequence data from frame 4 to 7\relax }{figure.caption.77}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.39}{\ignorespaces Impact of training batch size on the evaluation dataset prediction. The model is trained with batch of 2, 4 and 8. The performance of the model is plotted with different metrics. Higher the value means top performing model.\relax }}{68}{figure.caption.78}\protected@file@percent }
\newlabel{fig:batch_wise_performance}{{4.39}{68}{Impact of training batch size on the evaluation dataset prediction. The model is trained with batch of 2, 4 and 8. The performance of the model is plotted with different metrics. Higher the value means top performing model.\relax }{figure.caption.78}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces Performance of Vanilla model with respect to different metric with clone, sunset, rain, 15 degree right, and 30 degree left as validation data. Higher the value means top performing model.\relax }}{68}{table.caption.80}\protected@file@percent }
\newlabel{table:Vanilla_conti_seq}{{4.7}{68}{Performance of Vanilla model with respect to different metric with clone, sunset, rain, 15 degree right, and 30 degree left as validation data. Higher the value means top performing model.\relax }{table.caption.80}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.8}Experiment 1.6: U-Net Vanilla, and Temporally Fused GP and LSTM Model on Vkitti Dataset With Clone, Sunset, Rain, 15-Deg-Right, and 30-Deg-Left Categories as Testing Data}{68}{subsection.4.3.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.40}{\ignorespaces Plotting of raw input image, ground truth and predicted output for Vanilla, GP and LSTM for vkitti dataset with five categories of dataset taken for validation.\relax }}{69}{figure.caption.79}\protected@file@percent }
\newlabel{fig:unet_side_by_side_five_classes}{{4.40}{69}{Plotting of raw input image, ground truth and predicted output for Vanilla, GP and LSTM for vkitti dataset with five categories of dataset taken for validation.\relax }{figure.caption.79}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.41}{\ignorespaces IoU for Vanilla model with clone, sunset, rain, 15 degree right, and 30 degree left as validation data\relax }}{69}{figure.caption.81}\protected@file@percent }
\newlabel{fig:performance_metric_unet1}{{4.41}{69}{IoU for Vanilla model with clone, sunset, rain, 15 degree right, and 30 degree left as validation data\relax }{figure.caption.81}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.42}{\ignorespaces IoU for GP model with clone, sunset, rain, 15 degree right, and 30 degree left as validation data\relax }}{70}{figure.caption.82}\protected@file@percent }
\newlabel{fig:performance_metric_unet2}{{4.42}{70}{IoU for GP model with clone, sunset, rain, 15 degree right, and 30 degree left as validation data\relax }{figure.caption.82}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.43}{\ignorespaces IoU for LSTM model with clone, sunset, rain, 15 degree right, and 30 degree left as validation data\relax }}{70}{figure.caption.83}\protected@file@percent }
\newlabel{fig:performance_metric_unet3}{{4.43}{70}{IoU for LSTM model with clone, sunset, rain, 15 degree right, and 30 degree left as validation data\relax }{figure.caption.83}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.44}{\ignorespaces Comparison of Vanilla, GP, and LSTM model performance with clone, sunset, rain, 15-deg-right, 30-deg-left as the validation data. Higher the value means top performing model.\relax }}{71}{figure.caption.84}\protected@file@percent }
\newlabel{fig:performance_metric_unet}{{4.44}{71}{Comparison of Vanilla, GP, and LSTM model performance with clone, sunset, rain, 15-deg-right, 30-deg-left as the validation data. Higher the value means top performing model.\relax }{figure.caption.84}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.8}{\ignorespaces Comparison of Vanilla, GP and LSTM model predictions based on four metrics and three different conducted experiments for scannet dataset. In all the experiments LSTM model out performed Vanilla and GP.\relax }}{71}{table.caption.85}\protected@file@percent }
\newlabel{table:hype1}{{4.8}{71}{Comparison of Vanilla, GP and LSTM model predictions based on four metrics and three different conducted experiments for scannet dataset. In all the experiments LSTM model out performed Vanilla and GP.\relax }{table.caption.85}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}RQ3 Conclusion}{71}{section.4.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.9}{\ignorespaces Comparison of Vanilla, GP and LSTM model predictions based on four metrics and three different conducted experiments for vkitti dataset. In all the experiments LSTM model out-performed Vanilla and GP.\relax }}{72}{table.caption.86}\protected@file@percent }
\newlabel{table:hype2}{{4.9}{72}{Comparison of Vanilla, GP and LSTM model predictions based on four metrics and three different conducted experiments for vkitti dataset. In all the experiments LSTM model out-performed Vanilla and GP.\relax }{table.caption.86}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.10}{\ignorespaces The Vanilla, GP and LSTM model are trained with different batch sizes. The model prediction is compared side by side with different metrics. Hidden pattern in the data is learned quickly with increase in the batch size.\relax }}{72}{table.caption.87}\protected@file@percent }
\newlabel{table:hype3}{{4.10}{72}{The Vanilla, GP and LSTM model are trained with different batch sizes. The model prediction is compared side by side with different metrics. Hidden pattern in the data is learned quickly with increase in the batch size.\relax }{table.caption.87}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Hyper-Parameter and Configuration Details }{73}{section.4.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.11}{\ignorespaces Hyperparameter to train the model.\relax }}{73}{table.caption.88}\protected@file@percent }
\newlabel{table:parameters}{{4.11}{73}{Hyperparameter to train the model.\relax }{table.caption.88}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Android Deployment}{75}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:androiddeploy}{{5}{75}{Android Deployment}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Framework}{75}{section.5.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Specification of the android device\relax }}{75}{table.caption.90}\protected@file@percent }
\newlabel{table:android_spec}{{5.1}{75}{Specification of the android device\relax }{table.caption.90}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Kotlin integration with python and sequence of steps from loading the image to display of predicted semantic map.\relax }}{76}{figure.caption.89}\protected@file@percent }
\newlabel{fig:android_pipeline}{{5.1}{76}{Kotlin integration with python and sequence of steps from loading the image to display of predicted semantic map.\relax }{figure.caption.89}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Display of loading the image and predicted semantic map\relax }}{77}{figure.caption.91}\protected@file@percent }
\newlabel{fig:android_result}{{5.2}{77}{Display of loading the image and predicted semantic map\relax }{figure.caption.91}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Display of Results}{77}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Runtime}{78}{section.5.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Runtime detail for processing of single image\relax }}{78}{table.caption.92}\protected@file@percent }
\newlabel{table:response_of_android}{{5.2}{78}{Runtime detail for processing of single image\relax }{table.caption.92}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusions}{79}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:conclusion}{{6}{79}{Conclusions}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Contributions}{80}{section.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Limitation}{81}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Future Work}{81}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Appendix \numberline {A}Design Details}{83}{appendix.a.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{lst:unetbaseline}{{A.1}{83}{Unet Vanilla model architecture description}{lstlisting.a.A.1}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {A.1}Unet Vanilla model architecture description}{83}{lstlisting.a.A.1}\protected@file@percent }
\newlabel{lst:unetgp}{{A.2}{86}{Unet GP model architecture description}{lstlisting.a.A.2}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {A.2}Unet GP model architecture description}{86}{lstlisting.a.A.2}\protected@file@percent }
\newlabel{lst:unetlstm}{{A.3}{89}{Unet LSTM model architecture description}{lstlisting.a.A.3}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {A.3}Unet LSTM model architecture description}{89}{lstlisting.a.A.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Appendix \numberline {B}Parameters}{93}{appendix.a.B}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces Classes and ids of the Scannet dataset\relax }}{94}{table.caption.93}\protected@file@percent }
\newlabel{table:Classes in scannet_1}{{B.1}{94}{Classes and ids of the Scannet dataset\relax }{table.caption.93}{}}
\@writefile{lot}{\contentsline {table}{\numberline {B.2}{\ignorespaces Classes and ids of the Scannet dataset\relax }}{95}{table.caption.94}\protected@file@percent }
\newlabel{table:Classes in scannet_2}{{B.2}{95}{Classes and ids of the Scannet dataset\relax }{table.caption.94}{}}
\@writefile{lot}{\contentsline {table}{\numberline {B.3}{\ignorespaces Classes and ids of the Scannet dataset\relax }}{96}{table.caption.95}\protected@file@percent }
\newlabel{table:Classes in scannet_3}{{B.3}{96}{Classes and ids of the Scannet dataset\relax }{table.caption.95}{}}
\@writefile{lot}{\contentsline {table}{\numberline {B.4}{\ignorespaces Vkitti dataset classes\relax }}{97}{table.caption.96}\protected@file@percent }
\newlabel{table:Classes in scannet_2}{{B.4}{97}{Vkitti dataset classes\relax }{table.caption.96}{}}
\@writefile{toc}{\contentsline {chapter}{Appendix \numberline {C}Training Details}{99}{appendix.a.C}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {C.1}{\ignorespaces Scannet step losses for vanilla\relax }}{99}{figure.caption.97}\protected@file@percent }
\newlabel{fig:android_result}{{C.1}{99}{Scannet step losses for vanilla\relax }{figure.caption.97}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {C.2}{\ignorespaces Scannet epoch losses for vanilla\relax }}{100}{figure.caption.98}\protected@file@percent }
\newlabel{fig:android_result}{{C.2}{100}{Scannet epoch losses for vanilla\relax }{figure.caption.98}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {C.3}{\ignorespaces Scannet step losses for GP\relax }}{100}{figure.caption.99}\protected@file@percent }
\newlabel{fig:android_result}{{C.3}{100}{Scannet step losses for GP\relax }{figure.caption.99}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {C.4}{\ignorespaces Scannet epoch losses for GP\relax }}{100}{figure.caption.100}\protected@file@percent }
\newlabel{fig:android_result}{{C.4}{100}{Scannet epoch losses for GP\relax }{figure.caption.100}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {C.5}{\ignorespaces Scannet step losses for LSTM\relax }}{101}{figure.caption.101}\protected@file@percent }
\newlabel{fig:android_result}{{C.5}{101}{Scannet step losses for LSTM\relax }{figure.caption.101}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {C.6}{\ignorespaces Scannet epoch losses for LSTM\relax }}{101}{figure.caption.102}\protected@file@percent }
\newlabel{fig:android_result}{{C.6}{101}{Scannet epoch losses for LSTM\relax }{figure.caption.102}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {C.7}{\ignorespaces Vkitti step loss vanilla\relax }}{101}{figure.caption.103}\protected@file@percent }
\newlabel{fig:android_result}{{C.7}{101}{Vkitti step loss vanilla\relax }{figure.caption.103}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {C.8}{\ignorespaces Vkitti step loss GP\relax }}{102}{figure.caption.104}\protected@file@percent }
\newlabel{fig:android_result}{{C.8}{102}{Vkitti step loss GP\relax }{figure.caption.104}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {C.9}{\ignorespaces Vkitti step loss LSTM\relax }}{102}{figure.caption.105}\protected@file@percent }
\newlabel{fig:android_result}{{C.9}{102}{Vkitti step loss LSTM\relax }{figure.caption.105}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {C.10}{\ignorespaces Learning rate finder\relax }}{102}{figure.caption.106}\protected@file@percent }
\newlabel{fig:android_result}{{C.10}{102}{Learning rate finder\relax }{figure.caption.106}{}}
\bibstyle{unsrt}
\bibdata{references}
\bibcite{35_mldl}{1}
\bibcite{52_hou2019multi}{2}
\bibcite{55_WinNT}{3}
\bibcite{60_minaee2021image}{4}
\bibcite{64_noh2015learning}{5}
\bibcite{62_badrinarayanan2017segnet}{6}
\bibcite{70_ronneberger2015u}{7}
\bibcite{78_hu2020temporally}{8}
\bibcite{82_iou}{9}
\bibcite{01_mandic2005data}{10}
\bibcite{06_castanedo2013review}{11}
\bibcite{02_lim2021temporal}{12}
\@writefile{toc}{\contentsline {chapter}{References}{103}{appendix*.107}\protected@file@percent }
\bibcite{03_duzceker2021deepvideomvs}{13}
\bibcite{04_li2021spatial}{14}
\bibcite{005_hsiao2005temporal}{15}
\bibcite{07_hsiao2005temporal}{16}
\bibcite{08_krause2003unsupervised}{17}
\bibcite{09_lee2003line}{18}
\bibcite{10_han2016seq}{19}
\bibcite{11_kang2017t}{20}
\bibcite{12_ning2017spatially}{21}
\bibcite{13_lu2020retinatrack}{22}
\bibcite{14_kopuklu2019you}{23}
\bibcite{15_feichtenhofer2016convolutional}{24}
\bibcite{16_wang2016temporal}{25}
\bibcite{17_erccelik2021temp}{26}
\bibcite{18_zhu2009spatial}{27}
\bibcite{19_wu2003multi}{28}
\bibcite{20_teutsch2012spatio}{29}
\bibcite{21_forsyth2011computer}{30}
\bibcite{22_dai2016instance}{31}
\bibcite{23_fu1981survey}{32}
\bibcite{24_ladys1994colour}{33}
\bibcite{25_minaee2021image}{34}
\bibcite{26_ronneberger2015u}{35}
\bibcite{27_lin2017refinenet}{36}
\bibcite{28_jegou2017one}{37}
\bibcite{29_iandola2014densenet}{38}
\bibcite{30_yang2018denseaspp}{39}
\bibcite{31_chen2014semantic}{40}
\bibcite{32_zhao2018icnet}{41}
\bibcite{33_deng2020lightweight}{42}
\bibcite{34_hsieh2010real}{43}
\bibcite{36_lecun2015deep}{44}
\bibcite{37_farabet2012learning}{45}
\bibcite{38_hinton2012deep}{46}
\bibcite{39_patel2020machine}{47}
\bibcite{40_ciodaro2012online}{48}
\bibcite{41_zhang2021deep}{49}
\bibcite{42_hirschberg2015advances}{50}
\bibcite{46_o2019deep}{51}
\bibcite{44_mohanty2016using}{52}
\bibcite{45_han2021ecological}{53}
\bibcite{43_srivastava2021comparative}{54}
\bibcite{47_minaee2021image}{55}
\bibcite{48_jensen1999temporal}{56}
\bibcite{49_atluri2018spatio}{57}
\bibcite{50_lim2021temporal}{58}
\bibcite{51_meng2021adafuse}{59}
\bibcite{53_duzceker2021deepvideomvs}{60}
\bibcite{54_zhang2021multiple}{61}
\bibcite{56_otsu1979threshold}{62}
\bibcite{57_otsu1979threshold}{63}
\bibcite{58_boykov2001fast}{64}
\bibcite{59_starck2005image}{65}
\bibcite{61_chen2017rethinking}{66}
\bibcite{63_goodfellow2014generative}{67}
\bibcite{65_kendall2015bayesian}{68}
\bibcite{66_sun2019high}{69}
\bibcite{67_fu2019stacked}{70}
\bibcite{68_hu2018learning}{71}
\bibcite{69_xia2017w}{72}
\bibcite{71_milletari2016v}{73}
\bibcite{72_jin2017video}{74}
\bibcite{73_gadde2017semantic}{75}
\bibcite{74_jin2017video}{76}
\bibcite{75_nilsson2018semantic}{77}
\bibcite{76_jain2019accel}{78}
\bibcite{77_mahasseni2017budget}{79}
\bibcite{79_dai2017scannet}{80}
\bibcite{80_cabon2020vkitti2}{81}
\bibcite{81_minodaRAL2021}{82}
\bibcite{84_ulku2022survey}{83}
\bibcite{83_iou}{84}
\bibcite{85_kag_challenge}{85}
\bibcite{05_hsiao2005temporal}{86}
\bibcite{86_li2022hybrid}{87}
\bibcite{87_heo2022vita}{88}
\bibcite{88_huang2022minvis}{89}
\bibcite{89_cheng2021mask2former}{90}
\bibcite{90_dai2017scannet}{91}
\bibcite{91_ScanNet}{92}
\bibcite{92_cabon2020virtual}{93}
\bibcite{93_ScanNet}{94}
\bibcite{94_ScanNet}{95}
\bibcite{95_mazzotti2016measure}{96}
\bibcite{96_GPML}{97}
\bibcite{97_williams2006gaussian}{98}
\bibcite{98_ConvLSTM}{99}
\bibcite{99_shi2015convolutional}{100}
\citation{*}
