{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "459f1193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/latai/anaconda3/envs/latai/lib/python3.7/site-packages/cv2/../../../../lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 60.5 MB 520 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /home/latai/anaconda3/envs/latai/lib/python3.7/site-packages (from opencv-python) (1.21.5)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.5.5.64\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c61dba6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl (750.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 750.6 MB 39 kB/s s eta 0:00:01     |██████████▋                     | 249.8 MB 10.7 MB/s eta 0:00:47     |███████████████▊                | 369.3 MB 8.3 MB/s eta 0:00:46\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/latai/anaconda3/envs/latai/lib/python3.7/site-packages (from torch) (4.1.1)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02796191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from numpy.linalg import inv\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9445cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices matrix refer Testpy_numpy.ipynb\n",
    "pixel_coordinate = np.indices([320, 256]).astype(np.float32)\n",
    "# Concatenate the pixel coordinate matrix ([2, 320, 256]) with the ones matrix [1, 320, 256] and the resultant matrix is [3, 320, 256]\n",
    "pixel_coordinate = np.concatenate(\n",
    "    (pixel_coordinate, np.ones([1, 320, 256])), axis=0)\n",
    "# Reshape refer Testpy_numpy.ipynb\n",
    "pixel_coordinate = np.reshape(pixel_coordinate, [3, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75e7c3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_poses = []\n",
    "\n",
    "# Get the extrinsic parameters i,e the position of the camera while capturing the images, rotation matrix and translation.\n",
    "with open('/home/latai/Documents/Master_thesis/Data/seq-01_formatted/poses.txt') as f:\n",
    "    for l in f.readlines():\n",
    "        l = l.strip('\\n')\n",
    "        gt_poses.append(np.array(l.split(' ')).astype(np.float32).reshape(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff728c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next image pose\n",
    "r_pose = gt_poses[1]\n",
    "# current image pose\n",
    "n_pose = gt_poses[0]\n",
    "\n",
    "# read the next frame image \n",
    "r_img = cv2.imread(\"/home/latai/Documents/Master_thesis/Data/seq-01_formatted/images/0001.png\")\n",
    "# read the current frame image\n",
    "n_img = cv2.imread(\"/home/latai/Documents/Master_thesis/Data/seq-01_formatted/images/0000.png\")\n",
    "\n",
    "K = np.loadtxt('/home/latai/Documents/Master_thesis/Data/seq-01_formatted//K.txt').astype(np.float32).reshape((3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30347380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.8744711  -0.33191165  0.3535954   0.33358625]\n",
      " [ 0.33531535  0.9405307   0.05358316  0.1364616 ]\n",
      " [-0.3503717   0.07171355  0.933794   -0.53893393]\n",
      " [ 0.          0.          0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "left_pose = r_pose\n",
    "right_pose = n_pose\n",
    "print(left_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e1000f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.87456155  0.33534923 -0.35040918 -0.5263514 ]\n",
      " [-0.33194345  0.9406187   0.0717207   0.02102616]\n",
      " [ 0.35363948  0.05358973  0.9339139   0.37803566]\n",
      " [ 0.          0.          0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(inv(left_pose))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136d846c",
   "metadata": {},
   "source": [
    "- The dot product tells you what amount of one vector goes in the direction of another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "263826b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9997334 ,  0.02308935, -0.00166941, -0.07349059],\n",
       "       [-0.02311522,  0.99957764, -0.01766039, -0.02184239],\n",
       "       [ 0.00126078,  0.01769348,  0.9998439 ,  0.06695917],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.        ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(right_pose, inv(left_pose))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3194c48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]]\n",
      "(4, 5, 3)\n",
      "[[[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros((3, 4, 5))\n",
    "print(x)\n",
    "print(np.moveaxis(x, 0, -1).shape)\n",
    "print(np.moveaxis(x, -1, 0))\n",
    "# print(np.moveaxis(x, 0, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebd890f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n",
      "(3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "print(n_img.shape)\n",
    "print(np.moveaxis(n_img, -1, 0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85f0ba0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 95,  92,  87],\n",
       "        [ 95,  94,  86],\n",
       "        [ 94,  91,  85],\n",
       "        ...,\n",
       "        [179, 170, 171],\n",
       "        [178, 167, 171],\n",
       "        [178, 167, 171]],\n",
       "\n",
       "       [[ 95,  92,  87],\n",
       "        [ 95,  91,  85],\n",
       "        [ 94,  88,  84],\n",
       "        ...,\n",
       "        [179, 167, 172],\n",
       "        [178, 166, 171],\n",
       "        [178, 169, 171]],\n",
       "\n",
       "       [[ 97,  90,  87],\n",
       "        [ 97,  90,  85],\n",
       "        [ 97,  90,  83],\n",
       "        ...,\n",
       "        [177, 165, 174],\n",
       "        [177, 168, 171],\n",
       "        [177, 174, 171]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[128, 116, 116],\n",
       "        [128, 109, 109],\n",
       "        [121, 115, 101],\n",
       "        ...,\n",
       "        [ 48,  44,  59],\n",
       "        [ 49,  44,  59],\n",
       "        [ 51,  45,  59]],\n",
       "\n",
       "       [[120, 101, 124],\n",
       "        [120,  94, 112],\n",
       "        [119,  96, 100],\n",
       "        ...,\n",
       "        [ 47,  44,  57],\n",
       "        [ 48,  44,  57],\n",
       "        [ 50,  45,  57]],\n",
       "\n",
       "       [[112,  94, 124],\n",
       "        [112,  92, 112],\n",
       "        [117,  89, 100],\n",
       "        ...,\n",
       "        [ 47,  44,  57],\n",
       "        [ 48,  46,  57],\n",
       "        [ 49,  45,  57]]], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb4686f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2]\n",
      "[[1 2]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2])\n",
    "y = np.expand_dims(x, axis=0)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61b89cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def down_conv_layer(input_channels, output_channels, kernel_size):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(\n",
    "            input_channels,\n",
    "            output_channels,\n",
    "            kernel_size,\n",
    "            padding=(kernel_size - 1) // 2,\n",
    "            stride=1,\n",
    "            bias=False),\n",
    "   nn.BatchNorm2d(output_channels),\n",
    "   nn.ReLU(),\n",
    "        nn.Conv2d(\n",
    "            output_channels,\n",
    "            output_channels,\n",
    "            kernel_size,\n",
    "            padding=(kernel_size - 1) // 2,\n",
    "            stride=2,\n",
    "            bias=False),\n",
    "   nn.BatchNorm2d(output_channels),\n",
    "   nn.ReLU())\n",
    "\n",
    "def conv_layer(input_channels, output_channels, kernel_size):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(\n",
    "            input_channels,\n",
    "            output_channels,\n",
    "            kernel_size,\n",
    "            padding=(kernel_size - 1) // 2,\n",
    "            bias=False),\n",
    "  nn.BatchNorm2d(output_channels),\n",
    "        nn.ReLU())\n",
    "\n",
    "def depth_layer(input_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(input_channels, 1, 3, padding=1), nn.Sigmoid())\n",
    "\n",
    "def refine_layer(input_channels):\n",
    "    return nn.Conv2d(input_channels, 1, 3, padding=1)\n",
    "\n",
    "def up_conv_layer(input_channels, output_channels, kernel_size):\n",
    "    return nn.Sequential(\n",
    "        nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "        nn.Conv2d(\n",
    "            input_channels,\n",
    "            output_channels,\n",
    "            kernel_size,\n",
    "            padding=(kernel_size - 1) // 2,\n",
    "            bias=False),\n",
    "  nn.BatchNorm2d(output_channels),\n",
    "        nn.ReLU())\n",
    "\n",
    "def get_trainable_number(variable):\n",
    "    num = 1\n",
    "    shape = list(variable.shape)\n",
    "    for i in shape:\n",
    "        num *= i\n",
    "    return num\n",
    "\n",
    "class enCoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(enCoder, self).__init__()\n",
    "\n",
    "        self.conv1 = down_conv_layer(67, 128, 7)\n",
    "        self.conv2 = down_conv_layer(128, 256, 5)\n",
    "        self.conv3 = down_conv_layer(256, 512, 3)\n",
    "        self.conv4 = down_conv_layer(512, 512, 3)\n",
    "        self.conv5 = down_conv_layer(512, 512, 3)\n",
    "\n",
    "\n",
    "    def getVolume(self, left_image, right_image, KRKiUV_T, KT_T):\n",
    "\n",
    "        idepth_base = 1.0 / 50.0\n",
    "        idepth_step = (1.0 / 0.5 - 1.0 / 50.0) / 63.0\n",
    "\n",
    "        costvolume = Variable(\n",
    "            torch.FloatTensor(left_image.shape[0], 64,\n",
    "                                   left_image.shape[2], left_image.shape[3]))\n",
    "\n",
    "        image_height = 256\n",
    "        image_width = 320\n",
    "        batch_number = left_image.shape[0]\n",
    "\n",
    "        normalize_base = torch.FloatTensor(\n",
    "            [image_width / 2.0, image_height / 2.0])\n",
    "\n",
    "        normalize_base = normalize_base.unsqueeze(0).unsqueeze(-1)\n",
    "\n",
    "        for depth_i in range(64):\n",
    "            this_depth = 1.0 / (idepth_base + depth_i * idepth_step)\n",
    "            transformed = KRKiUV_T * this_depth + KT_T\n",
    "            demon = transformed[:, 2, :].unsqueeze(1)  \n",
    "            warp_uv = transformed[:, 0: 2, :] / (demon + 1e-6)\n",
    "            warp_uv = (warp_uv - normalize_base) / normalize_base\n",
    "            warp_uv = warp_uv.view(\n",
    "                batch_number, 2, image_width,\n",
    "                image_height) \n",
    "\n",
    "            warp_uv = Variable(warp_uv.permute(\n",
    "                0, 3, 2, 1))  \n",
    "            warped = F.grid_sample(right_image, warp_uv)\n",
    "\n",
    "            costvolume[:, depth_i, :, :] = torch.sum(\n",
    "                torch.abs(warped - left_image), dim=1)\n",
    "        return costvolume\n",
    "\n",
    "    def forward(self, left_image, right_image, KRKiUV_T, KT_T):\n",
    "        plane_sweep_volume = self.getVolume(left_image, right_image, KRKiUV_T, KT_T)\n",
    "\n",
    "        x = torch.cat((left_image, plane_sweep_volume), 1)\n",
    "\n",
    "        conv1 = self.conv1(x)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv3 = self.conv3(conv2)\n",
    "        conv4 = self.conv4(conv3)\n",
    "\n",
    "        conv5 = self.conv5(conv4)\n",
    "\n",
    "\n",
    "        return [conv5, conv4, conv3, conv2, conv1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63b27859",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encoder_forward(r_img,n_img, r_pose,n_pose, K):\n",
    "    \n",
    "    # Left and right image\n",
    "    left_image = r_img\n",
    "    right_image = n_img\n",
    "\n",
    "    # Left and right poses\n",
    "    left_pose = r_pose\n",
    "    right_pose = n_pose\n",
    "    # Camera intrinsic matrix\n",
    "    camera_k = K\n",
    "\n",
    "    # Finds how one vector component on the other. Mostly for similarity\n",
    "    left2right = np.dot(right_pose, inv(left_pose))\n",
    "\n",
    "    # scale to 320x256\n",
    "    # Find original width and height\n",
    "    original_width = left_image.shape[1]\n",
    "    original_height = left_image.shape[0]\n",
    "    # Find the scaling factor to the standardized image size\n",
    "    factor_x = 320.0 / original_width\n",
    "    factor_y = 256.0 / original_height\n",
    "    \n",
    "    # Resize the image to (320, 256)\n",
    "    left_image = cv2.resize(left_image, (320, 256))\n",
    "    right_image = cv2.resize(right_image, (320, 256))\n",
    "    \n",
    "    # left and right image with dimension (320, 256)\n",
    "    \n",
    "    # Take the first row and multiply the first row with factor_x, scaling value\n",
    "    camera_k[0, :] *= factor_x\n",
    "    # Take the second row and multiply the second row with factor_y, scaling value\n",
    "    camera_k[1, :] *= factor_y\n",
    "\n",
    "    # convert to torch\n",
    "    # Change the axis\n",
    "    torch_left_image = np.moveaxis(left_image, -1, 0)\n",
    "    # torch_left_image dimension (3, 256, 320)\n",
    "    \n",
    "    #Expand the dimension\n",
    "    torch_left_image = np.expand_dims(torch_left_image, 0)\n",
    "    # torch_left_image (1, 3, 256, 320)\n",
    "    \n",
    "    torch_left_image = (torch_left_image - 81.0)/ 35.0\n",
    "    \n",
    "    torch_right_image = np.moveaxis(right_image, -1, 0)\n",
    "    # torch_right_image dimension (3, 256, 320)\n",
    "    torch_right_image = np.expand_dims(torch_right_image, 0)\n",
    "    # torch_right_image dimension (1, 3, 256, 320)\n",
    "    torch_right_image = (torch_right_image - 81.0)/ 35.0\n",
    "\n",
    "    # Transform the image to tensor \n",
    "    left_image_cuda = Tensor(torch_left_image)\n",
    "    # set the input to a network to volatile if you are doing inference only \n",
    "    # and won't be running backpropagation in order to conserve memory.\n",
    "    left_image_cuda = Variable(left_image_cuda, volatile=True)\n",
    "\n",
    "    right_image_cuda = Tensor(torch_right_image)\n",
    "    right_image_cuda = Variable(right_image_cuda, volatile=True)\n",
    "    \n",
    "    # take 0 to 3rd row with only the 4th column\n",
    "    # Translation matrix\n",
    "    left_in_right_T = left2right[0:3, 3]\n",
    "    # take 3x3 matrix\n",
    "    # Rotation matrix\n",
    "    left_in_right_R = left2right[0:3, 0:3]\n",
    "    # Take camera matrix\n",
    "    K = camera_k\n",
    "    # Take the inverse of camera matrix\n",
    "    K_inverse = inv(K)\n",
    "    # Dot product between 3x3 matrix and inversed camera matrix\n",
    "    KRK_i = K.dot(left_in_right_R.dot(K_inverse))\n",
    "    # Take the dot product between the matrix and pixel coordinate\n",
    "    KRKiUV = KRK_i.dot(pixel_coordinate)\n",
    "    # Dot product between the camera matrix and left in right matrix transalation vector\n",
    "    KT = K.dot(left_in_right_T)\n",
    "    # Expand the dimension\n",
    "    KT = np.expand_dims(KT, -1)\n",
    "    KT = np.expand_dims(KT, 0)\n",
    "    # Transform it to floating value\n",
    "    KT = KT.astype(np.float32)\n",
    "    KRKiUV = KRKiUV.astype(np.float32)\n",
    "    KRKiUV = np.expand_dims(KRKiUV, 0)\n",
    "    # Transform the values to cuda tensor type\n",
    "    KRKiUV_cuda_T = Tensor(KRKiUV)\n",
    "    KT_cuda_T = Tensor(KT)\n",
    "    \n",
    "    pretrained_encoder = \"/home/latai/Documents/Master_thesis/Tutorials/encoder_model_best.pth.tar\"\n",
    "    encoder = enCoder()\n",
    "    encoder = torch.nn.DataParallel(encoder)\n",
    "    weights = torch.load(pretrained_encoder, map_location=torch.device('cpu'))\n",
    "    encoder.load_state_dict(weights['state_dict'])\n",
    "    encoder.eval()\n",
    "\n",
    "    conv5, conv4, conv3, conv2, conv1= encoder(left_image_cuda, right_image_cuda, KRKiUV_cuda_T,KT_cuda_T)\n",
    "\n",
    "#     return conv5, conv4, conv3, conv2, conv1\n",
    "    return conv5, conv4, conv3, conv2, conv1, left_image_cuda, right_image_cuda, KRKiUV_cuda_T,KT_cuda_T\n",
    "#     return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae2ab42d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/latai/anaconda3/envs/latai/lib/python3.7/site-packages/ipykernel_launcher.py:56: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/latai/anaconda3/envs/latai/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/latai/anaconda3/envs/latai/lib/python3.7/site-packages/torch/nn/functional.py:4194: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  \"Default grid_sample and affine_grid behavior has changed \"\n"
     ]
    }
   ],
   "source": [
    "conv5, conv4, conv3, conv2, conv1, left_image_cuda, right_image_cuda, KRKiUV_cuda_T,KT_cuda_T = encoder_forward(r_img,n_img, r_pose,n_pose, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9a1565",
   "metadata": {},
   "source": [
    "## Cost volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "510016c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480\n",
      "640\n",
      "3\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-231c5b54af8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "left_image = r_img\n",
    "print(left_image.shape[0])\n",
    "print(left_image.shape[1])\n",
    "print(left_image.shape[2])\n",
    "print(left_image.shape[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7aac26e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVolume(left_image, right_image, KRKiUV_T, KT_T):\n",
    "    \n",
    "    # ideal base depth\n",
    "    idepth_base = 1.0 / 50.0\n",
    "    # Move from 0.5m to maximum of 50m hypothesized depth such that the hypothesized planes are 64\n",
    "    idepth_step = (1.0 / 0.5 - 1.0 / 50.0) / 63.0\n",
    "    \n",
    "    # Didn't understand this part\n",
    "    costvolume = Variable(\n",
    "        torch.FloatTensor(left_image.shape[0], 64,\n",
    "                               left_image.shape[2], left_image.shape[3]))\n",
    "\n",
    "    image_height = 256\n",
    "    image_width = 320\n",
    "    batch_number = left_image.shape[0]\n",
    "    \n",
    "    # Normalize the image with certain width and height\n",
    "    normalize_base = torch.FloatTensor(\n",
    "        [image_width / 2.0, image_height / 2.0])\n",
    "    \n",
    "    # Normalize the image\n",
    "    normalize_base = normalize_base.unsqueeze(0).unsqueeze(-1)\n",
    "\n",
    "    for depth_i in range(64):\n",
    "        this_depth = 1.0 / (idepth_base + depth_i * idepth_step)\n",
    "        \n",
    "        print(KRKiUV_T)\n",
    "        transformed = KRKiUV_T * this_depth + KT_T\n",
    "        \n",
    "        demon = transformed[:, 2, :].unsqueeze(1)\n",
    "        \n",
    "        warp_uv = transformed[:, 0: 2, :] / (demon + 1e-6)\n",
    "        \n",
    "        warp_uv = (warp_uv - normalize_base) / normalize_base\n",
    "        \n",
    "        warp_uv = warp_uv.view(\n",
    "            batch_number, 2, image_width,\n",
    "            image_height) \n",
    "        \n",
    "        warp_uv = Variable(warp_uv.permute(\n",
    "            0, 3, 2, 1))\n",
    "        \n",
    "        warped = F.grid_sample(right_image, warp_uv)\n",
    "            \n",
    "        costvolume[:, depth_i, :, :] = torch.sum(\n",
    "            torch.abs(warped - left_image), dim=1)\n",
    "    print(\"left\")   \n",
    "    print(\"This depth:\", this_depth)\n",
    "    print(\"KRKiUV_T:\", KRKiUV_T.shape)\n",
    "    print(\"KT_T:\", KT_T.shape)\n",
    "    print(\"transformed:\",transformed.shape)\n",
    "    print(\"demon\", demon.shape)\n",
    "    print(\"warp_uv:\", warp_uv.shape)\n",
    "    print(\"warped\", warped.shape)\n",
    "    print(\"costvolume\", costvolume.shape)\n",
    "    \n",
    "    return costvolume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "110e9ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "tensor([[[ -4.5131,  -4.4824,  -4.4517,  ..., 322.3940, 322.4247, 322.4554],\n",
      "         [ -2.5484,  -1.5415,  -0.5347,  ..., 244.4919, 245.4988, 246.5056],\n",
      "         [  0.9919,   0.9920,   0.9920,  ...,   1.0076,   1.0077,   1.0077]]])\n",
      "left\n",
      "This depth: 0.5\n",
      "KRKiUV_T: torch.Size([1, 3, 81920])\n",
      "KT_T: torch.Size([1, 3, 1])\n",
      "transformed: torch.Size([1, 3, 81920])\n",
      "demon torch.Size([1, 1, 81920])\n",
      "warp_uv: torch.Size([1, 256, 320, 2])\n",
      "warped torch.Size([1, 3, 256, 320])\n",
      "costvolume torch.Size([1, 64, 256, 320])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.4286, 1.4571, 1.2286,  ..., 6.4857, 6.5429, 6.3714],\n",
       "          [1.1429, 1.1714, 1.1143,  ..., 6.6286, 6.7714, 6.7429],\n",
       "          [1.2286, 1.1429, 1.1143,  ..., 6.8286, 6.8857, 6.9714],\n",
       "          ...,\n",
       "          [0.2682, 0.2408, 1.1851,  ..., 0.4909, 0.6922, 1.1583],\n",
       "          [0.5825, 1.1345, 1.8884,  ..., 2.1074, 1.1669, 0.5891],\n",
       "          [1.6801, 1.9008, 1.3072,  ..., 7.3109, 5.2035, 3.1958]],\n",
       "\n",
       "         [[1.4286, 1.4571, 1.2286,  ..., 6.4857, 6.5429, 6.3714],\n",
       "          [1.1429, 1.1714, 1.1143,  ..., 6.6286, 6.7714, 6.7429],\n",
       "          [1.2286, 1.1429, 1.1143,  ..., 6.8286, 6.8857, 6.9714],\n",
       "          ...,\n",
       "          [0.5596, 0.1792, 1.4232,  ..., 0.6074, 0.6525, 1.3457],\n",
       "          [0.6276, 1.3507, 2.3749,  ..., 2.0556, 1.0582, 0.3920],\n",
       "          [1.7353, 2.4194, 1.3259,  ..., 7.1970, 5.1202, 3.2227]],\n",
       "\n",
       "         [[1.4286, 1.4571, 1.2286,  ..., 6.4857, 6.5429, 6.3714],\n",
       "          [1.1429, 1.1714, 1.1143,  ..., 6.6286, 6.7714, 6.7429],\n",
       "          [1.2286, 1.1429, 1.1143,  ..., 6.8286, 6.8857, 6.9714],\n",
       "          ...,\n",
       "          [1.2329, 0.1811, 1.5049,  ..., 0.4289, 0.7773, 1.2733],\n",
       "          [0.6245, 1.4833, 2.6489,  ..., 2.0011, 1.0773, 0.4756],\n",
       "          [1.7873, 2.6872, 1.7784,  ..., 7.2795, 5.0619, 3.1440]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.4286, 1.4571, 1.2286,  ..., 6.4857, 6.5429, 6.3714],\n",
       "          [1.1429, 1.1714, 1.1143,  ..., 6.6286, 6.7714, 6.7429],\n",
       "          [1.2286, 1.1429, 1.1143,  ..., 6.8286, 6.8857, 6.9714],\n",
       "          ...,\n",
       "          [4.5143, 4.1714, 2.9143,  ..., 2.2954, 1.9132, 1.7060],\n",
       "          [3.7714, 2.7714, 1.6000,  ..., 4.0530, 3.1964, 2.4823],\n",
       "          [2.5143, 1.3143, 2.0571,  ..., 8.9365, 6.8436, 5.0017]],\n",
       "\n",
       "         [[1.4286, 1.4571, 1.2286,  ..., 6.4857, 6.5429, 6.3714],\n",
       "          [1.1429, 1.1714, 1.1143,  ..., 6.6286, 6.7714, 6.7429],\n",
       "          [1.2286, 1.1429, 1.1143,  ..., 6.8286, 6.8857, 6.9714],\n",
       "          ...,\n",
       "          [4.5143, 4.1714, 2.9143,  ..., 2.2643, 1.8789, 1.6434],\n",
       "          [3.7714, 2.7714, 1.6000,  ..., 4.1362, 3.2282, 2.5050],\n",
       "          [2.5143, 1.3143, 2.0571,  ..., 8.9770, 6.8841, 5.0452]],\n",
       "\n",
       "         [[1.4286, 1.4571, 1.2286,  ..., 6.4857, 6.5429, 6.3714],\n",
       "          [1.1429, 1.1714, 1.1143,  ..., 6.6286, 6.7714, 6.7429],\n",
       "          [1.2286, 1.1429, 1.1143,  ..., 6.8286, 6.8857, 6.9714],\n",
       "          ...,\n",
       "          [4.5143, 4.1714, 2.9143,  ..., 2.2054, 1.8728, 1.6236],\n",
       "          [3.7714, 2.7714, 1.6000,  ..., 4.0459, 3.2219, 2.4703],\n",
       "          [2.5143, 1.3143, 2.0571,  ..., 8.9950, 6.9430, 5.0621]]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getVolume(left_image_cuda, right_image_cuda, KRKiUV_cuda_T,KT_cuda_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df72212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ideal base depth\n",
    "idepth_base = 1.0 / 50.0\n",
    "# Move from 0.5m to maximum of 50m hypothesized depth such that the hypothesized planes are 64\n",
    "idepth_step = (1.0 / 0.5 - 1.0 / 50.0) / 63.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63999ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02\n",
      "0.03142857142857143\n",
      "2.0114285714285716\n"
     ]
    }
   ],
   "source": [
    "print(idepth_base)\n",
    "print(idepth_step)\n",
    "\n",
    "print(idepth_step*64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18360b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genDistM(poses):\n",
    "    n = len(poses)\n",
    "    D = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            D[i, j] = pose_distance(poses[i], poses[j])\n",
    "    return D\n",
    "\n",
    "def pose_distance(p1, p2):\n",
    "    rel_pose = np.dot(p1, inv(p2))\n",
    "    R = rel_pose[:3, :3]\n",
    "    t = rel_pose[:3, 3]\n",
    "\n",
    "    return round(np.sqrt(np.linalg.norm(t) ** 2 + 2 * (1 - min(3.0, np.matrix.trace(R)) / 3)), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93de472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = conv5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2fe36c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-63b32849ecf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenDistM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_poses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Output of the encoder at conv5 layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor"
     ]
    }
   ],
   "source": [
    "D = torch.from_numpy(np.expand_dims(genDistM(gt_poses), 0))\n",
    "# Output of the encoder at conv5 layer\n",
    "Y = torch.stack(latents, dim=1).cpu()\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38e5cf91",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-385c9e5aacd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mscene_list_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'train_wo_gtav.txt'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'val.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscenes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene_list_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscenes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "self.root = Path(root)\n",
    "scene_list_path = self.root / 'train_wo_gtav.txt' if train else self.root / 'val.txt'\n",
    "self.scenes = [self.root / folder[:-1] for folder in open(scene_list_path)]\n",
    "self.scenes\n",
    "\n",
    "for folder in folders_list:\n",
    "    imgs = sorted(folder.files('*.png'))\n",
    " \n",
    "n = len(imgs)\n",
    "\n",
    "with torch.no_grad():\n",
    "    poses = []\n",
    "    idepths = []\n",
    "    idepths_after = []\n",
    "    latents = []\n",
    "    conv1s = []\n",
    "    conv2s = []\n",
    "    conv3s = []\n",
    "    conv4s = []\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    depth_gts = []\n",
    "    for i in tqdm(range(1, n)):  # start with the 2nd frame\n",
    "\n",
    "        r_pose = gt_poses[i]\n",
    "        n_pose = gt_poses[i - 1]\n",
    "\n",
    "        r_img = cv2.imread(imgs[i])\n",
    "        n_img = cv2.imread(imgs[i - 1])\n",
    "\n",
    "        camera_k = np.loadtxt(scene / 'K.txt').astype(np.float32).reshape((3, 3))\n",
    "\n",
    "        conv5, conv4, conv3, conv2, conv1 = encoder_forward(r_img, n_img, r_pose, n_pose, camera_k)\n",
    "\n",
    "        poses.append(r_pose)\n",
    "\n",
    "        latents.append(conv5)\n",
    "        conv4s.append(conv4)\n",
    "        conv3s.append(conv3)\n",
    "        conv2s.append(conv2)\n",
    "        conv1s.append(conv1)\n",
    "\n",
    "        gt_depth = np.load(gts[i])\n",
    "        depth_gts.append(gt_depth)\n",
    "\n",
    "    D = torch.from_numpy(np.expand_dims(genDistM(poses), 0))\n",
    "    Y = torch.stack(latents, dim=1).cpu()\n",
    "\n",
    "#     Z = gplayer(D, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5052338",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_encoder = \"/home/latai/Documents/Master_thesis/Tutorials/encoder_model_best.pth.tar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81836234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): enCoder(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(67, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(128, 128, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(256, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (conv4): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (conv5): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from enCoder import enCoder\n",
    "pretrained_encoder = \"/home/latai/Documents/Master_thesis/Tutorials/encoder_model_best.pth.tar\"\n",
    "encoder = enCoder()\n",
    "encoder = torch.nn.DataParallel(encoder)\n",
    "weights = torch.load(pretrained_encoder, map_location=torch.device('cpu'))\n",
    "encoder.load_state_dict(weights['state_dict'])\n",
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee4d3e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
