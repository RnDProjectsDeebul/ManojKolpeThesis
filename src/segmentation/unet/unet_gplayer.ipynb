{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "unet_gplayer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.activation import LeakyReLU\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import pickle\n",
        "import math\n",
        "from itertools import chain\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = torch.device(device)\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# train_dir_color = '/scratch/mkolpe2s/MT/Main_data_folder/Segmentation_dataset/scannet/experiment_data/data/color_train/'\n",
        "# train_dir_label = '/scratch/mkolpe2s/MT/Main_data_folder/Segmentation_dataset/scannet/experiment_data/data/label_train_n/'\n",
        "# val_dir_color = '/scratch/mkolpe2s/MT/Main_data_folder/Segmentation_dataset/scannet/experiment_data/data/color_valid/'\n",
        "# val_dir_label = '/scratch/mkolpe2s/MT/Main_data_folder/Segmentation_dataset/scannet/experiment_data/data/label_valid_n/'\n",
        "\n",
        "train_dir_color = '/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/color_train/'\n",
        "# train_dir_label = '/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/label_train/'\n",
        "train_dir_label = '/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/label_train_n'\n",
        "val_dir_color = '/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/color_valid/'\n",
        "# val_dir_label = '/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/label_valid/'\n",
        "val_dir_label = '/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/label_valid_n'\n",
        "pos_dir = '/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/pose'\n",
        "\n",
        "train_fns_color = os.listdir(train_dir_color)\n",
        "train_fns_label = os.listdir(train_dir_label)\n",
        "val_fns_color = os.listdir(val_dir_color)\n",
        "val_fns_label = os.listdir(val_dir_label)\n",
        "pose_fns = os.listdir(pos_dir)\n",
        "\n",
        "print(\"train_fns_color:\", len(train_fns_color), \"train_fns_label:\",len(train_fns_label),\"val_fns_color:\", len(val_fns_color),\"val_fns_label:\", len(val_fns_label))\n",
        "\n",
        "class scannet(Dataset):\n",
        "    \n",
        "    def __init__(self, image_dir, label_dir, pose_dir):\n",
        "        self.image_dir = image_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.pose_dir = pose_dir\n",
        "        self.image_fns = os.listdir(image_dir)\n",
        "        self.label_fns = os.listdir(label_dir)\n",
        "        self.pose_fns = os.listdir(pose_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_fns)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        image_fn = self.image_fns[index]\n",
        "        # image_fn2 = self.image_fns[index+1]\n",
        "        label_fn = self.label_fns[index]\n",
        "        # pose_fn = self.pose_fns[index]\n",
        "        image_fp = os.path.join(self.image_dir, image_fn)\n",
        "        image = Image.open(image_fp).convert('RGB')\n",
        "        image = np.array(image)\n",
        "        label_fp = os.path.join(self.label_dir, image_fn.split('.')[0]+'.png')\n",
        "        label = Image.open(label_fp)\n",
        "        label = np.array(label)\n",
        "        cityscape, label = image, label\n",
        "        cityscape = self.transform(cityscape)\n",
        "        label_class = torch.Tensor(label).long()\n",
        "\n",
        "        pose_fp1 =  os.path.join(self.pose_dir, image_fn.split('.')[0]+'.txt')\n",
        "        # pose_fp2 =  os.path.join(self.pose_dir, image_fn2.split('.')[0]+'.txt')\n",
        "        # D = self.pose_process(pose_fp1,pose_fp2)\n",
        "        D = self.test()\n",
        "        return cityscape, label_class, D\n",
        "    \n",
        "    def test(self):\n",
        "\n",
        "        gt_poses = []\n",
        "        with open('/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/pose.txt') as f:\n",
        "            for l in f.readlines():\n",
        "                l = l.strip('\\n')\n",
        "                gt_poses.append(np.array(l.split(' ')).astype(np.float32).reshape(4, 4))\n",
        "        poses = [gt_poses[0], gt_poses[1]]\n",
        "        D = torch.from_numpy(np.expand_dims(genDistM(poses), 0))\n",
        "        return D\n",
        "\n",
        "    def pose_process(self, file1, file2):\n",
        "\n",
        "\n",
        "        poses = [gt_poses[0], gt_poses[1]]\n",
        "        D = torch.from_numpy(np.expand_dims(genDistM(poses), 0))\n",
        "    \n",
        "    def pose_distance(p1, p2):\n",
        "        rel_pose = np.dot(np.linalg.inv(p1), p2)\n",
        "        R = rel_pose[:3, :3]\n",
        "        t = rel_pose[:3, 3]\n",
        "\n",
        "        return round(np.sqrt(np.linalg.norm(t) ** 2 + 2 * (1 - min(3.0, np.matrix.trace(R)) / 3)), 4)\n",
        "    \n",
        "    def genDistM(poses):\n",
        "        n = len(poses)\n",
        "        D = np.zeros((n, n))\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                D[i, j] = pose_distance(poses[i], poses[j])\n",
        "        return D\n",
        "\n",
        "    def split_image(self, image):\n",
        "        image = np.array(image)\n",
        "        cityscape, label = image[:, :256, :], image[:, 256:, :]\n",
        "        return cityscape, label\n",
        "    \n",
        "    def transform(self, image):\n",
        "        transform_ops = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "        ])\n",
        "        return transform_ops(image)\n",
        "\n",
        "dataset = scannet(train_dir_color, train_dir_label, pos_dir)\n",
        "print(\"Length of dataset:\", len(dataset))\n",
        "\n",
        "gt_poses = []\n",
        "with open('/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/pose.txt') as f:\n",
        "    for l in f.readlines():\n",
        "        l = l.strip('\\n')\n",
        "        gt_poses.append(np.array(l.split(' ')).astype(np.float32).reshape(4, 4))\n",
        "\n",
        "def pose_distance(p1, p2):\n",
        "    rel_pose = np.dot(np.linalg.inv(p1), p2)\n",
        "    R = rel_pose[:3, :3]\n",
        "    t = rel_pose[:3, 3]\n",
        "\n",
        "    return round(np.sqrt(np.linalg.norm(t) ** 2 + 2 * (1 - min(3.0, np.matrix.trace(R)) / 3)), 4)\n",
        "    \n",
        "def genDistM(poses):\n",
        "    n = len(poses)\n",
        "    D = np.zeros((n, n))\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            D[i, j] = pose_distance(poses[i], poses[j])\n",
        "    return D\n",
        "\n",
        "poses = [gt_poses[0], gt_poses[1]]\n",
        "D = torch.from_numpy(np.expand_dims(genDistM(poses), 0))\n",
        "\n",
        "tensor_list = []\n",
        "\n",
        "class encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes):\n",
        "        super(encoder, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.contracting_11 = self.conv_block(in_channels=3, out_channels=64)\n",
        "        self.contracting_12 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.contracting_21 = self.conv_block(in_channels=64, out_channels=128)\n",
        "        self.contracting_22 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.contracting_31 = self.conv_block(in_channels=128, out_channels=256)\n",
        "        self.contracting_32 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.contracting_41 = self.conv_block(in_channels=256, out_channels=512)\n",
        "        self.contracting_42 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.middle = self.conv_block(in_channels=512, out_channels=1024)\n",
        "      \n",
        "    def conv_block(self, in_channels, out_channels):\n",
        "        block = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.BatchNorm2d(num_features=out_channels),\n",
        "                                    nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.BatchNorm2d(num_features=out_channels))\n",
        "        return block\n",
        "    \n",
        "    def forward(self, X):\n",
        "        contracting_11_out = self.contracting_11(X) # [-1, 64, 256, 256]\n",
        "        contracting_12_out = self.contracting_12(contracting_11_out) # [-1, 64, 128, 128]\n",
        "        contracting_21_out = self.contracting_21(contracting_12_out) # [-1, 128, 128, 128]\n",
        "        contracting_22_out = self.contracting_22(contracting_21_out) # [-1, 128, 64, 64]\n",
        "        contracting_31_out = self.contracting_31(contracting_22_out) # [-1, 256, 64, 64]\n",
        "        contracting_32_out = self.contracting_32(contracting_31_out) # [-1, 256, 32, 32]\n",
        "        contracting_41_out = self.contracting_41(contracting_32_out) # [-1, 512, 32, 32]\n",
        "        contracting_42_out = self.contracting_42(contracting_41_out) # [-1, 512, 16, 16]\n",
        "        middle_out = self.middle(contracting_42_out) # [-1, 1024, 16, 16]\n",
        "        return middle_out, contracting_41_out, contracting_31_out, contracting_21_out, contracting_11_out\n",
        "\n",
        "class GPlayer(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self):\n",
        "        super(GPlayer, self).__init__()\n",
        "\n",
        "        self.gamma2 = nn.Parameter(torch.randn(1), requires_grad=True).float()\n",
        "        self.ell = nn.Parameter(torch.randn(1), requires_grad=True).float()\n",
        "        self.sigma2 = nn.Parameter(torch.randn(1), requires_grad=True).float()\n",
        "\n",
        "\n",
        "    def forward(self, D, Y):\n",
        "        '''\n",
        "        :param D: Distance matrix\n",
        "        :param Y: Stacked outputs from encoder\n",
        "        :return: Z: transformed latent space\n",
        "        '''\n",
        "        b=1\n",
        "        l,c,h,w = Y.size()\n",
        "        # print(\"----------------\")\n",
        "        # print(\"Y\", Y.shape)\n",
        "        # print(\"b\", b)\n",
        "        # print(\"l\", l)\n",
        "        # print(\"c\", c)\n",
        "        # print(\"h\", h)\n",
        "        # print(\"w\", w)\n",
        "\n",
        "        Y = Y.view(l,-1).cpu().float()\n",
        "        # print(\"Y.shape\",Y.shape)\n",
        "        D = D.float()\n",
        "        K = torch.exp(self.gamma2) * (1 + math.sqrt(3) * D / torch.exp(self.ell)) * torch.exp(-math.sqrt(3) * D / torch.exp(self.ell))\n",
        "        I = torch.eye(l).expand(l, l).float()\n",
        "        X,_ = torch.solve(Y, K+torch.exp(self.sigma2)*I)\n",
        "        Z = K.bmm(X)\n",
        "        Z = F.relu(Z)\n",
        "\n",
        "        # l = 1\n",
        "        # a, b, c, d = Y.size()\n",
        "        # Y = Y.view(1,-1).cpu().float()\n",
        "        # D = D.float()\n",
        "        # K = torch.exp(self.gamma2) * (1 + math.sqrt(3) * D / torch.exp(self.ell)) * torch.exp(-math.sqrt(3) * D / torch.exp(self.ell))\n",
        "        # I = torch.eye(l).expand(l, l).float()\n",
        "        # X,_ = torch.solve(Y, K+torch.exp(self.sigma2)*I)\n",
        "        # Z = K.bmm(X)\n",
        "        # Z = F.relu(Z)\n",
        "        # middle_out = Z[:, 0].view(a, 1024, 15, 20).to(device)\n",
        "\n",
        "        # b,l,c,h,w = Y.size()\n",
        "        # Y = Y.view(b,l,-1).cpu().float()\n",
        "        # D = D.float()\n",
        "\n",
        "        # K = torch.exp(self.gamma2) * (1 + math.sqrt(3) * D / torch.exp(self.ell)) * torch.exp(-math.sqrt(3) * D / torch.exp(self.ell))\n",
        "        # I = torch.eye(l).expand(b, l, l).float()\n",
        "\n",
        "        # X,_ = torch.solve(Y, K+torch.exp(self.sigma2)*I)\n",
        "\n",
        "        # Z = K.bmm(X)\n",
        "\n",
        "        # Z = F.relu(Z)\n",
        "\n",
        "        return middle_out\n",
        "\n",
        "class decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes):\n",
        "        super(decoder, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.expansive_11 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.expansive_12 = self.conv_block(in_channels=1024, out_channels=512)\n",
        "        self.expansive_21 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.expansive_22 = self.conv_block(in_channels=512, out_channels=256)\n",
        "        self.expansive_31 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.expansive_32 = self.conv_block(in_channels=256, out_channels=128)\n",
        "        self.expansive_41 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.expansive_42 = self.conv_block(in_channels=128, out_channels=64)\n",
        "        self.output = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=3, stride=1, padding=1)\n",
        "        \n",
        "    def conv_block(self, in_channels, out_channels):\n",
        "        block = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.BatchNorm2d(num_features=out_channels),\n",
        "                                    nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.BatchNorm2d(num_features=out_channels))\n",
        "        return block\n",
        "    \n",
        "    def forward(self, middle_out, contracting_41_out, contracting_31_out, contracting_21_out, contracting_11_out):\n",
        "        expansive_11_out = self.expansive_11(middle_out) # [-1, 512, 32, 32]\n",
        "        expansive_12_out = self.expansive_12(torch.cat((expansive_11_out, contracting_41_out), dim=1)) # [-1, 1024, 32, 32] -> [-1, 512, 32, 32]\n",
        "        expansive_21_out = self.expansive_21(expansive_12_out) # [-1, 256, 64, 64]\n",
        "        expansive_22_out = self.expansive_22(torch.cat((expansive_21_out, contracting_31_out), dim=1)) # [-1, 512, 64, 64] -> [-1, 256, 64, 64]\n",
        "        expansive_31_out = self.expansive_31(expansive_22_out) # [-1, 128, 128, 128]\n",
        "        expansive_32_out = self.expansive_32(torch.cat((expansive_31_out, contracting_21_out), dim=1)) # [-1, 256, 128, 128] -> [-1, 128, 128, 128]\n",
        "        expansive_41_out = self.expansive_41(expansive_32_out) # [-1, 64, 256, 256]\n",
        "        expansive_42_out = self.expansive_42(torch.cat((expansive_41_out, contracting_11_out), dim=1)) # [-1, 128, 256, 256] -> [-1, 64, 256, 256]\n",
        "        output_out = self.output(expansive_42_out) # [-1, num_classes, 256, 256]\n",
        "        return output_out\n",
        "\n",
        "# model = UNet(num_classes=num_classes)\n",
        "\n",
        "num_classes = 41\n",
        "batch_size = 16\n",
        "\n",
        "epochs = 1\n",
        "lr = 0.001\n",
        "\n",
        "dataset = scannet(train_dir_color, train_dir_label, pos_dir)\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "with open('data_loader.pkl', 'wb') as fp:\n",
        "    pickle.dump(data_loader, fp)\n",
        "\n",
        "encoder = encoder(num_classes=num_classes).to(device)\n",
        "\n",
        "gplayer = GPlayer().to(device)\n",
        "\n",
        "decoder = decoder(num_classes=num_classes).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "encoder = torch.nn.DataParallel(encoder)\n",
        "decoder = torch.nn.DataParallel(decoder)\n",
        "\n",
        "parameters = chain(encoder.parameters(), gplayer.parameters(), decoder.parameters())\n",
        "optimizer = torch.optim.Adam(parameters, lr=lr)\n",
        "\n",
        "step_losses = []\n",
        "epoch_losses = []\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    epoch_loss = 0\n",
        "    for X, Y, D_dum in tqdm(data_loader, total=len(data_loader), leave=False):\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "                \n",
        "        middle_out, contracting_41_out, contracting_31_out, contracting_21_out, contracting_11_out = encoder(X)\n",
        "\n",
        "        # Y = torch.stack(middle_out, dim = 1).cpu()\n",
        "        # print(middle_out.size())\n",
        "        # with open('middle_out.pkl', 'wb') as fp:\n",
        "        #       pickle.dump(middle_out, fp)\n",
        "        # with open('D.pkl', 'wb') as fp:\n",
        "        #       pickle.dump(D, fp)\n",
        "        if len(X) == 16:\n",
        "            value = 16\n",
        "            with open('/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/D.pkl', 'rb') as fp:\n",
        "                  D = pickle.load(fp)\n",
        "        if len(X) == 6:\n",
        "            value = 6\n",
        "            with open('/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/D_6.pkl', 'rb') as fp:\n",
        "                  D = pickle.load(fp)\n",
        "        Z = gplayer(D,middle_out)\n",
        "\n",
        "        l,c,h,w = Z.size()\n",
        "\n",
        "        # Y_pred_n = decoder(Z.to(device), contracting_41_out, contracting_31_out, contracting_21_out, contracting_11_out)\n",
        "\n",
        "        # with open('/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/Z.pkl', 'wb') as fp:\n",
        "        #       pickle.dump(Z, fp)\n",
        "        \n",
        "        # with open('/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/contracting_41_out.pkl', 'wb') as fp:\n",
        "        #       pickle.dump(contracting_41_out, fp)\n",
        "        \n",
        "        # with open('/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/contracting_31_out.pkl', 'wb') as fp:\n",
        "        #       pickle.dump(contracting_31_out, fp)\n",
        "        \n",
        "        # with open('/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/contracting_21_out.pkl', 'wb') as fp:\n",
        "        #       pickle.dump(contracting_21_out, fp)\n",
        "        \n",
        "        # with open('/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/contracting_11_out.pkl', 'wb') as fp:\n",
        "        #       pickle.dump(contracting_11_out, fp)\n",
        "        \n",
        "        temp_list = []\n",
        "        # print(Z.shape)\n",
        "        for i in range(value):\n",
        "            Z_new = torch.from_numpy((np.expand_dims(Z[i].detach().numpy(), axis=0)))\n",
        "            contracting_41_out_new = torch.from_numpy((np.expand_dims(contracting_41_out[i].detach().numpy(), axis=0)))\n",
        "            contracting_31_out_new = torch.from_numpy((np.expand_dims(contracting_31_out[i].detach().numpy(), axis=0)))\n",
        "            contracting_21_out_new = torch.from_numpy((np.expand_dims(contracting_21_out[i].detach().numpy(), axis=0)))\n",
        "            contracting_11_out_new = torch.from_numpy((np.expand_dims(contracting_11_out[i].detach().numpy(), axis=0)))\n",
        "            # print(contracting_41_out_new.shape, contracting_31_out_new.shape, contracting_21_out_new.shape, contracting_11_out_new.shape)\n",
        "            Y_pred_n = decoder(Z_new.to(device), contracting_41_out_new, contracting_31_out_new, contracting_21_out_new, contracting_11_out_new)\n",
        "            temp_list.append(np.squeeze(Y_pred_n.detach().numpy()))\n",
        "            # print(Y_pred_n.detach().numpy().shape)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        Y_pred = np.array(temp_list)\n",
        "        # print(Y_pred.shape)\n",
        "        loss = criterion(torch.from_numpy(Y_pred), Y)\n",
        "        loss = Variable(loss, requires_grad=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        step_losses.append(loss.item())\n",
        "        print(\"----------------------\")\n",
        "        print(\"epoch_loss:\",epoch_loss)\n",
        "        print(\"step_losses:\",loss.item() )\n",
        "    epoch_losses.append(epoch_loss/len(data_loader))\n",
        "\n",
        "with open('epoch_losses.pkl', 'wb') as fp:\n",
        "    pickle.dump(epoch_losses, fp)\n",
        "\n",
        "with open('step_losses.pkl', 'wb') as fp:\n",
        "    pickle.dump(step_losses, fp)\n",
        "\n",
        "model_name = \"/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/U-Net_encoder.pth\"\n",
        "torch.save(encoder.state_dict(), model_name)\n",
        "\n",
        "model_name = \"/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/U-Net_gplayer.pth\"\n",
        "torch.save(gplayer.state_dict(), model_name)\n",
        "\n",
        "model_name = \"/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/U-Net_decoder.pth\"\n",
        "torch.save(decoder.state_dict(), model_name)\n",
        "\n",
        "class scannet(Dataset):\n",
        "    \n",
        "    def __init__(self, image_dir, label_dir):\n",
        "        self.image_dir = image_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.image_fns = os.listdir(image_dir)\n",
        "        self.label_fns = os.listdir(label_dir)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_fns)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        image_fn = self.image_fns[index]\n",
        "        label_fn = self.label_fns[index]\n",
        "        image_fp = os.path.join(self.image_dir, image_fn)\n",
        "        image = Image.open(image_fp).convert('RGB')\n",
        "        image = np.array(image)\n",
        "        print(index)\n",
        "        print(image_fp)\n",
        "        label_fp = os.path.join(self.label_dir, image_fn.split('.')[0]+'.png')\n",
        "        label = Image.open(label_fp)\n",
        "        label = np.array(label)\n",
        "        print(label_fp)\n",
        "        cityscape, label = image, label\n",
        "        cityscape = self.transform(cityscape)\n",
        "        label_class = torch.Tensor(label).long()\n",
        "        return cityscape, label_class, \n",
        "    \n",
        "    def split_image(self, image):\n",
        "        image = np.array(image)\n",
        "        cityscape, label = image[:, :256, :], image[:, 256:, :]\n",
        "        return cityscape, label\n",
        "    \n",
        "    def transform(self, image):\n",
        "        transform_ops = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "        ])\n",
        "        return transform_ops(image)\n",
        "\n",
        "test_batch_size = 8\n",
        "dataset = scannet(val_dir_color, val_dir_label)\n",
        "data_loader = DataLoader(dataset, batch_size=test_batch_size)\n",
        "\n",
        "X, Y = next(iter(data_loader))\n",
        "X, Y = X.to(device), Y.to(device)\n",
        "Y_pred = model(X)\n",
        "Y_pred = torch.argmax(Y_pred, dim=1)\n",
        "\n",
        "inverse_transform = transforms.Compose([\n",
        "    transforms.Normalize((-0.485/0.229, -0.456/0.224, -0.406/0.225), (1/0.229, 1/0.224, 1/0.225))\n",
        "])\n",
        "\n",
        "X = X.cpu().detach().numpy()\n",
        "Y = Y.cpu().detach().numpy()\n",
        "Y_pred = Y_pred.cpu().detach().numpy()\n",
        "\n",
        "print(X.shape, Y.shape, Y_pred.shape)\n",
        "\n",
        "with open('X.pkl', 'wb') as fp:\n",
        "    pickle.dump(X, fp)\n",
        "\n",
        "with open('Y.pkl', 'wb') as fp:\n",
        "    pickle.dump(Y, fp)\n",
        "\n",
        "with open('Y_pred.pkl', 'wb') as fp:\n",
        "    pickle.dump(Y_pred, fp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LVBGnmL4T9wp",
        "outputId": "82fd339f-033a-4395-b571-961a783eada6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "train_fns_color: 230 train_fns_label: 230 val_fns_color: 49 val_fns_label: 49\n",
            "Length of dataset: 230\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "  7%|▋         | 1/15 [00:43<10:04, 43.14s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------\n",
            "epoch_loss: 3.8607120513916016\n",
            "step_losses: 3.8607120513916016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 13%|█▎        | 2/15 [01:22<08:54, 41.09s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------\n",
            "epoch_loss: 7.744035243988037\n",
            "step_losses: 3.8833231925964355\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 20%|██        | 3/15 [02:00<07:56, 39.71s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------\n",
            "epoch_loss: 11.63466501235962\n",
            "step_losses: 3.890629768371582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 27%|██▋       | 4/15 [02:38<07:09, 39.07s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------\n",
            "epoch_loss: 15.54025673866272\n",
            "step_losses: 3.9055917263031006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 33%|███▎      | 5/15 [03:18<06:33, 39.40s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------\n",
            "epoch_loss: 19.48065423965454\n",
            "step_losses: 3.9403975009918213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 40%|████      | 6/15 [03:56<05:50, 38.92s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------\n",
            "epoch_loss: 23.372063636779785\n",
            "step_losses: 3.891409397125244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 47%|████▋     | 7/15 [04:35<05:09, 38.66s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------\n",
            "epoch_loss: 27.272435903549194\n",
            "step_losses: 3.900372266769409\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 53%|█████▎    | 8/15 [05:13<04:29, 38.48s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------\n",
            "epoch_loss: 31.169292211532593\n",
            "step_losses: 3.8968563079833984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 60%|██████    | 9/15 [05:52<03:53, 38.85s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------\n",
            "epoch_loss: 35.05041766166687\n",
            "step_losses: 3.8811254501342773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 67%|██████▋   | 10/15 [06:31<03:13, 38.67s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------\n",
            "epoch_loss: 38.98189401626587\n",
            "step_losses: 3.931476354598999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 73%|███████▎  | 11/15 [07:09<02:33, 38.46s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------\n",
            "epoch_loss: 42.89477324485779\n",
            "step_losses: 3.912879228591919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 80%|████████  | 12/15 [07:47<01:54, 38.33s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------\n",
            "epoch_loss: 46.77863097190857\n",
            "step_losses: 3.8838577270507812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 87%|████████▋ | 13/15 [08:26<01:17, 38.59s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------\n",
            "epoch_loss: 50.67338728904724\n",
            "step_losses: 3.894756317138672\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 93%|█████████▎| 14/15 [09:04<00:38, 38.43s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------\n",
            "epoch_loss: 54.56041884422302\n",
            "step_losses: 3.8870315551757812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 15/15 [09:18<00:00, 31.17s/it]\u001b[A\n",
            "100%|██████████| 1/1 [09:18<00:00, 558.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------\n",
            "epoch_loss: 58.39350199699402\n",
            "step_losses: 3.833083152770996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/color_valid/4740.jpg\n",
            "/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/label_valid_n/4740.png\n",
            "1\n",
            "/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/color_valid/4720.jpg\n",
            "/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/label_valid_n/4720.png\n",
            "2\n",
            "/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/color_valid/4700.jpg\n",
            "/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/label_valid_n/4700.png\n",
            "3\n",
            "/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/color_valid/4680.jpg\n",
            "/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/label_valid_n/4680.png\n",
            "4\n",
            "/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/color_valid/4780.jpg\n",
            "/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/label_valid_n/4780.png\n",
            "5\n",
            "/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/color_valid/4600.jpg\n",
            "/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/label_valid_n/4600.png\n",
            "6\n",
            "/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/color_valid/4660.jpg\n",
            "/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/label_valid_n/4660.png\n",
            "7\n",
            "/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/color_valid/4620.jpg\n",
            "/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/label_valid_n/4620.png\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-688b37a15f6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class scannet(Dataset):\n",
        "    \n",
        "    def __init__(self, image_dir, label_dir):\n",
        "        self.image_dir = image_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.image_fns = os.listdir(image_dir)\n",
        "        self.label_fns = os.listdir(label_dir)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_fns)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        image_fn = self.image_fns[index]\n",
        "        label_fn = self.label_fns[index]\n",
        "        image_fp = os.path.join(self.image_dir, image_fn)\n",
        "        image = Image.open(image_fp).convert('RGB')\n",
        "        image = np.array(image)\n",
        "        print(index)\n",
        "        print(image_fp)\n",
        "        label_fp = os.path.join(self.label_dir, image_fn.split('.')[0]+'.png')\n",
        "        label = Image.open(label_fp)\n",
        "        label = np.array(label)\n",
        "        print(label_fp)\n",
        "        cityscape, label = image, label\n",
        "        cityscape = self.transform(cityscape)\n",
        "        label_class = torch.Tensor(label).long()\n",
        "        return cityscape, label_class, \n",
        "    \n",
        "    def split_image(self, image):\n",
        "        image = np.array(image)\n",
        "        cityscape, label = image[:, :256, :], image[:, 256:, :]\n",
        "        return cityscape, label\n",
        "    \n",
        "    def transform(self, image):\n",
        "        transform_ops = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "        ])\n",
        "        return transform_ops(image)\n",
        "\n",
        "test_batch_size = 6\n",
        "dataset = scannet(val_dir_color, val_dir_label)\n",
        "data_loader = DataLoader(dataset, batch_size=test_batch_size)\n",
        "\n",
        "model_path_encoder = \"/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/U-Net_encoder.pth\"\n",
        "model_path_gplayer = \"/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/U-Net_gplayer.pth\"\n",
        "model_path_decoder = \"/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/U-Net_decoder.pth\"\n",
        "\n",
        "encoder = encoder(num_classes=num_classes).to(device)\n",
        "encoder.load_state_dict(torch.load(model_path_encoder))\n",
        "\n",
        "gplayer = GPlayer().to(device)\n",
        "gplayer.load_state_dict(torch.load(model_path_gplayer))\n",
        "\n",
        "decoder = decoder(num_classes=num_classes).to(device)\n",
        "decoder.load_state_dict(torch.load(model_path_decoder))\n",
        "\n",
        "X, Y = next(iter(data_loader))\n",
        "\n",
        "X, Y = X.to(device), Y.to(device)\n",
        "\n",
        "middle_out, contracting_41_out, contracting_31_out, contracting_21_out, contracting_11_out = encoder(X)\n",
        "\n",
        "if len(X) == 16:\n",
        "    value = 16\n",
        "    with open('/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/D.pkl', 'rb') as fp:\n",
        "          D = pickle.load(fp)\n",
        "if len(X) == 6:\n",
        "    value = 6\n",
        "    with open('/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/D_6.pkl', 'rb') as fp:\n",
        "          D = pickle.load(fp)\n",
        "Z = gplayer(D,middle_out)\n",
        "\n",
        "l,c,h,w = Z.size()\n",
        "\n",
        "temp_list = []\n",
        "# print(Z.shape)\n",
        "for i in range(value):\n",
        "    Z_new = torch.from_numpy((np.expand_dims(Z[i].detach().numpy(), axis=0)))\n",
        "    contracting_41_out_new = torch.from_numpy((np.expand_dims(contracting_41_out[i].detach().numpy(), axis=0)))\n",
        "    contracting_31_out_new = torch.from_numpy((np.expand_dims(contracting_31_out[i].detach().numpy(), axis=0)))\n",
        "    contracting_21_out_new = torch.from_numpy((np.expand_dims(contracting_21_out[i].detach().numpy(), axis=0)))\n",
        "    contracting_11_out_new = torch.from_numpy((np.expand_dims(contracting_11_out[i].detach().numpy(), axis=0)))\n",
        "    # print(contracting_41_out_new.shape, contracting_31_out_new.shape, contracting_21_out_new.shape, contracting_11_out_new.shape)\n",
        "    Y_pred_n = decoder(Z_new.to(device), contracting_41_out_new, contracting_31_out_new, contracting_21_out_new, contracting_11_out_new)\n",
        "    temp_list.append(np.squeeze(Y_pred_n.detach().numpy()))\n",
        "\n",
        "Y_pred = np.array(temp_list)\n",
        "Y_pred = torch.from_numpy(Y_pred)\n",
        "Y_pred = torch.argmax(Y_pred, dim=1)\n",
        "\n",
        "inverse_transform = transforms.Compose([\n",
        "    transforms.Normalize((-0.485/0.229, -0.456/0.224, -0.406/0.225), (1/0.229, 1/0.224, 1/0.225))\n",
        "])\n",
        "\n",
        "X = X.cpu().detach().numpy()\n",
        "Y = Y.cpu().detach().numpy()\n",
        "Y_pred = Y_pred.cpu().detach().numpy()\n",
        "\n",
        "print(X.shape, Y.shape, Y_pred.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "ehBFB6fxubQd",
        "outputId": "d8a03539-725b-4f03-8b06-eac08d67f304"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-2a0824860189>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mmodel_path_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/U-Net_decoder.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path_encoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DataParallel.forward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'num_classes'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('4580.txt') as f:\n",
        "  for l in f.readlines():\n",
        "        l = l.strip('\\n')\n",
        "        print(l)\n",
        "      # gt_poses.append(np.array(l.split(' ')).astype(np.float32).reshape(4, 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMi3Vvf7rSZg",
        "outputId": "ea6963f3-f3fe-4599-ac4b-2f2ae7a7b78f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.851565 0.055141 0.521343 5.209820\n",
            "-0.523823 0.049350 0.850397 6.443484\n",
            "0.021163 -0.997258 0.070909 1.636609\n",
            "0.000000 0.000000 0.000000 1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "axes[0].plot(step_losses)\n",
        "axes[1].plot(epoch_losses)"
      ],
      "metadata": {
        "id": "Ngttf9hDWHUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_batch_size = 8\n",
        "inverse_transform = transforms.Compose([\n",
        "    transforms.Normalize((-0.485/0.229, -0.456/0.224, -0.406/0.225), (1/0.229, 1/0.224, 1/0.225))\n",
        "])\n",
        "\n",
        "fig, axes = plt.subplots(test_batch_size, 3, figsize=(3*5, test_batch_size*5))\n",
        "\n",
        "for i in range(test_batch_size):\n",
        "    \n",
        "    landscape = inverse_transform(torch.from_numpy(X[i])).permute(1, 2, 0).cpu().detach().numpy()\n",
        "    label_class = Y[i]\n",
        "    label_class_predicted = Y_pred[i]\n",
        "    \n",
        "    axes[i, 0].imshow(landscape)\n",
        "    axes[i, 0].set_title(\"Landscape\")\n",
        "    axes[i, 1].imshow(label_class)\n",
        "    axes[i, 1].set_title(\"Label Class\")\n",
        "    axes[i, 2].imshow(label_class_predicted)\n",
        "    axes[i, 2].set_title(\"Label Class - Predicted\")"
      ],
      "metadata": {
        "id": "4Nnfvc_4WJHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gt_poses = []\n",
        "        with open('/content/drive/MyDrive/Master_thesis/Unet/unet_scannet/data/pose.txt') as f:\n",
        "            for l in f.readlines():\n",
        "                l = l.strip('\\n')\n",
        "                gt_poses.append(np.array(l.split(' ')).astype(np.float32).reshape(4, 4))\n",
        "\n",
        "        def pose_distance(p1, p2):\n",
        "            rel_pose = np.dot(np.linalg.inv(p1), p2)\n",
        "            R = rel_pose[:3, :3]\n",
        "            t = rel_pose[:3, 3]\n",
        "\n",
        "            return round(np.sqrt(np.linalg.norm(t) ** 2 + 2 * (1 - min(3.0, np.matrix.trace(R)) / 3)), 4)\n",
        "            \n",
        "        def genDistM(poses):\n",
        "            n = len(poses)\n",
        "            D = np.zeros((n, n))\n",
        "            for i in range(n):\n",
        "                for j in range(n):\n",
        "                    D[i, j] = pose_distance(poses[i], poses[j])\n",
        "            return D\n",
        "\n",
        "        poses = [gt_poses[0], gt_poses[1]]\n",
        "        D = torch.from_numpy(np.expand_dims(genDistM(poses), 0))\n",
        "\n",
        "        D = torch.from_numpy(np.expand_dims(D, 0))\n",
        "        l = 1\n",
        "        \n",
        "        gamma2 = nn.Parameter(torch.randn(1), requires_grad=True).float()\n",
        "        ell = nn.Parameter(torch.randn(1), requires_grad=True).float()\n",
        "        sigma2 = nn.Parameter(torch.randn(1), requires_grad=True).float()\n",
        "\n",
        "        print(middle_out.shape)\n",
        "\n",
        "        a, b, c, d = middle_out.size()\n",
        "        print(a, b, c, d)\n",
        "        Y = middle_out.view(1,-1).cpu().float()\n",
        "        print(Y.shape)\n",
        "        D = D.float()\n",
        "        K = torch.exp(gamma2) * (1 + math.sqrt(3) * D / torch.exp(ell)) * torch.exp(-math.sqrt(3) * D / torch.exp(ell))\n",
        "        I = torch.eye(l).expand(l, l).float()\n",
        "        X,_ = torch.solve(Y, K+torch.exp(sigma2)*I)\n",
        "        Z = K.bmm(X)\n",
        "        Z = F.relu(Z)\n",
        "        middle_out = Z[:, 0].view(a, 1024, 15, 20).to(device)"
      ],
      "metadata": {
        "id": "FfAGPAWraZAw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}